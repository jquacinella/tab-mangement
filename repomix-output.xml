This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
database/
  schema/
    00_auth_setup.sql
    01_extensions.sql
    02_core_tables.sql
    03_indexes_views.sql
    04_seed_data.sql
enrichment_service/
  __init__.py
  Dockerfile
  dspy_setup.py
  main.py
  models.py
ingest/
  __init__.py
  cli.py
  db.py
  firefox_parser.py
n8n/
  workflows/
    enrich_tabs.json
  README.md
parser_service/
  parsers/
    __init__.py
    base.py
    generic.py
    registry.py
    twitter.py
    youtube.py
  __init__.py
  Dockerfile
  main.py
  models.py
shared/
  __init__.py
  search.py
tests/
  e2e/
    __init__.py
    test_api_endpoints.py
    test_web_ui.py
  unit/
    __init__.py
  __init__.py
  conftest.py
  README.md
web_ui/
  routes/
    __init__.py
    export.py
    search.py
    tabs.py
  static/
    css/
      style.css
  templates/
    fragments/
      tab_detail.html
      tab_row.html
      tab_rows.html
    base.html
    index.html
    stats.html
  __init__.py
  db.py
  Dockerfile
  main.py
  models.py
.dockerignore
.env.example
CLAUDE_CODE_SPEC.md
config.py
docker-compose.yml
PHOENIX_SETUP.md
pytest.ini
README.md
requirements.txt
tabbacklog-implementation.zip
test_phoenix_setup.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="database/schema/00_auth_setup.sql">
-- TabBacklog v1 - Auth Schema Setup
-- Creates a minimal auth.users table for standalone PostgreSQL
-- (Supabase provides this automatically)

-- Create auth schema if it doesn't exist
CREATE SCHEMA IF NOT EXISTS auth;

-- Create a minimal users table for standalone mode
CREATE TABLE IF NOT EXISTS auth.users (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  email text UNIQUE,
  created_at timestamptz NOT NULL DEFAULT now(),
  updated_at timestamptz NOT NULL DEFAULT now()
);

-- Create an index on email for lookups
CREATE INDEX IF NOT EXISTS idx_auth_users_email ON auth.users(email);

-- Insert a default user if none exists (for single-user mode)
-- This UUID should match your DEFAULT_USER_ID in .env
INSERT INTO auth.users (id, email)
VALUES ('00000000-0000-0000-0000-000000000000'::uuid, 'default@tabbacklog.local')
ON CONFLICT (id) DO NOTHING;
</file>

<file path="database/schema/01_extensions.sql">
-- TabBacklog v1 - PostgreSQL Extensions
-- Enable required PostgreSQL extensions

-- ============================================================================
-- Extensions
-- ============================================================================

-- Enable trigram fuzzy text search
CREATE EXTENSION IF NOT EXISTS pg_trgm;

-- Enable vector similarity search (requires pgvector)
CREATE EXTENSION IF NOT EXISTS vector;

-- Enable UUID generation
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
</file>

<file path="database/schema/02_core_tables.sql">
-- TabBacklog v1 - Core Database Schema
-- PostgreSQL with Supabase extensions

-- ============================================================================
-- Core Tables
-- ============================================================================

-- Main tab tracking table
CREATE TABLE tab_item (
  id             bigserial PRIMARY KEY,
  user_id        uuid NOT NULL REFERENCES auth.users(id),
  url            text NOT NULL,
  page_title     text,
  window_label   text,
  collected_at   timestamptz NOT NULL DEFAULT now(),

  -- Pipeline status tracking
  status         text NOT NULL DEFAULT 'new',
  -- Allowed values: 'new', 'fetch_pending', 'fetched', 'fetch_error',
  --                 'parse_pending', 'parsed', 'parse_error',
  --                 'llm_pending', 'enriched', 'llm_error'

  last_error     text,
  error_at       timestamptz,

  -- User workflow
  is_processed   boolean NOT NULL DEFAULT false,
  processed_at   timestamptz,

  -- Standard fields
  created_at     timestamptz NOT NULL DEFAULT now(),
  updated_at     timestamptz NOT NULL DEFAULT now(),
  deleted_at     timestamptz
);

-- Unique constraint on user_id + url (only active records)
CREATE UNIQUE INDEX idx_tab_item_user_url_unique
  ON tab_item(user_id, url)
  WHERE deleted_at IS NULL;

-- Index for common queries
CREATE INDEX idx_tab_item_user_status ON tab_item(user_id, status) WHERE deleted_at IS NULL;
CREATE INDEX idx_tab_item_user_processed ON tab_item(user_id, is_processed) WHERE deleted_at IS NULL;

-- ============================================================================

-- Parsed content from fetch/parse service
CREATE TABLE tab_parsed (
  tab_id          bigint PRIMARY KEY REFERENCES tab_item(id) ON DELETE CASCADE,
  site_kind       text NOT NULL,      -- youtube | twitter | generic_html | ...
  title_extracted text,
  text_full       text,
  word_count      integer,
  video_seconds   integer,
  metadata        jsonb NOT NULL DEFAULT '{}',
  created_at      timestamptz NOT NULL DEFAULT now(),
  updated_at      timestamptz NOT NULL DEFAULT now()
);

-- Index for searching by site kind
CREATE INDEX idx_tab_parsed_site_kind ON tab_parsed(site_kind);

-- ============================================================================

-- LLM-generated enrichments (current version)
CREATE TABLE tab_enrichment (
  tab_id         bigint PRIMARY KEY REFERENCES tab_item(id) ON DELETE CASCADE,
  summary        text,
  content_type   text,  -- article | video | paper | code_repo | reference | misc
  est_read_min   integer,
  video_seconds  integer,
  source_lang    text,
  priority       text,  -- high | medium | low
  raw_meta       jsonb NOT NULL DEFAULT '{}',
  model_name     text,
  created_at     timestamptz NOT NULL DEFAULT now(),
  updated_at     timestamptz NOT NULL DEFAULT now()
);

-- Index for filtering by content type
CREATE INDEX idx_tab_enrichment_content_type ON tab_enrichment(content_type);

-- ============================================================================

-- Historical record of all enrichment runs
CREATE TABLE tab_enrichment_history (
  id              bigserial PRIMARY KEY,
  tab_id          bigint NOT NULL REFERENCES tab_item(id) ON DELETE CASCADE,
  summary         text,
  content_type    text,
  est_read_min    integer,
  video_seconds   integer,
  source_lang     text,
  priority        text,
  raw_meta        jsonb NOT NULL DEFAULT '{}',
  model_name      text,
  run_started_at  timestamptz NOT NULL,
  run_finished_at timestamptz,
  created_at      timestamptz NOT NULL DEFAULT now()
);

-- Index for querying history by tab
CREATE INDEX idx_tab_enrichment_history_tab ON tab_enrichment_history(tab_id);

-- ============================================================================

-- Tag definitions
CREATE TABLE tag (
  id          bigserial PRIMARY KEY,
  user_id     uuid NOT NULL REFERENCES auth.users(id),
  name        text NOT NULL,
  kind        text NOT NULL DEFAULT 'generic',  -- generic | project | auto
  created_at  timestamptz NOT NULL DEFAULT now(),
  updated_at  timestamptz NOT NULL DEFAULT now(),
  deleted_at  timestamptz
);

-- Unique constraint on user_id + name
CREATE UNIQUE INDEX idx_tag_user_name_unique
  ON tag(user_id, name)
  WHERE deleted_at IS NULL;

-- ============================================================================

-- Many-to-many relationship between tabs and tags
CREATE TABLE tab_tag (
  tab_id      bigint NOT NULL REFERENCES tab_item(id) ON DELETE CASCADE,
  tag_id      bigint NOT NULL REFERENCES tag(id) ON DELETE CASCADE,
  created_at  timestamptz NOT NULL DEFAULT now(),
  PRIMARY KEY (tab_id, tag_id)
);

-- Indexes for efficient querying
CREATE INDEX idx_tab_tag_tag ON tab_tag(tag_id);

-- ============================================================================

-- Vector embeddings for semantic search
CREATE TABLE tab_embedding (
  tab_id      bigint PRIMARY KEY REFERENCES tab_item(id) ON DELETE CASCADE,
  embedding   vector(768),  -- Adjust dimension based on embedding model
  model_name  text NOT NULL,
  created_at  timestamptz NOT NULL DEFAULT now(),
  updated_at  timestamptz NOT NULL DEFAULT now()
);

-- Vector similarity index (requires pgvector extension)
CREATE INDEX idx_tab_embedding_vector ON tab_embedding
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

-- ============================================================================

-- Event log for debugging and audit trail
CREATE TABLE event_log (
  id           bigserial PRIMARY KEY,
  user_id      uuid REFERENCES auth.users(id),
  event_type   text NOT NULL,
  -- Examples: tab_created, tab_duplicate_skipped, fetch_started, fetch_success,
  --           fetch_error, llm_enrich_success, llm_enrich_error, tab_processed
  entity_type  text,  -- tab_item | tag | etc.
  entity_id    bigint,
  details      jsonb NOT NULL DEFAULT '{}',
  created_at   timestamptz NOT NULL DEFAULT now()
);

-- Indexes for common event queries
CREATE INDEX idx_event_log_user ON event_log(user_id, created_at DESC);
CREATE INDEX idx_event_log_type ON event_log(event_type, created_at DESC);
CREATE INDEX idx_event_log_entity ON event_log(entity_type, entity_id);

-- ============================================================================

-- Auto-update updated_at timestamps
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = now();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create triggers for tables with updated_at
CREATE TRIGGER update_tab_item_updated_at
    BEFORE UPDATE ON tab_item
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_tab_parsed_updated_at
    BEFORE UPDATE ON tab_parsed
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_tab_enrichment_updated_at
    BEFORE UPDATE ON tab_enrichment
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_tag_updated_at
    BEFORE UPDATE ON tag
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_tab_embedding_updated_at
    BEFORE UPDATE ON tab_embedding
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
</file>

<file path="database/schema/03_indexes_views.sql">
-- TabBacklog v1 - Extensions and Additional Indexes
-- Enable required PostgreSQL extensions

-- ============================================================================
-- Extensions
-- ============================================================================

-- Enable trigram fuzzy text search
CREATE EXTENSION IF NOT EXISTS pg_trgm;

-- Enable vector similarity search (Supabase comes with pgvector)
CREATE EXTENSION IF NOT EXISTS vector;

-- Enable UUID generation (usually already enabled in Supabase)
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- ============================================================================
-- Trigram Indexes for Fuzzy Search
-- ============================================================================

-- Index on tab_item.page_title for fuzzy search
CREATE INDEX idx_tab_item_title_trgm
  ON tab_item USING gin (page_title gin_trgm_ops)
  WHERE deleted_at IS NULL;

-- Index on tab_item.url for fuzzy search
CREATE INDEX idx_tab_item_url_trgm
  ON tab_item USING gin (url gin_trgm_ops)
  WHERE deleted_at IS NULL;

-- Index on tab_enrichment.summary for fuzzy search
CREATE INDEX idx_tab_enrichment_summary_trgm
  ON tab_enrichment USING gin (summary gin_trgm_ops);

-- Index on tab_parsed.text_full for fuzzy search (optional - can be large)
-- Uncomment if you want to search full text
-- CREATE INDEX idx_tab_parsed_text_trgm
--   ON tab_parsed USING gin (text_full gin_trgm_ops);

-- ============================================================================
-- Additional Performance Indexes
-- ============================================================================

-- Index for date-based queries
CREATE INDEX idx_tab_item_collected_at ON tab_item(collected_at DESC) WHERE deleted_at IS NULL;

-- Composite index for common filter combinations
CREATE INDEX idx_tab_item_user_status_processed 
  ON tab_item(user_id, status, is_processed) 
  WHERE deleted_at IS NULL;

-- Index for tag-based filtering
CREATE INDEX idx_tag_kind ON tag(kind) WHERE deleted_at IS NULL;

-- Index for enrichment read time filtering
CREATE INDEX idx_tab_enrichment_read_time ON tab_enrichment(est_read_min) 
  WHERE est_read_min IS NOT NULL;

-- ============================================================================
-- Helper Functions for Search
-- ============================================================================

-- Function to get similarity score for fuzzy search
CREATE OR REPLACE FUNCTION tab_search_score(
  search_query text,
  title text,
  summary text
) RETURNS float AS $$
BEGIN
  RETURN GREATEST(
    COALESCE(similarity(search_query, title), 0),
    COALESCE(similarity(search_query, summary), 0)
  );
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- ============================================================================
-- Views for Common Queries
-- ============================================================================

-- View combining tab_item with enrichments and tags
CREATE OR REPLACE VIEW v_tabs_enriched AS
SELECT 
  ti.id,
  ti.user_id,
  ti.url,
  ti.page_title,
  ti.window_label,
  ti.collected_at,
  ti.status,
  ti.is_processed,
  ti.processed_at,
  ti.created_at,
  ti.updated_at,
  tp.site_kind,
  tp.word_count,
  tp.video_seconds AS parsed_video_seconds,
  te.summary,
  te.content_type,
  te.est_read_min,
  te.video_seconds AS enriched_video_seconds,
  te.priority,
  te.model_name,
  COALESCE(te.video_seconds, tp.video_seconds) AS video_seconds_combined,
  array_agg(DISTINCT t.name) FILTER (WHERE t.name IS NOT NULL) AS tags,
  array_agg(DISTINCT t.id) FILTER (WHERE t.id IS NOT NULL) AS tag_ids
FROM tab_item ti
LEFT JOIN tab_parsed tp ON ti.id = tp.tab_id
LEFT JOIN tab_enrichment te ON ti.id = te.tab_id
LEFT JOIN tab_tag tt ON ti.id = tt.tab_id
LEFT JOIN tag t ON tt.tag_id = t.id AND t.deleted_at IS NULL
WHERE ti.deleted_at IS NULL
GROUP BY 
  ti.id, ti.user_id, ti.url, ti.page_title, ti.window_label,
  ti.collected_at, ti.status, ti.is_processed, ti.processed_at,
  ti.created_at, ti.updated_at,
  tp.site_kind, tp.word_count, tp.video_seconds,
  te.summary, te.content_type, te.est_read_min, te.video_seconds,
  te.priority, te.model_name;

-- ============================================================================
-- Statistics and Monitoring Views
-- ============================================================================

-- View for pipeline status overview
CREATE OR REPLACE VIEW v_pipeline_stats AS
SELECT 
  user_id,
  status,
  COUNT(*) as count,
  MIN(created_at) as oldest,
  MAX(created_at) as newest
FROM tab_item
WHERE deleted_at IS NULL
GROUP BY user_id, status;

-- View for content type distribution
CREATE OR REPLACE VIEW v_content_type_stats AS
SELECT 
  ti.user_id,
  te.content_type,
  COUNT(*) as count,
  AVG(te.est_read_min) as avg_read_min,
  SUM(te.est_read_min) as total_read_min
FROM tab_item ti
JOIN tab_enrichment te ON ti.id = te.tab_id
WHERE ti.deleted_at IS NULL
GROUP BY ti.user_id, te.content_type;

-- View for error tracking
CREATE OR REPLACE VIEW v_error_summary AS
SELECT 
  user_id,
  status,
  COUNT(*) as error_count,
  MAX(error_at) as last_error_at,
  array_agg(DISTINCT COALESCE(SUBSTRING(last_error, 1, 100), 'No error message')) 
    FILTER (WHERE last_error IS NOT NULL) as error_samples
FROM tab_item
WHERE status IN ('fetch_error', 'parse_error', 'llm_error')
  AND deleted_at IS NULL
GROUP BY user_id, status;
</file>

<file path="database/schema/04_seed_data.sql">
-- TabBacklog v1 - Seed Data
-- Initial project tags and sample data

-- ============================================================================
-- Project Tags
-- ============================================================================
-- Note: Replace 'YOUR_USER_ID_HERE' with actual user UUID when running

-- Function to seed project tags for a user
CREATE OR REPLACE FUNCTION seed_project_tags(p_user_id uuid)
RETURNS void AS $$
BEGIN
  -- Insert project tags if they don't already exist
  INSERT INTO tag (user_id, name, kind)
  VALUES 
    (p_user_id, 'argumentation_on_the_web', 'project'),
    (p_user_id, 'democratic_economic_planning', 'project'),
    (p_user_id, 'other_research', 'project')
  ON CONFLICT (user_id, name) WHERE deleted_at IS NULL
  DO NOTHING;
END;
$$ LANGUAGE plpgsql;

-- Example: Call this function after creating a user
-- SELECT seed_project_tags('YOUR_USER_ID_HERE'::uuid);

-- ============================================================================
-- Common Auto-Generated Tags
-- ============================================================================

CREATE OR REPLACE FUNCTION seed_common_tags(p_user_id uuid)
RETURNS void AS $$
BEGIN
  -- Insert common tags used for auto-tagging
  INSERT INTO tag (user_id, name, kind)
  VALUES 
    -- Content markers
    (p_user_id, '#video', 'auto'),
    (p_user_id, '#longread', 'auto'),
    (p_user_id, '#shortread', 'auto'),
    (p_user_id, '#paper', 'auto'),
    (p_user_id, '#code', 'auto'),
    (p_user_id, '#reference', 'auto'),
    
    -- Priority markers
    (p_user_id, '#priority_high', 'auto'),
    (p_user_id, '#priority_medium', 'auto'),
    (p_user_id, '#priority_low', 'auto'),
    
    -- Language markers
    (p_user_id, '#english', 'auto'),
    (p_user_id, '#non_english', 'auto')
  ON CONFLICT (user_id, name) WHERE deleted_at IS NULL
  DO NOTHING;
END;
$$ LANGUAGE plpgsql;

-- Example: Call this function after creating a user
-- SELECT seed_common_tags('YOUR_USER_ID_HERE'::uuid);

-- ============================================================================
-- Helper Function to Initialize a New User
-- ============================================================================

CREATE OR REPLACE FUNCTION initialize_user_data(p_user_id uuid)
RETURNS void AS $$
BEGIN
  -- Seed project tags
  PERFORM seed_project_tags(p_user_id);
  
  -- Seed common auto-generated tags
  PERFORM seed_common_tags(p_user_id);
  
  -- Log the initialization
  INSERT INTO event_log (user_id, event_type, entity_type, details)
  VALUES (
    p_user_id,
    'user_initialized',
    'user',
    jsonb_build_object(
      'project_tags_created', true,
      'common_tags_created', true,
      'initialized_at', now()
    )
  );
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Sample Data (for development/testing)
-- ============================================================================

-- Uncomment this section to insert sample data for testing
/*
-- Create a test user (if not using Supabase auth)
-- INSERT INTO auth.users (id, email) 
-- VALUES ('00000000-0000-0000-0000-000000000001', 'test@example.com')
-- ON CONFLICT (id) DO NOTHING;

-- Initialize the test user
SELECT initialize_user_data('00000000-0000-0000-0000-000000000001'::uuid);

-- Insert some sample tabs
INSERT INTO tab_item (user_id, url, page_title, window_label, status)
VALUES 
  (
    '00000000-0000-0000-0000-000000000001'::uuid,
    'https://www.youtube.com/watch?v=dQw4w9WgXcQ',
    'Sample Video',
    'Research',
    'new'
  ),
  (
    '00000000-0000-0000-0000-000000000001'::uuid,
    'https://arxiv.org/abs/2301.00000',
    'Sample Research Paper',
    'Papers',
    'new'
  ),
  (
    '00000000-0000-0000-0000-000000000001'::uuid,
    'https://example.com/article',
    'Sample Article',
    'Reading',
    'new'
  )
ON CONFLICT (user_id, url) WHERE deleted_at IS NULL
DO NOTHING;
*/

-- ============================================================================
-- Data Validation Queries
-- ============================================================================

-- Check that all required tags exist for a user
CREATE OR REPLACE FUNCTION validate_user_tags(p_user_id uuid)
RETURNS TABLE(missing_tags text[]) AS $$
BEGIN
  RETURN QUERY
  SELECT ARRAY(
    SELECT required_tag
    FROM unnest(ARRAY[
      'argumentation_on_the_web',
      'democratic_economic_planning', 
      'other_research'
    ]) AS required_tag
    WHERE NOT EXISTS (
      SELECT 1 FROM tag
      WHERE user_id = p_user_id
        AND name = required_tag
        AND kind = 'project'
        AND deleted_at IS NULL
    )
  );
END;
$$ LANGUAGE plpgsql;

-- Usage: SELECT * FROM validate_user_tags('YOUR_USER_ID_HERE'::uuid);
</file>

<file path="enrichment_service/__init__.py">
"""
TabBacklog v1 - Enrichment Service

FastAPI service for LLM-based content enrichment using DSPy.
Generates summaries, content types, tags, and metadata for tabs.
"""

__version__ = "1.0.0"
</file>

<file path="enrichment_service/models.py">
"""
TabBacklog v1 - Enrichment Service Pydantic Models

API request/response models and enrichment schema.
"""

from typing import Literal, Optional
from pydantic import BaseModel, Field


class EnrichmentRequest(BaseModel):
    """Request model for /enrich_tab endpoint"""
    url: str = Field(..., description="URL of the tab")
    title: Optional[str] = Field(None, description="Page title")
    site_kind: str = Field(..., description="Type of site (youtube, twitter, generic_html)")
    text: Optional[str] = Field(None, description="Extracted text content")
    word_count: Optional[int] = Field(None, description="Word count of content")
    video_seconds: Optional[int] = Field(None, description="Video duration in seconds")

    class Config:
        json_schema_extra = {
            "example": {
                "url": "https://example.com/article",
                "title": "Understanding Machine Learning",
                "site_kind": "generic_html",
                "text": "This article explains the fundamentals of machine learning...",
                "word_count": 1500,
            }
        }


class Enrichment(BaseModel):
    """
    LLM-generated enrichment data for a tab.

    This is the structured output from the DSPy enrichment module.
    """
    summary: str = Field(
        ...,
        description="Brief summary of the content (2-3 sentences)",
        min_length=10,
        max_length=500,
    )
    content_type: Literal["article", "video", "paper", "code_repo", "reference", "misc"] = Field(
        ...,
        description="Type of content",
    )
    tags: list[str] = Field(
        default_factory=list,
        description="Relevant tags (e.g., #video, #longread, #tutorial)",
        max_length=10,
    )
    projects: list[str] = Field(
        default_factory=list,
        description="Related project categories",
        max_length=5,
    )
    est_read_min: Optional[int] = Field(
        None,
        description="Estimated reading/watch time in minutes",
        ge=1,
        le=600,
    )
    priority: Optional[Literal["high", "medium", "low"]] = Field(
        None,
        description="Suggested priority level",
    )

    class Config:
        json_schema_extra = {
            "example": {
                "summary": "A comprehensive guide to machine learning fundamentals, covering supervised and unsupervised learning approaches with practical examples.",
                "content_type": "article",
                "tags": ["#tutorial", "#machinelearning", "#longread"],
                "projects": ["other_research"],
                "est_read_min": 15,
                "priority": "medium",
            }
        }


class EnrichmentResponse(BaseModel):
    """Response model for successful enrichment"""
    url: str = Field(..., description="URL that was enriched")
    enrichment: Enrichment = Field(..., description="Generated enrichment data")
    model_name: str = Field(..., description="LLM model used for enrichment")


class EnrichmentErrorResponse(BaseModel):
    """Response model for enrichment errors"""
    error: str = Field(..., description="Error message")
    detail: Optional[str] = Field(None, description="Detailed error information")
    url: Optional[str] = Field(None, description="URL that failed enrichment")
    raw_output: Optional[str] = Field(None, description="Raw LLM output if parsing failed")
    attempts: int = Field(default=1, description="Number of attempts made")


class HealthResponse(BaseModel):
    """Health check response"""
    status: str = Field(..., description="Service status")
    version: str = Field(..., description="Service version")
    model_name: str = Field(..., description="Configured LLM model")
    llm_status: str = Field(..., description="LLM connection status")
</file>

<file path="ingest/__init__.py">
"""
TabBacklog v1 - Ingest Module

This module provides functionality to parse Firefox bookmarks exports
and ingest them into the database.
"""

from .firefox_parser import FirefoxParser, BookmarkItem
from .db import IngestDB

__all__ = ["FirefoxParser", "BookmarkItem", "IngestDB"]
</file>

<file path="ingest/cli.py">
"""
TabBacklog v1 - Ingest CLI

Command-line interface for ingesting Firefox bookmarks into the database.

Usage:
    python -m ingest.cli --file ~/bookmarks.html --user-id UUID
    python -m ingest.cli stats --file ~/bookmarks.html
"""

import os
import sys
from pathlib import Path

import click
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn

from .firefox_parser import FirefoxParser
from .db import IngestDB

console = Console()


def get_database_url() -> str:
    """Get database URL from environment"""
    url = os.environ.get("DATABASE_URL")
    if not url:
        console.print("[red]Error:[/red] DATABASE_URL environment variable not set")
        console.print("Set it with: export DATABASE_URL=postgresql://user:pass@host/db")
        sys.exit(1)
    return url


@click.group()
@click.version_option(version="1.0.0")
def cli():
    """TabBacklog - Firefox Tab Ingest Tool"""
    pass


@cli.command()
@click.option(
    "--file", "-f",
    required=True,
    type=click.Path(exists=True, path_type=Path),
    help="Path to Firefox bookmarks HTML export file"
)
@click.option(
    "--user-id", "-u",
    required=True,
    help="User UUID for the tab owner"
)
@click.option(
    "--batch-size", "-b",
    default=100,
    type=int,
    help="Number of records to process per batch (default: 100)"
)
@click.option(
    "--dry-run", "-n",
    is_flag=True,
    help="Parse and show stats without writing to database"
)
def ingest(file: Path, user_id: str, batch_size: int, dry_run: bool):
    """
    Ingest Firefox bookmarks into the database.

    Parses the Firefox bookmarks HTML export and inserts tabs from
    Session-* folders into the database.
    """
    console.print(f"\n[bold blue]TabBacklog Ingest[/bold blue]")
    console.print(f"File: {file}")
    console.print(f"User ID: {user_id}")
    console.print()

    # Parse the bookmarks file
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        task = progress.add_task("Parsing bookmarks file...", total=None)

        try:
            parser = FirefoxParser(file)
            stats = parser.get_stats()
            bookmarks = parser.parse()
        except Exception as e:
            console.print(f"[red]Error parsing file:[/red] {e}")
            sys.exit(1)

        progress.update(task, completed=True)

    # Display parsing results
    console.print(f"[green]Parsed successfully![/green]")
    console.print(f"  Session folders found: {stats['total_session_folders']}")
    console.print(f"  Total bookmarks: {stats['total_bookmarks']}")
    console.print()

    # Show folder breakdown
    if stats['session_folders']:
        table = Table(title="Session Folders")
        table.add_column("Window Label", style="cyan")
        table.add_column("Bookmark Count", justify="right", style="green")

        for folder in stats['session_folders']:
            table.add_row(folder['label'], str(folder['count']))

        console.print(table)
        console.print()

    if not bookmarks:
        console.print("[yellow]No bookmarks found in Session-* folders.[/yellow]")
        console.print("Make sure your bookmarks are organized in folders with 'Session-' prefix.")
        return

    if dry_run:
        console.print("[yellow]Dry run mode - no changes written to database[/yellow]")
        return

    # Ingest into database
    database_url = get_database_url()

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        task = progress.add_task(
            f"Ingesting {len(bookmarks)} bookmarks...",
            total=None
        )

        try:
            db = IngestDB(database_url)
            result = db.ingest_bookmarks(bookmarks, user_id, batch_size)
        except Exception as e:
            console.print(f"[red]Database error:[/red] {e}")
            sys.exit(1)

        progress.update(task, completed=True)

    # Display results
    console.print()
    console.print("[bold green]Ingest Complete![/bold green]")

    results_table = Table(title="Results")
    results_table.add_column("Metric", style="cyan")
    results_table.add_column("Count", justify="right", style="green")

    results_table.add_row("Total processed", str(result.total_processed))
    results_table.add_row("New tabs inserted", str(result.inserted))
    results_table.add_row("Duplicates skipped", str(result.skipped_duplicates))
    results_table.add_row("Errors", str(result.errors))

    console.print(results_table)

    if result.error_messages:
        console.print()
        console.print("[yellow]Errors encountered:[/yellow]")
        for error in result.error_messages[:10]:  # Show first 10 errors
            console.print(f"  - {error}")
        if len(result.error_messages) > 10:
            console.print(f"  ... and {len(result.error_messages) - 10} more")

    # Show current totals
    try:
        total_tabs = db.get_user_tab_count(user_id)
        summary = db.get_ingest_summary(user_id)

        console.print()
        console.print(f"[bold]User now has {total_tabs} total tabs[/bold]")

        if summary:
            status_table = Table(title="Tabs by Status")
            status_table.add_column("Status", style="cyan")
            status_table.add_column("Count", justify="right", style="green")

            for status, count in sorted(summary.items()):
                status_table.add_row(status, str(count))

            console.print(status_table)
    except Exception:
        pass  # Non-critical, just skip the summary


@cli.command()
@click.option(
    "--file", "-f",
    required=True,
    type=click.Path(exists=True, path_type=Path),
    help="Path to Firefox bookmarks HTML export file"
)
def stats(file: Path):
    """
    Show statistics about a bookmarks file without ingesting.

    Useful for previewing what would be imported.
    """
    console.print(f"\n[bold blue]Bookmarks File Statistics[/bold blue]")
    console.print(f"File: {file}")
    console.print()

    try:
        parser = FirefoxParser(file)
        stats = parser.get_stats()
    except Exception as e:
        console.print(f"[red]Error parsing file:[/red] {e}")
        sys.exit(1)

    console.print(f"Total Session folders: {stats['total_session_folders']}")
    console.print(f"Total bookmarks in Session folders: {stats['total_bookmarks']}")
    console.print()

    if stats['session_folders']:
        table = Table(title="Session Folders Breakdown")
        table.add_column("Window Label", style="cyan")
        table.add_column("Bookmark Count", justify="right", style="green")

        for folder in stats['session_folders']:
            table.add_row(folder['label'], str(folder['count']))

        console.print(table)
    else:
        console.print("[yellow]No Session-* folders found in the bookmarks file.[/yellow]")
        console.print()
        console.print("Expected folder structure:")
        console.print("  Session-Research")
        console.print("    └── bookmark1")
        console.print("    └── bookmark2")
        console.print("  Session-Work")
        console.print("    └── bookmark3")


@cli.command()
@click.option(
    "--user-id", "-u",
    required=True,
    help="User UUID to query"
)
def status(user_id: str):
    """
    Show current database status for a user.
    """
    database_url = get_database_url()

    try:
        db = IngestDB(database_url)
        total = db.get_user_tab_count(user_id)
        summary = db.get_ingest_summary(user_id)
    except Exception as e:
        console.print(f"[red]Database error:[/red] {e}")
        sys.exit(1)

    console.print(f"\n[bold blue]Database Status[/bold blue]")
    console.print(f"User ID: {user_id}")
    console.print(f"Total active tabs: {total}")
    console.print()

    if summary:
        table = Table(title="Tabs by Status")
        table.add_column("Status", style="cyan")
        table.add_column("Count", justify="right", style="green")

        for status_name, count in sorted(summary.items()):
            table.add_row(status_name, str(count))

        console.print(table)
    else:
        console.print("[yellow]No tabs found for this user.[/yellow]")


def main():
    """Entry point for the CLI"""
    cli()


if __name__ == "__main__":
    main()
</file>

<file path="ingest/db.py">
"""
TabBacklog v1 - Database Operations for Ingest

Provides database operations for ingesting parsed bookmarks into the database.
Handles upserts, deduplication, and event logging.
"""

import json
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime
from typing import Iterator
from uuid import UUID

import psycopg
from psycopg.rows import dict_row

from .firefox_parser import BookmarkItem


@dataclass
class IngestResult:
    """Result of an ingest operation"""
    total_processed: int = 0
    inserted: int = 0
    skipped_duplicates: int = 0
    errors: int = 0
    error_messages: list[str] | None = None

    def __post_init__(self):
        if self.error_messages is None:
            self.error_messages = []


class IngestDB:
    """
    Database operations for ingesting tab items.

    Handles connection management, upserts with deduplication,
    and event logging.
    """

    def __init__(self, database_url: str):
        """
        Initialize the database connection.

        Args:
            database_url: PostgreSQL connection string
        """
        self.database_url = database_url
        self._conn: psycopg.Connection | None = None

    @contextmanager
    def connection(self) -> Iterator[psycopg.Connection]:
        """Context manager for database connections"""
        conn = psycopg.connect(self.database_url, row_factory=dict_row)
        try:
            yield conn
        finally:
            conn.close()

    def ingest_bookmarks(
        self,
        bookmarks: list[BookmarkItem],
        user_id: str | UUID,
        batch_size: int = 100,
    ) -> IngestResult:
        """
        Ingest a list of bookmarks into the database.

        Performs upsert operations, deduplicating on (user_id, url).
        Logs events for each operation.

        Args:
            bookmarks: List of BookmarkItem objects to ingest
            user_id: UUID of the user owning these tabs
            batch_size: Number of records to process per transaction

        Returns:
            IngestResult with counts of processed, inserted, skipped records
        """
        user_id_str = str(user_id)
        result = IngestResult()

        with self.connection() as conn:
            # Process in batches
            for i in range(0, len(bookmarks), batch_size):
                batch = bookmarks[i : i + batch_size]
                batch_result = self._process_batch(conn, batch, user_id_str)

                result.total_processed += batch_result.total_processed
                result.inserted += batch_result.inserted
                result.skipped_duplicates += batch_result.skipped_duplicates
                result.errors += batch_result.errors
                if batch_result.error_messages:
                    result.error_messages.extend(batch_result.error_messages)

        return result

    def _process_batch(
        self,
        conn: psycopg.Connection,
        bookmarks: list[BookmarkItem],
        user_id: str,
    ) -> IngestResult:
        """Process a batch of bookmarks in a single transaction"""
        result = IngestResult()

        with conn.cursor() as cur:
            for bookmark in bookmarks:
                result.total_processed += 1

                try:
                    inserted = self._upsert_tab(cur, bookmark, user_id)
                    if inserted:
                        result.inserted += 1
                        self._log_event(
                            cur,
                            user_id=user_id,
                            event_type="tab_created",
                            entity_type="tab_item",
                            details={
                                "url": bookmark.url,
                                "source": "firefox_bookmarks_import",
                            },
                        )
                    else:
                        result.skipped_duplicates += 1
                        self._log_event(
                            cur,
                            user_id=user_id,
                            event_type="tab_duplicate_skipped",
                            entity_type="tab_item",
                            details={
                                "url": bookmark.url,
                                "source": "firefox_bookmarks_import",
                            },
                        )
                except Exception as e:
                    result.errors += 1
                    result.error_messages.append(f"Error processing {bookmark.url}: {e}")
                    # Continue processing other bookmarks
                    continue

            conn.commit()

        return result

    def _upsert_tab(
        self,
        cur: psycopg.Cursor,
        bookmark: BookmarkItem,
        user_id: str,
    ) -> bool:
        """
        Upsert a single tab item.

        Returns True if a new record was inserted, False if it was a duplicate.
        """
        # Use INSERT ... ON CONFLICT to handle duplicates
        # Only insert if no existing active record (deleted_at IS NULL)
        query = """
            INSERT INTO tab_item (
                user_id,
                url,
                page_title,
                window_label,
                collected_at,
                status,
                created_at,
                updated_at
            )
            VALUES (
                %(user_id)s,
                %(url)s,
                %(page_title)s,
                %(window_label)s,
                %(collected_at)s,
                'new',
                now(),
                now()
            )
            ON CONFLICT (user_id, url) WHERE deleted_at IS NULL
            DO UPDATE SET
                -- Only update if the existing record has less info
                page_title = COALESCE(tab_item.page_title, EXCLUDED.page_title),
                window_label = COALESCE(tab_item.window_label, EXCLUDED.window_label),
                updated_at = now()
            RETURNING (xmax = 0) AS inserted
        """

        cur.execute(
            query,
            {
                "user_id": user_id,
                "url": bookmark.url,
                "page_title": bookmark.page_title,
                "window_label": bookmark.window_label,
                "collected_at": bookmark.collected_at,
            },
        )

        row = cur.fetchone()
        # xmax = 0 means it was an INSERT, not an UPDATE
        return row["inserted"] if row else False

    def _log_event(
        self,
        cur: psycopg.Cursor,
        user_id: str,
        event_type: str,
        entity_type: str | None = None,
        entity_id: int | None = None,
        details: dict | None = None,
    ) -> None:
        """Log an event to the event_log table"""
        query = """
            INSERT INTO event_log (
                user_id,
                event_type,
                entity_type,
                entity_id,
                details,
                created_at
            )
            VALUES (
                %(user_id)s,
                %(event_type)s,
                %(entity_type)s,
                %(entity_id)s,
                %(details)s,
                now()
            )
        """

        cur.execute(
            query,
            {
                "user_id": user_id,
                "event_type": event_type,
                "entity_type": entity_type,
                "entity_id": entity_id,
                "details": json.dumps(details or {}),
            },
        )

    def get_user_tab_count(self, user_id: str | UUID) -> int:
        """Get the total count of active tabs for a user"""
        with self.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT COUNT(*) as count
                    FROM tab_item
                    WHERE user_id = %s AND deleted_at IS NULL
                    """,
                    (str(user_id),),
                )
                row = cur.fetchone()
                return row["count"] if row else 0

    def get_ingest_summary(self, user_id: str | UUID) -> dict:
        """Get a summary of tabs for a user grouped by status"""
        with self.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT status, COUNT(*) as count
                    FROM tab_item
                    WHERE user_id = %s AND deleted_at IS NULL
                    GROUP BY status
                    ORDER BY status
                    """,
                    (str(user_id),),
                )
                rows = cur.fetchall()
                return {row["status"]: row["count"] for row in rows}
</file>

<file path="ingest/firefox_parser.py">
"""
TabBacklog v1 - Firefox Bookmarks Parser

Parses Firefox bookmarks HTML export files to extract tab information.
Specifically looks for "Session-" prefixed folders which contain saved browser tabs.
"""

from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Iterator
from bs4 import BeautifulSoup, Tag


@dataclass
class BookmarkItem:
    """Represents a single bookmark/tab extracted from Firefox export"""
    url: str
    page_title: str | None
    window_label: str | None
    collected_at: datetime = field(default_factory=datetime.utcnow)

    def __post_init__(self):
        # Normalize URL by stripping whitespace
        self.url = self.url.strip()
        # Normalize title
        if self.page_title:
            self.page_title = self.page_title.strip() or None


class FirefoxParser:
    """
    Parser for Firefox bookmarks HTML export files.

    Looks for folders with "Session-" prefix and extracts all bookmarks
    within them. The folder name after "Session-" is used as the window_label.

    Example folder structure:
        Session-Research
            - bookmark1
            - bookmark2
        Session-Work
            - bookmark3
    """

    SESSION_PREFIX = "Session-"

    def __init__(self, file_path: str | Path):
        self.file_path = Path(file_path)
        if not self.file_path.exists():
            raise FileNotFoundError(f"Bookmarks file not found: {self.file_path}")

    def parse(self) -> list[BookmarkItem]:
        """
        Parse the bookmarks file and return all tab items from Session- folders.

        Returns:
            List of BookmarkItem objects
        """
        return list(self._iter_bookmarks())

    def _iter_bookmarks(self) -> Iterator[BookmarkItem]:
        """
        Iterate over all bookmarks in Session- folders.

        Yields:
            BookmarkItem objects for each bookmark found
        """
        html_content = self.file_path.read_text(encoding="utf-8")
        soup = BeautifulSoup(html_content, "html.parser")

        # Find all H3 tags (folder headers)
        for h3 in soup.find_all("h3"):
            folder_name = h3.get_text(strip=True)

            # Check if this is a Session- folder
            if not folder_name.startswith(self.SESSION_PREFIX):
                continue

            # Extract window label (everything after "Session-")
            window_label = folder_name[len(self.SESSION_PREFIX):]
            if not window_label:
                window_label = "default"

            # Find the DL element that follows this H3 (contains bookmarks)
            dl = self._find_folder_contents(h3)
            if dl is None:
                continue

            # Extract all bookmarks from this folder
            yield from self._extract_bookmarks_from_dl(dl, window_label)

    def _find_folder_contents(self, h3_tag: Tag) -> Tag | None:
        """
        Find the DL element containing bookmarks for a folder.

        In Firefox bookmark HTML format:
        <DT>
            <H3>Folder Name</H3>
            <DL><p>
                ... bookmarks ...
            </DL>
        </DT>
        """
        # The H3 is inside a DT, and the DL follows it
        dt_parent = h3_tag.find_parent("dt")
        if dt_parent is None:
            return None

        # Find the DL that is a sibling after the H3
        for sibling in h3_tag.find_next_siblings():
            if sibling.name == "dl":
                return sibling

        return None

    def _extract_bookmarks_from_dl(
        self,
        dl: Tag,
        window_label: str
    ) -> Iterator[BookmarkItem]:
        """
        Extract all bookmark links from a DL element.

        Args:
            dl: The DL tag containing bookmarks
            window_label: The window/session label for these bookmarks

        Yields:
            BookmarkItem objects
        """
        # Find all anchor tags with href
        for a_tag in dl.find_all("a", href=True):
            url = a_tag.get("href", "").strip()

            # Skip empty URLs and non-http(s) URLs
            if not url or not url.startswith(("http://", "https://")):
                continue

            title = a_tag.get_text(strip=True) or None

            # Try to get add_date attribute if present
            add_date_str = a_tag.get("add_date")
            if add_date_str:
                try:
                    # Firefox stores timestamps in seconds since epoch
                    timestamp = int(add_date_str)
                    collected_at = datetime.utcfromtimestamp(timestamp)
                except (ValueError, OSError):
                    collected_at = datetime.utcnow()
            else:
                collected_at = datetime.utcnow()

            yield BookmarkItem(
                url=url,
                page_title=title,
                window_label=window_label,
                collected_at=collected_at,
            )

    def get_stats(self) -> dict:
        """
        Get statistics about the bookmarks file without fully parsing.

        Returns:
            Dictionary with stats like total_bookmarks, session_folders, etc.
        """
        html_content = self.file_path.read_text(encoding="utf-8")
        soup = BeautifulSoup(html_content, "html.parser")

        session_folders = []
        total_bookmarks = 0

        for h3 in soup.find_all("h3"):
            folder_name = h3.get_text(strip=True)
            if folder_name.startswith(self.SESSION_PREFIX):
                window_label = folder_name[len(self.SESSION_PREFIX):] or "default"
                dl = self._find_folder_contents(h3)
                if dl:
                    count = len([
                        a for a in dl.find_all("a", href=True)
                        if a.get("href", "").startswith(("http://", "https://"))
                    ])
                    session_folders.append({"label": window_label, "count": count})
                    total_bookmarks += count

        return {
            "file_path": str(self.file_path),
            "session_folders": session_folders,
            "total_session_folders": len(session_folders),
            "total_bookmarks": total_bookmarks,
        }


def parse_bookmarks_file(file_path: str | Path) -> list[BookmarkItem]:
    """
    Convenience function to parse a Firefox bookmarks file.

    Args:
        file_path: Path to the Firefox bookmarks HTML file

    Returns:
        List of BookmarkItem objects from Session- folders
    """
    parser = FirefoxParser(file_path)
    return parser.parse()
</file>

<file path="n8n/workflows/enrich_tabs.json">
{
  "name": "TabBacklog - Enrich Tabs",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "minutes",
              "minutesInterval": 10
            }
          ]
        }
      },
      "id": "cron-trigger",
      "name": "Every 10 Minutes",
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.1,
      "position": [0, 0]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT id, user_id, url, page_title, window_label, status\nFROM tab_item\nWHERE status = 'new'\n  AND deleted_at IS NULL\nORDER BY created_at ASC\nLIMIT {{ $json.batch_size || 10 }}",
        "options": {}
      },
      "id": "get-new-tabs",
      "name": "Get New Tabs",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [220, 0],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "check-has-tabs",
              "leftValue": "={{ $json.id }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "exists"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "check-tabs-exist",
      "name": "Has Tabs?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [440, 0]
    },
    {
      "parameters": {
        "batchSize": 2,
        "options": {}
      },
      "id": "split-batches",
      "name": "Split In Batches",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [660, -100]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE tab_item\nSET status = 'fetch_pending', updated_at = now()\nWHERE id = {{ $json.id }}\nRETURNING id, url, page_title",
        "options": {}
      },
      "id": "update-fetch-pending",
      "name": "Set Fetch Pending",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [880, -100],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $env.PARSER_SERVICE_URL }}/fetch_parse",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"url\": \"{{ $json.url }}\",\n  \"timeout\": 30\n}",
        "options": {
          "timeout": 60000
        }
      },
      "id": "call-parser",
      "name": "Call Parser Service",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [1100, -100],
      "continueOnFail": true
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "check-parse-success",
              "leftValue": "={{ $json.site_kind }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "exists"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "check-parse-success",
      "name": "Parse Success?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [1320, -100]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO tab_parsed (tab_id, site_kind, title_extracted, text_full, word_count, video_seconds, metadata)\nVALUES (\n  {{ $('Set Fetch Pending').item.json.id }},\n  '{{ $json.site_kind }}',\n  {{ $json.title ? \"'\" + $json.title.replace(/'/g, \"''\") + \"'\" : \"NULL\" }},\n  {{ $json.text_full ? \"'\" + $json.text_full.replace(/'/g, \"''\").substring(0, 50000) + \"'\" : \"NULL\" }},\n  {{ $json.word_count || \"NULL\" }},\n  {{ $json.video_seconds || \"NULL\" }},\n  '{{ JSON.stringify($json.metadata || {}) }}'\n)\nON CONFLICT (tab_id) DO UPDATE SET\n  site_kind = EXCLUDED.site_kind,\n  title_extracted = EXCLUDED.title_extracted,\n  text_full = EXCLUDED.text_full,\n  word_count = EXCLUDED.word_count,\n  video_seconds = EXCLUDED.video_seconds,\n  metadata = EXCLUDED.metadata,\n  updated_at = now()",
        "options": {}
      },
      "id": "insert-parsed",
      "name": "Insert Parsed Content",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [1540, -200],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE tab_item\nSET status = 'parsed', updated_at = now()\nWHERE id = {{ $('Set Fetch Pending').item.json.id }};\n\nINSERT INTO event_log (user_id, event_type, entity_type, entity_id, details)\nSELECT user_id, 'fetch_success', 'tab_item', id, '{\"source\": \"n8n_workflow\"}'::jsonb\nFROM tab_item WHERE id = {{ $('Set Fetch Pending').item.json.id }};",
        "options": {}
      },
      "id": "update-parsed-status",
      "name": "Set Status Parsed",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [1760, -200],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE tab_item\nSET status = 'fetch_error',\n    last_error = '{{ $json.error || \"Unknown fetch error\" }}',\n    error_at = now(),\n    updated_at = now()\nWHERE id = {{ $('Set Fetch Pending').item.json.id }};\n\nINSERT INTO event_log (user_id, event_type, entity_type, entity_id, details)\nSELECT user_id, 'fetch_error', 'tab_item', id, \n  jsonb_build_object('error', '{{ $json.error || \"Unknown error\" }}', 'source', 'n8n_workflow')\nFROM tab_item WHERE id = {{ $('Set Fetch Pending').item.json.id }};",
        "options": {}
      },
      "id": "update-fetch-error",
      "name": "Set Fetch Error",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [1540, 0],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE tab_item\nSET status = 'llm_pending', updated_at = now()\nWHERE id = {{ $('Set Fetch Pending').item.json.id }}\nRETURNING id, url, page_title",
        "options": {}
      },
      "id": "update-llm-pending",
      "name": "Set LLM Pending",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [1980, -200],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $env.ENRICHMENT_SERVICE_URL }}/enrich_tab",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"url\": \"{{ $('Set Fetch Pending').item.json.url }}\",\n  \"title\": {{ $('Call Parser Service').item.json.title ? '\"' + $('Call Parser Service').item.json.title.replace(/\"/g, '\\\\\"') + '\"' : 'null' }},\n  \"site_kind\": \"{{ $('Call Parser Service').item.json.site_kind }}\",\n  \"text\": {{ $('Call Parser Service').item.json.text_full ? '\"' + $('Call Parser Service').item.json.text_full.replace(/\"/g, '\\\\\"').replace(/\\n/g, '\\\\n').substring(0, 8000) + '\"' : 'null' }},\n  \"word_count\": {{ $('Call Parser Service').item.json.word_count || 0 }},\n  \"video_seconds\": {{ $('Call Parser Service').item.json.video_seconds || 0 }}\n}",
        "options": {
          "timeout": 120000
        }
      },
      "id": "call-enrichment",
      "name": "Call Enrichment Service",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [2200, -200],
      "continueOnFail": true
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "check-enrich-success",
              "leftValue": "={{ $json.enrichment }}",
              "rightValue": "",
              "operator": {
                "type": "object",
                "operation": "exists"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "check-enrich-success",
      "name": "Enrich Success?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [2420, -200]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO tab_enrichment (tab_id, summary, content_type, est_read_min, priority, raw_meta, model_name)\nVALUES (\n  {{ $('Set Fetch Pending').item.json.id }},\n  '{{ $json.enrichment.summary.replace(/'/g, \"''\") }}',\n  '{{ $json.enrichment.content_type }}',\n  {{ $json.enrichment.est_read_min || \"NULL\" }},\n  {{ $json.enrichment.priority ? \"'\" + $json.enrichment.priority + \"'\" : \"NULL\" }},\n  '{{ JSON.stringify({tags: $json.enrichment.tags, projects: $json.enrichment.projects}) }}',\n  '{{ $json.model_name }}'\n)\nON CONFLICT (tab_id) DO UPDATE SET\n  summary = EXCLUDED.summary,\n  content_type = EXCLUDED.content_type,\n  est_read_min = EXCLUDED.est_read_min,\n  priority = EXCLUDED.priority,\n  raw_meta = EXCLUDED.raw_meta,\n  model_name = EXCLUDED.model_name,\n  updated_at = now();",
        "options": {}
      },
      "id": "upsert-enrichment",
      "name": "Upsert Enrichment",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [2640, -300],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO tab_enrichment_history (tab_id, summary, content_type, est_read_min, priority, raw_meta, model_name, run_started_at, run_finished_at)\nVALUES (\n  {{ $('Set Fetch Pending').item.json.id }},\n  '{{ $json.enrichment.summary.replace(/'/g, \"''\") }}',\n  '{{ $json.enrichment.content_type }}',\n  {{ $json.enrichment.est_read_min || \"NULL\" }},\n  {{ $json.enrichment.priority ? \"'\" + $json.enrichment.priority + \"'\" : \"NULL\" }},\n  '{{ JSON.stringify({tags: $json.enrichment.tags, projects: $json.enrichment.projects}) }}',\n  '{{ $json.model_name }}',\n  now() - interval '2 seconds',\n  now()\n);",
        "options": {}
      },
      "id": "insert-history",
      "name": "Insert History",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [2860, -300],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "-- Upsert tags from enrichment\nWITH tag_data AS (\n  SELECT unnest(ARRAY{{ JSON.stringify($json.enrichment.tags || []) }}::text[]) as tag_name\n),\nuser_info AS (\n  SELECT user_id FROM tab_item WHERE id = {{ $('Set Fetch Pending').item.json.id }}\n),\ninserted_tags AS (\n  INSERT INTO tag (user_id, name, kind)\n  SELECT u.user_id, t.tag_name, 'auto'\n  FROM tag_data t, user_info u\n  ON CONFLICT (user_id, name) WHERE deleted_at IS NULL DO NOTHING\n  RETURNING id, name\n),\nall_tags AS (\n  SELECT id, name FROM inserted_tags\n  UNION\n  SELECT t.id, t.name FROM tag t, user_info u, tag_data td\n  WHERE t.user_id = u.user_id AND t.name = td.tag_name AND t.deleted_at IS NULL\n)\nINSERT INTO tab_tag (tab_id, tag_id)\nSELECT {{ $('Set Fetch Pending').item.json.id }}, at.id\nFROM all_tags at\nON CONFLICT (tab_id, tag_id) DO NOTHING;",
        "options": {}
      },
      "id": "upsert-tags",
      "name": "Upsert Tags",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [3080, -300],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE tab_item\nSET status = 'enriched', updated_at = now()\nWHERE id = {{ $('Set Fetch Pending').item.json.id }};\n\nINSERT INTO event_log (user_id, event_type, entity_type, entity_id, details)\nSELECT user_id, 'llm_enrich_success', 'tab_item', id,\n  jsonb_build_object(\n    'model', '{{ $json.model_name }}',\n    'content_type', '{{ $json.enrichment.content_type }}',\n    'source', 'n8n_workflow'\n  )\nFROM tab_item WHERE id = {{ $('Set Fetch Pending').item.json.id }};",
        "options": {}
      },
      "id": "update-enriched-status",
      "name": "Set Status Enriched",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [3300, -300],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "UPDATE tab_item\nSET status = 'llm_error',\n    last_error = '{{ ($json.error || $json.detail || \"Unknown LLM error\").replace(/'/g, \"''\") }}',\n    error_at = now(),\n    updated_at = now()\nWHERE id = {{ $('Set Fetch Pending').item.json.id }};\n\nINSERT INTO event_log (user_id, event_type, entity_type, entity_id, details)\nSELECT user_id, 'llm_enrich_error', 'tab_item', id,\n  jsonb_build_object(\n    'error', '{{ ($json.error || \"Unknown error\").replace(/'/g, \"''\") }}',\n    'source', 'n8n_workflow'\n  )\nFROM tab_item WHERE id = {{ $('Set Fetch Pending').item.json.id }};",
        "options": {}
      },
      "id": "update-llm-error",
      "name": "Set LLM Error",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.3,
      "position": [2640, -100],
      "credentials": {
        "postgres": {
          "id": "postgres-cred",
          "name": "TabBacklog Postgres"
        }
      }
    },
    {
      "parameters": {},
      "id": "no-op-end",
      "name": "No Tabs to Process",
      "type": "n8n-nodes-base.noOp",
      "typeVersion": 1,
      "position": [660, 100]
    },
    {
      "parameters": {
        "values": {
          "number": [
            {
              "name": "batch_size",
              "value": 10
            }
          ]
        },
        "options": {}
      },
      "id": "set-config",
      "name": "Set Config",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.3,
      "position": [220, 200]
    },
    {
      "parameters": {
        "mode": "combine",
        "combineBy": "combineAll",
        "options": {}
      },
      "id": "merge-for-loop",
      "name": "Merge Back",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [3520, -200]
    }
  ],
  "connections": {
    "Every 10 Minutes": {
      "main": [
        [
          {
            "node": "Get New Tabs",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Get New Tabs": {
      "main": [
        [
          {
            "node": "Has Tabs?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Tabs?": {
      "main": [
        [
          {
            "node": "Split In Batches",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "No Tabs to Process",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split In Batches": {
      "main": [
        [
          {
            "node": "Set Fetch Pending",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set Fetch Pending": {
      "main": [
        [
          {
            "node": "Call Parser Service",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Parser Service": {
      "main": [
        [
          {
            "node": "Parse Success?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Success?": {
      "main": [
        [
          {
            "node": "Insert Parsed Content",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Set Fetch Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert Parsed Content": {
      "main": [
        [
          {
            "node": "Set Status Parsed",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set Status Parsed": {
      "main": [
        [
          {
            "node": "Set LLM Pending",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set Fetch Error": {
      "main": [
        [
          {
            "node": "Merge Back",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set LLM Pending": {
      "main": [
        [
          {
            "node": "Call Enrichment Service",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Enrichment Service": {
      "main": [
        [
          {
            "node": "Enrich Success?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Enrich Success?": {
      "main": [
        [
          {
            "node": "Upsert Enrichment",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Set LLM Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upsert Enrichment": {
      "main": [
        [
          {
            "node": "Insert History",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert History": {
      "main": [
        [
          {
            "node": "Upsert Tags",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upsert Tags": {
      "main": [
        [
          {
            "node": "Set Status Enriched",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set Status Enriched": {
      "main": [
        [
          {
            "node": "Merge Back",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set LLM Error": {
      "main": [
        [
          {
            "node": "Merge Back",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Back": {
      "main": [
        [
          {
            "node": "Split In Batches",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [
    {
      "name": "tabbacklog",
      "createdAt": "2024-01-01T00:00:00.000Z",
      "updatedAt": "2024-01-01T00:00:00.000Z"
    }
  ],
  "triggerCount": 1,
  "active": false,
  "pinData": {}
}
</file>

<file path="parser_service/parsers/__init__.py">
"""
TabBacklog v1 - Parser Plugins

Site-specific parsers for extracting content from web pages.
"""

from .base import BaseParser, ParsedPage, ParserRegistry
from .generic import GenericHtmlParser
from .youtube import YouTubeParser
from .twitter import TwitterParser
from .registry import get_default_registry, parse_url

__all__ = [
    "BaseParser",
    "ParsedPage",
    "ParserRegistry",
    "GenericHtmlParser",
    "YouTubeParser",
    "TwitterParser",
    "get_default_registry",
    "parse_url",
]
</file>

<file path="parser_service/parsers/base.py">
"""
TabBacklog v1 - Parser Base Class and Registry

This module defines the base parser interface and registry system
for site-specific content parsers.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Optional, Type, Dict, List
import re
from urllib.parse import urlparse


@dataclass
class ParsedPage:
    """Structured representation of parsed page content"""
    
    site_kind: str  # youtube | twitter | generic_html | etc.
    title: Optional[str] = None
    text_full: Optional[str] = None
    word_count: Optional[int] = None
    video_seconds: Optional[int] = None
    metadata: Dict = field(default_factory=dict)
    
    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "site_kind": self.site_kind,
            "title": self.title,
            "text_full": self.text_full,
            "word_count": self.word_count,
            "video_seconds": self.video_seconds,
            "metadata": self.metadata,
        }


class BaseParser(ABC):
    """
    Abstract base class for site-specific parsers.
    
    Each parser must implement:
    - match(url): Return True if this parser can handle the URL
    - parse(url, html_content): Return ParsedPage with extracted content
    """
    
    @abstractmethod
    def match(self, url: str) -> bool:
        """
        Determine if this parser can handle the given URL.
        
        Args:
            url: The URL to check
            
        Returns:
            True if this parser should be used for this URL
        """
        pass
    
    @abstractmethod
    def parse(self, url: str, html_content: str) -> ParsedPage:
        """
        Parse the page content and extract relevant information.
        
        Args:
            url: The original URL
            html_content: The raw HTML content
            
        Returns:
            ParsedPage with extracted content
        """
        pass
    
    def extract_domain(self, url: str) -> str:
        """Helper to extract domain from URL"""
        parsed = urlparse(url)
        return parsed.netloc.lower()
    
    def count_words(self, text: Optional[str]) -> int:
        """Helper to count words in text"""
        if not text:
            return 0
        # Simple word count - split on whitespace
        return len(text.split())


class ParserRegistry:
    """
    Registry for managing parser instances.
    
    Parsers are checked in registration order, so register more specific
    parsers before generic ones.
    """
    
    def __init__(self):
        self._parsers: List[BaseParser] = []
    
    def register(self, parser: BaseParser) -> None:
        """
        Register a parser instance.
        
        Args:
            parser: Instance of a BaseParser subclass
        """
        self._parsers.append(parser)
    
    def get_parser(self, url: str) -> Optional[BaseParser]:
        """
        Find the first parser that matches the given URL.
        
        Args:
            url: The URL to find a parser for
            
        Returns:
            A parser instance if found, None otherwise
        """
        for parser in self._parsers:
            if parser.match(url):
                return parser
        return None
    
    def parse_page(self, url: str, html_content: str) -> ParsedPage:
        """
        Parse a page using the appropriate parser.
        
        Args:
            url: The URL of the page
            html_content: The raw HTML content
            
        Returns:
            ParsedPage with extracted content
            
        Raises:
            ValueError: If no parser matches the URL
        """
        parser = self.get_parser(url)
        if parser is None:
            raise ValueError(f"No parser found for URL: {url}")
        
        return parser.parse(url, html_content)
    
    def list_parsers(self) -> List[str]:
        """Get list of registered parser class names"""
        return [parser.__class__.__name__ for parser in self._parsers]


# Global registry instance
_global_registry = ParserRegistry()


def register_parser(parser: BaseParser) -> None:
    """
    Register a parser with the global registry.
    
    Args:
        parser: Instance of a BaseParser subclass
    """
    _global_registry.register(parser)


def get_parser(url: str) -> Optional[BaseParser]:
    """
    Get a parser for the given URL from the global registry.
    
    Args:
        url: The URL to find a parser for
        
    Returns:
        A parser instance if found, None otherwise
    """
    return _global_registry.get_parser(url)


def parse_page(url: str, html_content: str) -> ParsedPage:
    """
    Parse a page using the global registry.
    
    Args:
        url: The URL of the page
        html_content: The raw HTML content
        
    Returns:
        ParsedPage with extracted content
        
    Raises:
        ValueError: If no parser matches the URL
    """
    return _global_registry.parse_page(url, html_content)


def list_parsers() -> List[str]:
    """Get list of registered parsers from global registry"""
    return _global_registry.list_parsers()


# Example usage and testing
if __name__ == "__main__":
    # Example: Create a simple test parser
    class TestParser(BaseParser):
        def match(self, url: str) -> bool:
            return "example.com" in url
        
        def parse(self, url: str, html_content: str) -> ParsedPage:
            return ParsedPage(
                site_kind="test",
                title="Test Page",
                text_full="This is test content",
                word_count=4,
                metadata={"test": True}
            )
    
    # Register the test parser
    register_parser(TestParser())
    
    # Test
    test_url = "https://example.com/test"
    parser = get_parser(test_url)
    print(f"Parser for {test_url}: {parser.__class__.__name__ if parser else 'None'}")
    
    # List all parsers
    print(f"Registered parsers: {list_parsers()}")
</file>

<file path="parser_service/parsers/generic.py">
"""
TabBacklog v1 - Generic HTML Parser

Parses generic web pages to extract title, main content, and metadata.
Used as a fallback for URLs that don't match specific parsers.
"""

import re
from typing import Optional
from bs4 import BeautifulSoup, NavigableString

from .base import BaseParser, ParsedPage


class GenericHtmlParser(BaseParser):
    """
    Generic parser for HTML web pages.

    Extracts:
    - Page title from <title> tag
    - Main text content from <article>, <main>, or <p> tags
    - Word count
    - Basic metadata (description, author, etc.)
    """

    # Tags that typically contain main content
    CONTENT_TAGS = ["article", "main", "[role='main']"]

    # Tags to exclude from text extraction
    EXCLUDED_TAGS = {
        "script", "style", "nav", "header", "footer", "aside",
        "noscript", "iframe", "form", "button", "input", "select",
        "textarea", "svg", "canvas", "video", "audio"
    }

    def match(self, url: str) -> bool:
        """
        Generic parser matches all HTTP(S) URLs as a fallback.
        Should be registered last in the registry.
        """
        return url.startswith(("http://", "https://"))

    def parse(self, url: str, html_content: str) -> ParsedPage:
        """Parse HTML content and extract relevant information."""
        soup = BeautifulSoup(html_content, "html.parser")

        title = self._extract_title(soup)
        text_content = self._extract_text(soup)
        word_count = self.count_words(text_content)
        metadata = self._extract_metadata(soup, url)

        return ParsedPage(
            site_kind="generic_html",
            title=title,
            text_full=text_content,
            word_count=word_count,
            video_seconds=None,
            metadata=metadata,
        )

    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract page title from <title> or <h1> tag."""
        # Try <title> first
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True)
            if title:
                return self._clean_title(title)

        # Fall back to first <h1>
        h1_tag = soup.find("h1")
        if h1_tag:
            return h1_tag.get_text(strip=True)

        return None

    def _clean_title(self, title: str) -> str:
        """Clean up title by removing common suffixes."""
        # Remove common site name suffixes like " | Site Name" or " - Site Name"
        separators = [" | ", " - ", " – ", " — ", " :: "]
        for sep in separators:
            if sep in title:
                parts = title.split(sep)
                # Keep the first part if it's substantial
                if len(parts[0]) > 10:
                    title = parts[0]
                    break
        return title.strip()

    def _extract_text(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract main text content from the page."""
        # Remove excluded tags
        for tag in soup.find_all(self.EXCLUDED_TAGS):
            tag.decompose()

        # Try to find main content container
        main_content = None
        for selector in self.CONTENT_TAGS:
            if selector.startswith("["):
                # Attribute selector
                main_content = soup.select_one(selector)
            else:
                main_content = soup.find(selector)
            if main_content:
                break

        # Fall back to body if no main content found
        if not main_content:
            main_content = soup.find("body")

        if not main_content:
            return None

        # Extract text from paragraphs
        paragraphs = []
        for p in main_content.find_all(["p", "li", "h1", "h2", "h3", "h4", "h5", "h6"]):
            text = p.get_text(strip=True)
            if text and len(text) > 20:  # Filter out very short fragments
                paragraphs.append(text)

        if not paragraphs:
            # Fall back to all text
            text = main_content.get_text(separator=" ", strip=True)
            return self._clean_text(text) if text else None

        return "\n\n".join(paragraphs)

    def _clean_text(self, text: str) -> str:
        """Clean up extracted text."""
        # Normalize whitespace
        text = re.sub(r"\s+", " ", text)
        # Remove excessive punctuation
        text = re.sub(r"[.]{3,}", "...", text)
        return text.strip()

    def _extract_metadata(self, soup: BeautifulSoup, url: str) -> dict:
        """Extract metadata from meta tags."""
        metadata = {
            "url": url,
            "domain": self.extract_domain(url),
        }

        # Extract common meta tags
        meta_mappings = {
            "description": ["description", "og:description", "twitter:description"],
            "author": ["author", "og:author", "article:author"],
            "published": ["article:published_time", "datePublished", "date"],
            "image": ["og:image", "twitter:image"],
            "site_name": ["og:site_name"],
            "type": ["og:type"],
        }

        for key, names in meta_mappings.items():
            for name in names:
                # Try name attribute
                meta = soup.find("meta", attrs={"name": name})
                if meta and meta.get("content"):
                    metadata[key] = meta["content"]
                    break
                # Try property attribute (for Open Graph)
                meta = soup.find("meta", attrs={"property": name})
                if meta and meta.get("content"):
                    metadata[key] = meta["content"]
                    break

        # Extract canonical URL
        canonical = soup.find("link", attrs={"rel": "canonical"})
        if canonical and canonical.get("href"):
            metadata["canonical_url"] = canonical["href"]

        # Extract language
        html_tag = soup.find("html")
        if html_tag and html_tag.get("lang"):
            metadata["language"] = html_tag["lang"]

        return metadata
</file>

<file path="parser_service/parsers/registry.py">
"""
TabBacklog v1 - Parser Registry

Pre-configured parser registry with all available parsers.
Parsers are registered in order of specificity (most specific first).
"""

from typing import Optional
import httpx

from .base import ParsedPage, ParserRegistry
from .youtube import YouTubeParser
from .twitter import TwitterParser
from .generic import GenericHtmlParser


def get_default_registry() -> ParserRegistry:
    """
    Create and return a parser registry with all default parsers.

    Parsers are registered in order of specificity:
    1. YouTube (most specific)
    2. Twitter/X
    3. Generic HTML (fallback, matches everything)
    """
    registry = ParserRegistry()

    # Register specific parsers first
    registry.register(YouTubeParser())
    registry.register(TwitterParser())

    # Register generic parser last (fallback)
    registry.register(GenericHtmlParser())

    return registry


# Global default registry
_default_registry: Optional[ParserRegistry] = None


def get_registry() -> ParserRegistry:
    """Get the global default registry, creating it if necessary."""
    global _default_registry
    if _default_registry is None:
        _default_registry = get_default_registry()
    return _default_registry


async def fetch_url(url: str, timeout: float = 30.0) -> str:
    """
    Fetch HTML content from a URL.

    Args:
        url: The URL to fetch
        timeout: Request timeout in seconds

    Returns:
        HTML content as string

    Raises:
        httpx.HTTPError: If the request fails
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
    }

    async with httpx.AsyncClient(follow_redirects=True, timeout=timeout) as client:
        response = await client.get(url, headers=headers)
        response.raise_for_status()
        return response.text


async def parse_url(url: str, timeout: float = 30.0) -> ParsedPage:
    """
    Fetch and parse a URL using the appropriate parser.

    Args:
        url: The URL to fetch and parse
        timeout: Request timeout in seconds

    Returns:
        ParsedPage with extracted content

    Raises:
        ValueError: If no parser matches the URL
        httpx.HTTPError: If the request fails
    """
    registry = get_registry()

    # Fetch the HTML content
    html_content = await fetch_url(url, timeout)

    # Parse with the appropriate parser
    return registry.parse_page(url, html_content)


def parse_html(url: str, html_content: str) -> ParsedPage:
    """
    Parse HTML content using the appropriate parser.

    Args:
        url: The original URL (used for parser selection)
        html_content: The HTML content to parse

    Returns:
        ParsedPage with extracted content

    Raises:
        ValueError: If no parser matches the URL
    """
    registry = get_registry()
    return registry.parse_page(url, html_content)
</file>

<file path="parser_service/parsers/twitter.py">
"""
TabBacklog v1 - Twitter/X Parser

Parses Twitter (X) posts/tweets by extracting content from meta tags
and structured data.
"""

import re
from typing import Optional
from urllib.parse import urlparse
from bs4 import BeautifulSoup

from .base import BaseParser, ParsedPage


class TwitterParser(BaseParser):
    """
    Parser for Twitter/X posts.

    Extracts:
    - Tweet text from meta tags
    - Author information
    - Timestamp
    - Engagement metrics (if available)

    Note: Due to Twitter's heavy JavaScript usage, we rely on meta tags
    which are generally present for SEO/sharing purposes.
    """

    # Twitter/X domains
    TWITTER_DOMAINS = {"twitter.com", "www.twitter.com", "x.com", "www.x.com", "mobile.twitter.com"}

    # URL patterns for tweets/posts
    TWEET_PATTERNS = [
        r"(?:https?://)?(?:www\.|mobile\.)?(?:twitter|x)\.com/\w+/status/\d+",
    ]

    def match(self, url: str) -> bool:
        """Check if URL is a Twitter/X post."""
        domain = self.extract_domain(url)
        if domain not in self.TWITTER_DOMAINS:
            return False

        # Check if it's a status/post URL (not profile, search, etc.)
        for pattern in self.TWEET_PATTERNS:
            if re.match(pattern, url):
                return True

        # Also match if path contains /status/
        parsed = urlparse(url)
        return "/status/" in parsed.path

    def parse(self, url: str, html_content: str) -> ParsedPage:
        """Parse Twitter/X post from HTML content."""
        soup = BeautifulSoup(html_content, "html.parser")

        title = self._extract_title(soup)
        text_content = self._extract_tweet_text(soup)
        author = self._extract_author(soup, url)
        metadata = self._extract_metadata(soup, url, author)

        # Combine title and text for full content
        text_full = text_content or title

        return ParsedPage(
            site_kind="twitter",
            title=title,
            text_full=text_full,
            word_count=self.count_words(text_full),
            video_seconds=None,
            metadata=metadata,
        )

    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract title from Twitter page."""
        # Try og:title first
        og_title = soup.find("meta", attrs={"property": "og:title"})
        if og_title and og_title.get("content"):
            return og_title["content"]

        # Try twitter:title
        tw_title = soup.find("meta", attrs={"name": "twitter:title"})
        if tw_title and tw_title.get("content"):
            return tw_title["content"]

        # Fall back to page title
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True)
            # Clean up Twitter title format
            if " / X" in title:
                title = title.replace(" / X", "")
            elif " / Twitter" in title:
                title = title.replace(" / Twitter", "")
            return title

        return None

    def _extract_tweet_text(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract the actual tweet text."""
        # Try og:description - usually contains the tweet
        og_desc = soup.find("meta", attrs={"property": "og:description"})
        if og_desc and og_desc.get("content"):
            text = og_desc["content"]
            # Twitter often wraps text in quotes for og:description
            if text.startswith('"') and text.endswith('"'):
                text = text[1:-1]
            return text

        # Try twitter:description
        tw_desc = soup.find("meta", attrs={"name": "twitter:description"})
        if tw_desc and tw_desc.get("content"):
            return tw_desc["content"]

        # Try standard description
        desc = soup.find("meta", attrs={"name": "description"})
        if desc and desc.get("content"):
            return desc["content"]

        return None

    def _extract_author(self, soup: BeautifulSoup, url: str) -> dict:
        """Extract author information."""
        author = {}

        # Extract username from URL
        parsed = urlparse(url)
        path_parts = parsed.path.strip("/").split("/")
        if path_parts:
            author["username"] = path_parts[0]

        # Try to get display name from title or meta
        # Title format is often: "Display Name on X: "tweet text""
        title_tag = soup.find("title")
        if title_tag:
            title_text = title_tag.get_text(strip=True)
            # Pattern: "Display Name on X: ..." or "Display Name (@username) / X"
            match = re.match(r'^([^(@]+?)(?:\s+on\s+(?:X|Twitter):|(?:\s*\(@\w+\)\s*/\s*(?:X|Twitter)))', title_text)
            if match:
                author["display_name"] = match.group(1).strip()

        # Try twitter:creator meta tag
        creator = soup.find("meta", attrs={"name": "twitter:creator"})
        if creator and creator.get("content"):
            author["twitter_handle"] = creator["content"]

        return author

    def _extract_metadata(self, soup: BeautifulSoup, url: str, author: dict) -> dict:
        """Extract metadata from the page."""
        metadata = {
            "url": url,
            "domain": self.extract_domain(url),
            "platform": "twitter" if "twitter.com" in url else "x",
        }

        # Add author info
        if author:
            metadata["author"] = author

        # Extract tweet ID from URL
        tweet_id = self._extract_tweet_id(url)
        if tweet_id:
            metadata["tweet_id"] = tweet_id

        # Try to get image
        og_image = soup.find("meta", attrs={"property": "og:image"})
        if og_image and og_image.get("content"):
            metadata["image"] = og_image["content"]

        # Try to get site name
        og_site = soup.find("meta", attrs={"property": "og:site_name"})
        if og_site and og_site.get("content"):
            metadata["site_name"] = og_site["content"]

        # Check for media type indicators
        og_type = soup.find("meta", attrs={"property": "og:type"})
        if og_type and og_type.get("content"):
            metadata["content_type"] = og_type["content"]

        # Check for video content
        og_video = soup.find("meta", attrs={"property": "og:video"})
        if og_video:
            metadata["has_video"] = True

        # Check for card type
        tw_card = soup.find("meta", attrs={"name": "twitter:card"})
        if tw_card and tw_card.get("content"):
            metadata["card_type"] = tw_card["content"]

        return metadata

    def _extract_tweet_id(self, url: str) -> Optional[str]:
        """Extract tweet ID from URL."""
        # Pattern: /status/1234567890
        match = re.search(r"/status/(\d+)", url)
        return match.group(1) if match else None
</file>

<file path="parser_service/parsers/youtube.py">
"""
TabBacklog v1 - YouTube Parser

Parses YouTube video pages using yt-dlp to extract metadata.
"""

import json
import subprocess
import re
from typing import Optional
from urllib.parse import urlparse, parse_qs

from .base import BaseParser, ParsedPage


class YouTubeParser(BaseParser):
    """
    Parser for YouTube video pages.

    Uses yt-dlp to extract video metadata including:
    - Title
    - Description
    - Duration
    - Uploader/channel info
    - View count, likes
    - Upload date
    """

    # YouTube URL patterns
    YOUTUBE_PATTERNS = [
        r"(?:https?://)?(?:www\.)?youtube\.com/watch\?v=[\w-]+",
        r"(?:https?://)?(?:www\.)?youtube\.com/shorts/[\w-]+",
        r"(?:https?://)?youtu\.be/[\w-]+",
        r"(?:https?://)?(?:www\.)?youtube\.com/embed/[\w-]+",
        r"(?:https?://)?(?:www\.)?youtube\.com/v/[\w-]+",
    ]

    def __init__(self, timeout: int = 30):
        """
        Initialize YouTube parser.

        Args:
            timeout: Timeout in seconds for yt-dlp commands
        """
        self.timeout = timeout

    def match(self, url: str) -> bool:
        """Check if URL is a YouTube video."""
        for pattern in self.YOUTUBE_PATTERNS:
            if re.match(pattern, url):
                return True

        # Also check domain
        domain = self.extract_domain(url)
        return domain in ("youtube.com", "www.youtube.com", "youtu.be")

    def parse(self, url: str, html_content: str) -> ParsedPage:
        """
        Parse YouTube video using yt-dlp.

        Note: html_content is ignored - we use yt-dlp for reliable extraction.
        """
        try:
            video_info = self._fetch_video_info(url)
            return self._create_parsed_page(url, video_info)
        except Exception as e:
            # Fall back to basic HTML parsing if yt-dlp fails
            return self._fallback_parse(url, html_content, str(e))

    def _fetch_video_info(self, url: str) -> dict:
        """Fetch video metadata using yt-dlp."""
        cmd = [
            "yt-dlp",
            "--dump-json",
            "--no-download",
            "--no-playlist",
            "--no-warnings",
            url,
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=self.timeout,
        )

        if result.returncode != 0:
            raise RuntimeError(f"yt-dlp failed: {result.stderr}")

        return json.loads(result.stdout)

    def _create_parsed_page(self, url: str, info: dict) -> ParsedPage:
        """Create ParsedPage from yt-dlp JSON output."""
        title = info.get("title")
        description = info.get("description", "")
        duration = info.get("duration")  # in seconds

        # Build full text from title and description
        text_parts = []
        if title:
            text_parts.append(title)
        if description:
            text_parts.append(description)
        text_full = "\n\n".join(text_parts) if text_parts else None

        # Extract metadata
        metadata = {
            "url": url,
            "video_id": info.get("id"),
            "uploader": info.get("uploader"),
            "uploader_id": info.get("uploader_id"),
            "channel": info.get("channel"),
            "channel_id": info.get("channel_id"),
            "upload_date": info.get("upload_date"),
            "view_count": info.get("view_count"),
            "like_count": info.get("like_count"),
            "comment_count": info.get("comment_count"),
            "thumbnail": info.get("thumbnail"),
            "categories": info.get("categories", []),
            "tags": info.get("tags", []),
            "is_live": info.get("is_live", False),
            "was_live": info.get("was_live", False),
        }

        # Remove None values
        metadata = {k: v for k, v in metadata.items() if v is not None}

        return ParsedPage(
            site_kind="youtube",
            title=title,
            text_full=text_full,
            word_count=self.count_words(text_full),
            video_seconds=duration,
            metadata=metadata,
        )

    def _fallback_parse(self, url: str, html_content: str, error: str) -> ParsedPage:
        """Fallback parsing using HTML content if yt-dlp fails."""
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html_content, "html.parser")

        # Try to extract title
        title = None
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True)
            # Remove " - YouTube" suffix
            if title.endswith(" - YouTube"):
                title = title[:-10]

        # Try to extract description from meta
        description = None
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc:
            description = meta_desc.get("content")

        text_full = "\n\n".join(filter(None, [title, description]))

        # Try to extract video ID for metadata
        video_id = self._extract_video_id(url)

        metadata = {
            "url": url,
            "video_id": video_id,
            "parse_error": error,
            "fallback_used": True,
        }

        return ParsedPage(
            site_kind="youtube",
            title=title,
            text_full=text_full if text_full else None,
            word_count=self.count_words(text_full),
            video_seconds=None,  # Can't get duration from HTML easily
            metadata=metadata,
        )

    def _extract_video_id(self, url: str) -> Optional[str]:
        """Extract YouTube video ID from URL."""
        parsed = urlparse(url)

        # youtube.com/watch?v=VIDEO_ID
        if "youtube.com" in parsed.netloc:
            if parsed.path == "/watch":
                query = parse_qs(parsed.query)
                return query.get("v", [None])[0]
            # youtube.com/shorts/VIDEO_ID or /embed/VIDEO_ID
            match = re.match(r"^/(shorts|embed|v)/([^/?]+)", parsed.path)
            if match:
                return match.group(2)

        # youtu.be/VIDEO_ID
        if "youtu.be" in parsed.netloc:
            return parsed.path.lstrip("/").split("/")[0]

        return None
</file>

<file path="parser_service/__init__.py">
"""
TabBacklog v1 - Parser Service

FastAPI service for fetching and parsing web pages.
Supports generic HTML, YouTube, and Twitter content.
"""

__version__ = "1.0.0"
</file>

<file path="parser_service/Dockerfile">
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies including yt-dlp requirements
RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy parser service code
COPY parser_service/ ./parser_service/

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import httpx; httpx.get('http://localhost:8001/health').raise_for_status()"

# Run the service
CMD ["uvicorn", "parser_service.main:app", "--host", "0.0.0.0", "--port", "8001"]
</file>

<file path="parser_service/main.py">
"""
TabBacklog v1 - Parser Service

FastAPI service for fetching and parsing web pages.
Provides endpoints for parsing URLs with site-specific handlers.
"""

import logging
from contextlib import asynccontextmanager

import httpx
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from . import __version__
from .models import (
    FetchParseRequest,
    ParseHtmlRequest,
    ParsedPageResponse,
    ErrorResponse,
    HealthResponse,
)
from .parsers import get_default_registry, parse_url as do_parse_url
from .parsers.registry import parse_html, get_registry

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    # Initialize the parser registry on startup
    registry = get_registry()
    logger.info(f"Parser service starting with parsers: {registry.list_parsers()}")
    yield
    logger.info("Parser service shutting down")


app = FastAPI(
    title="TabBacklog Parser Service",
    description="Fetches and parses web pages to extract structured content",
    version=__version__,
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """Check service health and list available parsers."""
    registry = get_registry()
    return HealthResponse(
        status="healthy",
        version=__version__,
        parsers=registry.list_parsers(),
    )


@app.post(
    "/fetch_parse",
    response_model=ParsedPageResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        500: {"model": ErrorResponse, "description": "Parsing or fetch error"},
    },
    tags=["Parsing"],
)
async def fetch_and_parse(request: FetchParseRequest):
    """
    Fetch a URL and parse its content.

    The appropriate parser is automatically selected based on the URL:
    - YouTube URLs: Uses yt-dlp for metadata extraction
    - Twitter/X URLs: Extracts tweet content from meta tags
    - Other URLs: Generic HTML parsing

    Returns structured content including title, text, and metadata.
    """
    logger.info(f"Fetching and parsing URL: {request.url}")

    try:
        parsed = await do_parse_url(request.url, timeout=request.timeout)

        logger.info(
            f"Successfully parsed {request.url} as {parsed.site_kind}, "
            f"word_count={parsed.word_count}"
        )

        return ParsedPageResponse(
            site_kind=parsed.site_kind,
            title=parsed.title,
            text_full=parsed.text_full,
            word_count=parsed.word_count,
            video_seconds=parsed.video_seconds,
            metadata=parsed.metadata,
        )

    except httpx.TimeoutException:
        logger.warning(f"Timeout fetching URL: {request.url}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                error="Fetch timeout",
                detail=f"Request timed out after {request.timeout} seconds",
                url=request.url,
            ).model_dump(),
        )

    except httpx.HTTPStatusError as e:
        logger.warning(f"HTTP error fetching URL {request.url}: {e.response.status_code}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                error="HTTP error",
                detail=f"Received status {e.response.status_code}",
                url=request.url,
            ).model_dump(),
        )

    except httpx.RequestError as e:
        logger.warning(f"Request error fetching URL {request.url}: {e}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                error="Request failed",
                detail=str(e),
                url=request.url,
            ).model_dump(),
        )

    except ValueError as e:
        logger.warning(f"Parse error for URL {request.url}: {e}")
        raise HTTPException(
            status_code=400,
            detail=ErrorResponse(
                error="Parse error",
                detail=str(e),
                url=request.url,
            ).model_dump(),
        )

    except Exception as e:
        logger.exception(f"Unexpected error parsing URL {request.url}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                error="Internal error",
                detail=str(e),
                url=request.url,
            ).model_dump(),
        )


@app.post(
    "/parse_html",
    response_model=ParsedPageResponse,
    responses={
        400: {"model": ErrorResponse, "description": "Invalid request"},
        500: {"model": ErrorResponse, "description": "Parsing error"},
    },
    tags=["Parsing"],
)
async def parse_html_content(request: ParseHtmlRequest):
    """
    Parse pre-fetched HTML content.

    Use this endpoint when you already have the HTML content and don't
    need the service to fetch it. The URL is still required for
    parser selection.
    """
    logger.info(f"Parsing HTML for URL: {request.url}")

    try:
        parsed = parse_html(request.url, request.html_content)

        logger.info(
            f"Successfully parsed HTML for {request.url} as {parsed.site_kind}, "
            f"word_count={parsed.word_count}"
        )

        return ParsedPageResponse(
            site_kind=parsed.site_kind,
            title=parsed.title,
            text_full=parsed.text_full,
            word_count=parsed.word_count,
            video_seconds=parsed.video_seconds,
            metadata=parsed.metadata,
        )

    except ValueError as e:
        logger.warning(f"Parse error for URL {request.url}: {e}")
        raise HTTPException(
            status_code=400,
            detail=ErrorResponse(
                error="Parse error",
                detail=str(e),
                url=request.url,
            ).model_dump(),
        )

    except Exception as e:
        logger.exception(f"Unexpected error parsing HTML for {request.url}")
        raise HTTPException(
            status_code=500,
            detail=ErrorResponse(
                error="Internal error",
                detail=str(e),
                url=request.url,
            ).model_dump(),
        )


@app.get("/parsers", tags=["Info"])
async def list_parsers():
    """List all available parsers."""
    registry = get_registry()
    return {"parsers": registry.list_parsers()}


# Run with: uvicorn parser_service.main:app --host 0.0.0.0 --port 8001
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
</file>

<file path="parser_service/models.py">
"""
TabBacklog v1 - Parser Service Pydantic Models

API request/response models for the parser service.
"""

from typing import Optional
from pydantic import BaseModel, Field, HttpUrl


class FetchParseRequest(BaseModel):
    """Request model for /fetch_parse endpoint"""
    url: str = Field(..., description="URL to fetch and parse")
    timeout: float = Field(
        default=30.0,
        ge=1.0,
        le=120.0,
        description="Request timeout in seconds"
    )


class ParseHtmlRequest(BaseModel):
    """Request model for /parse_html endpoint (parse pre-fetched HTML)"""
    url: str = Field(..., description="Original URL (for parser selection)")
    html_content: str = Field(..., description="HTML content to parse")


class ParsedPageResponse(BaseModel):
    """Response model for parsed page content"""
    site_kind: str = Field(..., description="Type of site (youtube, twitter, generic_html)")
    title: Optional[str] = Field(None, description="Page/content title")
    text_full: Optional[str] = Field(None, description="Full extracted text content")
    word_count: Optional[int] = Field(None, description="Word count of text content")
    video_seconds: Optional[int] = Field(None, description="Video duration in seconds (if applicable)")
    metadata: dict = Field(default_factory=dict, description="Additional metadata")

    class Config:
        json_schema_extra = {
            "example": {
                "site_kind": "youtube",
                "title": "Example Video Title",
                "text_full": "Example Video Title\n\nThis is the video description...",
                "word_count": 150,
                "video_seconds": 600,
                "metadata": {
                    "url": "https://youtube.com/watch?v=abc123",
                    "video_id": "abc123",
                    "uploader": "Example Channel",
                    "view_count": 10000
                }
            }
        }


class ErrorResponse(BaseModel):
    """Error response model"""
    error: str = Field(..., description="Error message")
    detail: Optional[str] = Field(None, description="Detailed error information")
    url: Optional[str] = Field(None, description="URL that caused the error")


class HealthResponse(BaseModel):
    """Health check response"""
    status: str = Field(..., description="Service status")
    version: str = Field(..., description="Service version")
    parsers: list[str] = Field(..., description="Available parsers")
</file>

<file path="shared/__init__.py">
"""
TabBacklog v1 - Shared Utilities

Common utilities used across services.
"""

from .search import EmbeddingGenerator, SearchService

__all__ = ["EmbeddingGenerator", "SearchService"]
</file>

<file path="shared/search.py">
"""
TabBacklog v1 - Search Utilities

Embedding generation and semantic search functionality.
"""

import logging
import os
from typing import Optional

import httpx

logger = logging.getLogger(__name__)


class EmbeddingGenerator:
    """
    Generate embeddings using an OpenAI-compatible API.

    Supports local models via LM Studio, Ollama, or cloud APIs.
    """

    def __init__(
        self,
        api_base: str | None = None,
        api_key: str | None = None,
        model_name: str | None = None,
    ):
        self.api_base = api_base or os.environ.get("EMBEDDING_API_BASE", "http://localhost:1234/v1")
        self.api_key = api_key or os.environ.get("EMBEDDING_API_KEY", "dummy_key")
        self.model_name = model_name or os.environ.get("EMBEDDING_MODEL_NAME", "text-embedding-nomic-embed-text-v1.5")
        self._client: Optional[httpx.AsyncClient] = None

    async def _get_client(self) -> httpx.AsyncClient:
        """Get or create async HTTP client"""
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url=self.api_base,
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=60.0,
            )
        return self._client

    async def close(self):
        """Close the HTTP client"""
        if self._client:
            await self._client.aclose()
            self._client = None

    async def generate(self, text: str) -> list[float]:
        """
        Generate an embedding for the given text.

        Args:
            text: Text to embed

        Returns:
            List of floats representing the embedding vector
        """
        client = await self._get_client()

        # Truncate text if too long (most models have a limit)
        max_chars = 8000
        if len(text) > max_chars:
            text = text[:max_chars]

        response = await client.post(
            "/embeddings",
            json={
                "model": self.model_name,
                "input": text,
            },
        )
        response.raise_for_status()

        data = response.json()
        return data["data"][0]["embedding"]

    async def generate_batch(self, texts: list[str]) -> list[list[float]]:
        """
        Generate embeddings for multiple texts.

        Args:
            texts: List of texts to embed

        Returns:
            List of embedding vectors
        """
        client = await self._get_client()

        # Truncate texts
        max_chars = 8000
        truncated = [t[:max_chars] if len(t) > max_chars else t for t in texts]

        response = await client.post(
            "/embeddings",
            json={
                "model": self.model_name,
                "input": truncated,
            },
        )
        response.raise_for_status()

        data = response.json()
        # Sort by index to ensure correct order
        embeddings = sorted(data["data"], key=lambda x: x["index"])
        return [e["embedding"] for e in embeddings]


class SearchService:
    """
    High-level search service combining fuzzy and semantic search.
    """

    def __init__(self, embedding_generator: EmbeddingGenerator):
        self.embedding_generator = embedding_generator

    def prepare_text_for_embedding(
        self,
        title: str | None,
        summary: str | None,
        text: str | None,
    ) -> str:
        """
        Prepare text for embedding generation.

        Combines title, summary, and content into a single string
        optimized for embedding.
        """
        parts = []

        if title:
            parts.append(f"Title: {title}")

        if summary:
            parts.append(f"Summary: {summary}")

        if text:
            # Use first portion of text
            text_preview = text[:2000] if len(text) > 2000 else text
            parts.append(f"Content: {text_preview}")

        return "\n\n".join(parts) if parts else ""

    async def generate_query_embedding(self, query: str) -> list[float]:
        """
        Generate an embedding for a search query.

        Args:
            query: Search query text

        Returns:
            Embedding vector for the query
        """
        return await self.embedding_generator.generate(query)

    async def generate_document_embedding(
        self,
        title: str | None,
        summary: str | None,
        text: str | None,
    ) -> list[float]:
        """
        Generate an embedding for a document (tab content).

        Args:
            title: Document title
            summary: LLM-generated summary
            text: Full text content

        Returns:
            Embedding vector for the document
        """
        combined_text = self.prepare_text_for_embedding(title, summary, text)
        if not combined_text:
            raise ValueError("No text provided for embedding")

        return await self.embedding_generator.generate(combined_text)


async def test_embedding_connection() -> bool:
    """
    Test if the embedding API is reachable.

    Returns:
        True if connection successful, False otherwise
    """
    try:
        generator = EmbeddingGenerator()
        embedding = await generator.generate("test")
        await generator.close()
        return len(embedding) > 0
    except Exception as e:
        logger.warning(f"Embedding connection test failed: {e}")
        return False
</file>

<file path="tests/e2e/__init__.py">
"""TabBacklog v1 - End-to-End Tests"""
</file>

<file path="tests/e2e/test_api_endpoints.py">
"""
TabBacklog v1 - API Endpoint E2E Tests

Tests for the REST API endpoints using Playwright's request context.

Run with:
    pytest tests/e2e/test_api_endpoints.py -m e2e
"""

import pytest
from playwright.sync_api import Page, APIRequestContext


@pytest.fixture
def api_context(playwright, base_url: str) -> APIRequestContext:
    """Create an API request context for testing."""
    context = playwright.request.new_context(base_url=base_url)
    yield context
    context.dispose()


@pytest.mark.e2e
class TestHealthEndpoint:
    """Tests for the health check endpoint."""

    def test_health_returns_200(self, api_context: APIRequestContext):
        """Test that health endpoint returns 200."""
        response = api_context.get("/health")
        assert response.ok
        assert response.status == 200

    def test_health_returns_json(self, api_context: APIRequestContext):
        """Test that health endpoint returns valid JSON."""
        response = api_context.get("/health")
        data = response.json()

        assert "status" in data
        assert "version" in data
        assert "database" in data

    def test_health_status_values(self, api_context: APIRequestContext):
        """Test that health status has valid values."""
        response = api_context.get("/health")
        data = response.json()

        assert data["status"] in ["healthy", "degraded"]
        assert data["database"] in ["connected", "disconnected"]


@pytest.mark.e2e
class TestTabsEndpoint:
    """Tests for the tabs listing endpoint."""

    def test_tabs_endpoint_exists(self, api_context: APIRequestContext):
        """Test that /tabs endpoint exists and responds."""
        response = api_context.get("/tabs")
        # Should return HTML fragment for HTMX
        assert response.status in [200, 500]  # 500 if DB not set up

    def test_tabs_accepts_filters(self, api_context: APIRequestContext):
        """Test that /tabs endpoint accepts filter parameters."""
        response = api_context.get("/tabs", params={
            "status": "new",
            "content_type": "article",
            "is_processed": "false",
            "search": "test"
        })
        # Should accept parameters without error (even if no results)
        assert response.status in [200, 500]


@pytest.mark.e2e
class TestExportEndpoints:
    """Tests for export endpoints."""

    def test_export_json_requires_post(self, api_context: APIRequestContext):
        """Test that JSON export requires POST method."""
        response = api_context.get("/export/json")
        assert response.status == 405  # Method Not Allowed

    def test_export_markdown_requires_post(self, api_context: APIRequestContext):
        """Test that Markdown export requires POST method."""
        response = api_context.get("/export/markdown")
        assert response.status == 405

    def test_export_obsidian_requires_post(self, api_context: APIRequestContext):
        """Test that Obsidian export requires POST method."""
        response = api_context.get("/export/obsidian")
        assert response.status == 405

    def test_export_json_with_empty_ids(self, api_context: APIRequestContext):
        """Test JSON export with empty tab_ids."""
        response = api_context.post("/export/json", data={
            "tab_ids": []
        })
        # Should handle empty list gracefully
        assert response.status in [200, 400, 422]

    def test_export_json_with_ids(self, api_context: APIRequestContext):
        """Test JSON export with tab IDs."""
        response = api_context.post(
            "/export/json",
            headers={"Content-Type": "application/json"},
            data={"tab_ids": [1, 2, 3]}
        )
        # Will fail if tabs don't exist, but should process the request
        assert response.status in [200, 404, 500]


@pytest.mark.e2e
class TestSearchEndpoints:
    """Tests for search endpoints."""

    def test_semantic_search_endpoint_exists(self, api_context: APIRequestContext):
        """Test that semantic search endpoint exists."""
        response = api_context.get("/search/semantic", params={"q": "test"})
        # Should respond (even if no embeddings)
        assert response.status in [200, 500]

    def test_semantic_search_requires_query(self, api_context: APIRequestContext):
        """Test that semantic search needs a query parameter."""
        response = api_context.get("/search/semantic")
        # Should either accept empty or require query
        assert response.status in [200, 400, 422, 500]

    def test_generate_embeddings_requires_post(self, api_context: APIRequestContext):
        """Test that generate embeddings requires POST."""
        response = api_context.get("/search/generate-embeddings")
        assert response.status == 405


@pytest.mark.e2e
class TestStaticFiles:
    """Tests for static file serving."""

    def test_css_file_served(self, api_context: APIRequestContext):
        """Test that CSS files are served."""
        response = api_context.get("/static/css/style.css")
        assert response.ok
        assert "text/css" in response.headers.get("content-type", "")

    def test_nonexistent_static_returns_404(self, api_context: APIRequestContext):
        """Test that nonexistent static files return 404."""
        response = api_context.get("/static/nonexistent.xyz")
        assert response.status == 404


@pytest.mark.e2e
class TestIndexPage:
    """Tests for the index page."""

    def test_index_returns_html(self, api_context: APIRequestContext):
        """Test that index page returns HTML."""
        response = api_context.get("/")
        assert response.ok
        content_type = response.headers.get("content-type", "")
        assert "text/html" in content_type

    def test_index_contains_required_elements(self, api_context: APIRequestContext):
        """Test that index page contains required HTML elements."""
        response = api_context.get("/")
        html = response.text()

        # Should contain key elements
        assert "filter-form" in html
        assert "tabs-body" in html
        assert "search-input" in html
        assert "htmx" in html.lower()


@pytest.mark.e2e
class TestStatsPage:
    """Tests for the stats page."""

    def test_stats_returns_html(self, api_context: APIRequestContext):
        """Test that stats page returns HTML."""
        response = api_context.get("/stats")
        assert response.status in [200, 500]  # 500 if DB not connected

    def test_stats_contains_statistics(self, api_context: APIRequestContext):
        """Test that stats page has statistical content."""
        response = api_context.get("/stats")
        if response.ok:
            html = response.text()
            # Should have some stats-related content
            assert len(html) > 100  # Not empty


@pytest.mark.e2e
class TestErrorHandling:
    """Tests for error handling."""

    def test_404_for_nonexistent_page(self, api_context: APIRequestContext):
        """Test that nonexistent pages return 404."""
        response = api_context.get("/this-page-does-not-exist")
        assert response.status == 404

    def test_invalid_tab_id(self, api_context: APIRequestContext):
        """Test handling of invalid tab ID."""
        response = api_context.get("/tabs/invalid-id")
        assert response.status in [404, 422, 500]

    def test_nonexistent_tab_id(self, api_context: APIRequestContext):
        """Test handling of nonexistent tab ID."""
        response = api_context.get("/tabs/999999999")
        assert response.status in [404, 500]
</file>

<file path="tests/e2e/test_web_ui.py">
"""
TabBacklog v1 - Web UI End-to-End Tests

Playwright-based e2e tests for the web interface.

Run with:
    pytest tests/e2e/ -m e2e --headed  # With browser visible
    pytest tests/e2e/ -m e2e           # Headless mode

Prerequisites:
    1. Install Playwright browsers: playwright install
    2. Start the web UI server: uvicorn web_ui.main:app --port 8000
    3. Ensure database has test data
"""

import pytest
from playwright.sync_api import Page, expect


@pytest.mark.e2e
class TestPageLoad:
    """Tests for basic page loading and structure."""

    def test_homepage_loads(self, page: Page, base_url: str):
        """Test that the homepage loads successfully."""
        page.goto(base_url)

        # Check page title
        expect(page).to_have_title("TabBacklog - Your Tabs")

        # Check main elements are present
        expect(page.locator(".filters-section")).to_be_visible()
        expect(page.locator(".tabs-table")).to_be_visible()
        expect(page.locator(".actions-bar")).to_be_visible()

    def test_stats_page_loads(self, page: Page, base_url: str):
        """Test that the stats page loads successfully."""
        page.goto(f"{base_url}/stats")

        # Should have stats content
        expect(page.locator("body")).to_contain_text("Total")

    def test_health_endpoint(self, page: Page, base_url: str):
        """Test the health check endpoint."""
        response = page.request.get(f"{base_url}/health")
        assert response.ok
        data = response.json()
        assert data["status"] in ["healthy", "degraded"]
        assert "version" in data


@pytest.mark.e2e
class TestFilters:
    """Tests for filtering functionality."""

    def test_filter_elements_present(self, page: Page, base_url: str):
        """Test that all filter elements are present."""
        page.goto(base_url)

        expect(page.locator("#search-input")).to_be_visible()
        expect(page.locator("#status-filter")).to_be_visible()
        expect(page.locator("#content-type-filter")).to_be_visible()
        expect(page.locator("#processed-filter")).to_be_visible()
        expect(page.locator("#read-time-filter")).to_be_visible()

    def test_status_filter_options(self, page: Page, base_url: str):
        """Test that status filter has expected options."""
        page.goto(base_url)

        status_filter = page.locator("#status-filter")
        options = status_filter.locator("option").all_text_contents()

        # Should have "All Status" and various status options
        assert "All Status" in options

    def test_processed_filter_options(self, page: Page, base_url: str):
        """Test that processed filter has correct options."""
        page.goto(base_url)

        processed_filter = page.locator("#processed-filter")
        options = processed_filter.locator("option").all_text_contents()

        assert "All" in options
        assert "Unprocessed" in options
        assert "Processed" in options

    def test_filter_triggers_htmx_request(self, page: Page, base_url: str):
        """Test that changing a filter triggers an HTMX request."""
        page.goto(base_url)

        # Wait for initial load
        page.wait_for_load_state("networkidle")

        # Track network requests
        requests = []
        page.on("request", lambda r: requests.append(r.url) if "/tabs" in r.url else None)

        # Change status filter
        page.select_option("#status-filter", index=1)

        # Wait for HTMX request
        page.wait_for_timeout(500)

        # Should have made a request to /tabs
        assert any("/tabs" in r for r in requests)


@pytest.mark.e2e
class TestSearch:
    """Tests for search functionality."""

    def test_search_input_exists(self, page: Page, base_url: str):
        """Test that search input is present and functional."""
        page.goto(base_url)

        search_input = page.locator("#search-input")
        expect(search_input).to_be_visible()
        expect(search_input).to_have_attribute("placeholder", "Search tabs...")

    def test_semantic_search_toggle_exists(self, page: Page, base_url: str):
        """Test that semantic search toggle is present."""
        page.goto(base_url)

        toggle = page.locator("#semantic-search-toggle")
        expect(toggle).to_be_attached()

    def test_search_triggers_request(self, page: Page, base_url: str):
        """Test that typing in search triggers a request."""
        page.goto(base_url)
        page.wait_for_load_state("networkidle")

        requests = []
        page.on("request", lambda r: requests.append(r.url) if "/tabs" in r.url or "/search" in r.url else None)

        # Type in search
        page.fill("#search-input", "test query")

        # Wait for debounced request
        page.wait_for_timeout(500)

        # Should have made a search request
        assert len(requests) > 0

    def test_semantic_search_toggle_changes_endpoint(self, page: Page, base_url: str):
        """Test that toggling semantic search changes the search endpoint."""
        page.goto(base_url)
        page.wait_for_load_state("networkidle")

        # Enable semantic search
        page.check("#semantic-search-toggle")

        # Check that form action changed
        form = page.locator("#filter-form")
        expect(form).to_have_attribute("hx-get", "/search/semantic")

        # Disable semantic search
        page.uncheck("#semantic-search-toggle")

        # Should revert to /tabs
        expect(form).to_have_attribute("hx-get", "/tabs")


@pytest.mark.e2e
class TestTabSelection:
    """Tests for tab selection functionality."""

    def test_select_all_checkbox_exists(self, page: Page, base_url: str):
        """Test that select all checkbox is present."""
        page.goto(base_url)

        select_all = page.locator("#select-all")
        expect(select_all).to_be_visible()

    def test_selected_count_display(self, page: Page, base_url: str):
        """Test that selected count is displayed."""
        page.goto(base_url)

        count_display = page.locator("#selected-count")
        expect(count_display).to_be_visible()
        expect(count_display).to_have_text("0")

    def test_export_buttons_disabled_when_none_selected(self, page: Page, base_url: str):
        """Test that export buttons are disabled when no tabs are selected."""
        page.goto(base_url)

        # Export buttons should be disabled
        json_btn = page.locator("button:has-text('Export JSON')")
        markdown_btn = page.locator("button:has-text('Export Markdown')")
        obsidian_btn = page.locator("button:has-text('Export to Obsidian')")

        expect(json_btn).to_be_disabled()
        expect(markdown_btn).to_be_disabled()
        expect(obsidian_btn).to_be_disabled()


@pytest.mark.e2e
class TestExport:
    """Tests for export functionality."""

    def test_export_buttons_present(self, page: Page, base_url: str):
        """Test that all export buttons are present."""
        page.goto(base_url)

        expect(page.locator("button:has-text('Export JSON')")).to_be_visible()
        expect(page.locator("button:has-text('Export Markdown')")).to_be_visible()
        expect(page.locator("button:has-text('Export to Obsidian')")).to_be_visible()


@pytest.mark.e2e
class TestModal:
    """Tests for tab detail modal."""

    def test_modal_hidden_by_default(self, page: Page, base_url: str):
        """Test that the modal is hidden by default."""
        page.goto(base_url)

        modal = page.locator("#tab-modal")
        expect(modal).to_have_css("display", "none")

    def test_escape_key_closes_modal(self, page: Page, base_url: str):
        """Test that pressing Escape would close the modal."""
        page.goto(base_url)

        # This tests that the event listener is set up
        # The actual modal closing requires a tab to be clicked first
        page.evaluate("""
            () => {
                const modal = document.getElementById('tab-modal');
                modal.style.display = 'flex';
            }
        """)

        modal = page.locator("#tab-modal")
        expect(modal).to_have_css("display", "flex")

        # Press Escape
        page.keyboard.press("Escape")

        expect(modal).to_have_css("display", "none")


@pytest.mark.e2e
class TestStatsPage:
    """Tests for the statistics page."""

    def test_stats_page_accessible(self, page: Page, base_url: str):
        """Test that stats page is accessible."""
        page.goto(f"{base_url}/stats")
        expect(page.locator("body")).not_to_be_empty()

    def test_stats_shows_totals(self, page: Page, base_url: str):
        """Test that stats page shows total counts."""
        page.goto(f"{base_url}/stats")

        # Should contain some stats information
        body_text = page.locator("body").text_content()
        # Stats page should have some numeric content or labels
        assert body_text is not None


@pytest.mark.e2e
class TestResponsiveness:
    """Tests for responsive design."""

    def test_mobile_viewport(self, browser, base_url: str):
        """Test that the page works on mobile viewport."""
        context = browser.new_context(viewport={"width": 375, "height": 667})
        page = context.new_page()

        page.goto(base_url)

        # Main elements should still be visible
        expect(page.locator(".container")).to_be_visible()
        expect(page.locator(".tabs-table")).to_be_visible()

        context.close()

    def test_tablet_viewport(self, browser, base_url: str):
        """Test that the page works on tablet viewport."""
        context = browser.new_context(viewport={"width": 768, "height": 1024})
        page = context.new_page()

        page.goto(base_url)

        expect(page.locator(".container")).to_be_visible()
        expect(page.locator(".filters-section")).to_be_visible()

        context.close()


@pytest.mark.e2e
class TestAccessibility:
    """Basic accessibility tests."""

    def test_form_labels_exist(self, page: Page, base_url: str):
        """Test that form inputs have labels."""
        page.goto(base_url)

        # Search input should have a label
        search_label = page.locator("label[for='search-input']")
        expect(search_label).to_be_visible()

        # Filter selects should have labels
        status_label = page.locator("label[for='status-filter']")
        expect(status_label).to_be_visible()

    def test_buttons_have_text(self, page: Page, base_url: str):
        """Test that buttons have descriptive text."""
        page.goto(base_url)

        # Export buttons should have text
        buttons = page.locator(".export-btn").all()
        for button in buttons:
            text = button.text_content()
            assert text and len(text.strip()) > 0


@pytest.mark.e2e
class TestHTMXIntegration:
    """Tests for HTMX integration."""

    def test_htmx_loaded(self, page: Page, base_url: str):
        """Test that HTMX library is loaded."""
        page.goto(base_url)

        htmx_loaded = page.evaluate("() => typeof htmx !== 'undefined'")
        assert htmx_loaded

    def test_tabs_body_has_htmx_attributes(self, page: Page, base_url: str):
        """Test that tabs body has HTMX attributes for loading."""
        page.goto(base_url)

        tabs_body = page.locator("#tabs-body")
        expect(tabs_body).to_have_attribute("hx-get", "/tabs")
        expect(tabs_body).to_have_attribute("hx-trigger", "load")

    def test_filter_form_has_htmx_attributes(self, page: Page, base_url: str):
        """Test that filter form has HTMX attributes."""
        page.goto(base_url)

        form = page.locator("#filter-form")
        expect(form).to_have_attribute("hx-target", "#tabs-body")
</file>

<file path="tests/unit/__init__.py">
"""TabBacklog v1 - Unit Tests"""
</file>

<file path="tests/__init__.py">
"""TabBacklog v1 - Test Suite"""
</file>

<file path="web_ui/routes/export.py">
"""
TabBacklog v1 - Export Routes

Routes for exporting tabs to various formats.
"""

import os
from datetime import datetime

from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse, Response

from ..db import get_database
from ..models import ExportRequest

router = APIRouter(prefix="/export", tags=["Export"])


def get_user_id() -> str:
    """Get the default user ID from environment"""
    user_id = os.environ.get("DEFAULT_USER_ID")
    if not user_id:
        raise HTTPException(500, "DEFAULT_USER_ID not configured")
    return user_id


@router.post("/json")
async def export_json(request: ExportRequest):
    """
    Export selected tabs as JSON.

    Returns JSON array of tab data.
    """
    user_id = get_user_id()
    db = get_database()

    tabs = await db.get_tabs_for_export(user_id, request.tab_ids)

    if not tabs:
        raise HTTPException(404, "No tabs found")

    # Convert to JSON-serializable format
    data = [tab.model_dump() for tab in tabs]

    return JSONResponse(
        content=data,
        headers={
            "Content-Disposition": f'attachment; filename="tabs_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json"'
        },
    )


@router.post("/markdown")
async def export_markdown(request: ExportRequest):
    """
    Export selected tabs as Markdown.

    Returns Markdown file with tab summaries and metadata.
    """
    user_id = get_user_id()
    db = get_database()

    tabs = await db.get_tabs_for_export(user_id, request.tab_ids)

    if not tabs:
        raise HTTPException(404, "No tabs found")

    # Build markdown content
    lines = [
        "# TabBacklog Export",
        "",
        f"*Exported: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
        f"*Total: {len(tabs)} tabs*",
        "",
        "---",
        "",
    ]

    for tab in tabs:
        lines.append(tab.to_markdown())

    content = "\n".join(lines)

    return Response(
        content=content,
        media_type="text/markdown",
        headers={
            "Content-Disposition": f'attachment; filename="tabs_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md"'
        },
    )


@router.post("/obsidian")
async def export_obsidian(request: ExportRequest):
    """
    Export selected tabs in Obsidian-compatible format.

    Returns Markdown with YAML frontmatter for Obsidian.
    """
    user_id = get_user_id()
    db = get_database()

    tabs = await db.get_tabs_for_export(user_id, request.tab_ids)

    if not tabs:
        raise HTTPException(404, "No tabs found")

    # Build Obsidian-style markdown with frontmatter
    lines = [
        "---",
        "type: reading-list",
        f"exported: {datetime.now().isoformat()}",
        f"count: {len(tabs)}",
        "---",
        "",
        "# Reading List Export",
        "",
    ]

    for tab in tabs:
        # Create individual note format
        lines.append(f"## [{tab.title or 'Untitled'}]({tab.url})")
        lines.append("")

        if tab.summary:
            lines.append(f"> {tab.summary}")
            lines.append("")

        # Metadata as inline tags
        metadata_parts = []
        if tab.content_type:
            metadata_parts.append(f"#type/{tab.content_type}")
        if tab.priority:
            metadata_parts.append(f"#priority/{tab.priority}")
        for tag in tab.tags[:5]:  # Limit tags
            clean_tag = tag.lstrip("#").replace(" ", "-")
            metadata_parts.append(f"#{clean_tag}")

        if metadata_parts:
            lines.append(" ".join(metadata_parts))
            lines.append("")

        if tab.est_read_min:
            lines.append(f"⏱️ {tab.est_read_min} min")
            lines.append("")

        lines.append("---")
        lines.append("")

    content = "\n".join(lines)

    return Response(
        content=content,
        media_type="text/markdown",
        headers={
            "Content-Disposition": f'attachment; filename="obsidian_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md"'
        },
    )
</file>

<file path="web_ui/routes/search.py">
"""
TabBacklog v1 - Search Routes

Routes for fuzzy and semantic search functionality.
"""

import os
import logging
from typing import Optional

from fastapi import APIRouter, Request, HTTPException, Query, BackgroundTasks
from fastapi.responses import HTMLResponse, JSONResponse

from ..db import get_database
from ..models import TabFilters

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/search", tags=["Search"])


def get_user_id() -> str:
    """Get the default user ID from environment"""
    user_id = os.environ.get("DEFAULT_USER_ID")
    if not user_id:
        raise HTTPException(500, "DEFAULT_USER_ID not configured")
    return user_id


@router.get("/semantic", response_class=HTMLResponse)
async def semantic_search(
    request: Request,
    q: str = Query(..., min_length=2, description="Search query"),
    limit: int = Query(50, ge=1, le=200),
):
    """
    Perform semantic search using vector embeddings.

    Returns tabs sorted by semantic similarity to the query.
    """
    user_id = get_user_id()
    db = get_database()

    try:
        # Import here to avoid circular imports
        from shared.search import EmbeddingGenerator, SearchService

        # Generate query embedding
        generator = EmbeddingGenerator()
        search_service = SearchService(generator)

        query_embedding = await search_service.generate_query_embedding(q)
        await generator.close()

        # Search using vector similarity
        tabs = await db.semantic_search(user_id, query_embedding, limit)

        templates = request.app.state.templates
        return templates.TemplateResponse(
            "fragments/tab_rows.html",
            {
                "request": request,
                "tabs": tabs,
                "total": len(tabs),
                "page": 1,
                "total_pages": 1,
                "has_next": False,
                "has_prev": False,
                "filters": TabFilters(search=q),
                "search_mode": "semantic",
            },
        )

    except ImportError:
        raise HTTPException(500, "Semantic search not configured - embedding service unavailable")
    except Exception as e:
        logger.exception(f"Semantic search error: {e}")
        raise HTTPException(500, f"Search error: {str(e)}")


@router.post("/generate-embeddings")
async def generate_embeddings(
    background_tasks: BackgroundTasks,
    batch_size: int = Query(10, ge=1, le=100),
):
    """
    Trigger embedding generation for tabs without embeddings.

    Runs in background and returns immediately.
    """
    user_id = get_user_id()

    background_tasks.add_task(generate_embeddings_task, user_id, batch_size)

    return JSONResponse({
        "status": "started",
        "message": f"Generating embeddings for up to {batch_size} tabs",
    })


async def generate_embeddings_task(user_id: str, batch_size: int):
    """Background task to generate embeddings"""
    try:
        from shared.search import EmbeddingGenerator, SearchService

        db = get_database()
        generator = EmbeddingGenerator()
        search_service = SearchService(generator)

        # Get tabs without embeddings
        tabs = await db.get_tabs_without_embeddings(user_id, batch_size)

        if not tabs:
            logger.info("No tabs need embeddings")
            return

        logger.info(f"Generating embeddings for {len(tabs)} tabs")

        for tab in tabs:
            try:
                embedding = await search_service.generate_document_embedding(
                    title=tab.get("page_title"),
                    summary=tab.get("summary"),
                    text=tab.get("text_full"),
                )

                await db.save_embedding(
                    tab_id=tab["id"],
                    embedding=embedding,
                    model_name=generator.model_name,
                )

                logger.info(f"Generated embedding for tab {tab['id']}")

            except Exception as e:
                logger.warning(f"Failed to generate embedding for tab {tab['id']}: {e}")
                continue

        await generator.close()
        logger.info(f"Embedding generation complete for {len(tabs)} tabs")

    except Exception as e:
        logger.exception(f"Embedding generation task failed: {e}")


@router.get("/embedding-status")
async def embedding_status():
    """Get status of embedding coverage"""
    user_id = get_user_id()
    db = get_database()

    async with db.connection() as conn:
        stats = await conn.fetchrow("""
            SELECT
                COUNT(*) as total_enriched,
                COUNT(emb.tab_id) as with_embeddings,
                COUNT(*) - COUNT(emb.tab_id) as without_embeddings
            FROM tab_item t
            LEFT JOIN tab_embedding emb ON t.id = emb.tab_id
            WHERE t.user_id = $1
              AND t.deleted_at IS NULL
              AND t.status = 'enriched'
        """, user_id)

    return {
        "total_enriched": stats["total_enriched"],
        "with_embeddings": stats["with_embeddings"],
        "without_embeddings": stats["without_embeddings"],
        "coverage_percent": round(
            (stats["with_embeddings"] / stats["total_enriched"] * 100)
            if stats["total_enriched"] > 0 else 0,
            1
        ),
    }
</file>

<file path="web_ui/routes/tabs.py">
"""
TabBacklog v1 - Tabs Routes

Routes for tab listing, filtering, and management.
"""

import os
from typing import Optional

from fastapi import APIRouter, Request, HTTPException, Query
from fastapi.responses import HTMLResponse

from ..db import get_database
from ..models import TabFilters

router = APIRouter(tags=["Tabs"])


def get_user_id() -> str:
    """Get the default user ID from environment"""
    user_id = os.environ.get("DEFAULT_USER_ID")
    if not user_id:
        raise HTTPException(500, "DEFAULT_USER_ID not configured")
    return user_id


@router.get("/tabs", response_class=HTMLResponse)
async def get_tabs(
    request: Request,
    content_type: Optional[str] = Query(None),
    project: Optional[str] = Query(None),
    status: Optional[str] = Query(None),
    is_processed: Optional[bool] = Query(None),
    read_time_max: Optional[int] = Query(None),
    search: Optional[str] = Query(None),
    page: int = Query(1, ge=1),
    per_page: int = Query(50, ge=1, le=200),
):
    """
    Get filtered tabs list as HTMX fragment.

    Returns HTML tbody with tab rows for HTMX swap.
    """
    user_id = get_user_id()
    db = get_database()

    filters = TabFilters(
        content_type=content_type,
        project=project,
        status=status,
        is_processed=is_processed,
        read_time_max=read_time_max,
        search=search,
        page=page,
        per_page=per_page,
    )

    result = await db.get_tabs(user_id, filters)

    templates = request.app.state.templates
    return templates.TemplateResponse(
        "fragments/tab_rows.html",
        {
            "request": request,
            "tabs": result.tabs,
            "total": result.total,
            "page": result.page,
            "total_pages": result.total_pages,
            "has_next": result.has_next,
            "has_prev": result.has_prev,
            "filters": filters,
        },
    )


@router.post("/tabs/{tab_id}/toggle_processed", response_class=HTMLResponse)
async def toggle_processed(
    request: Request,
    tab_id: int,
):
    """
    Toggle the processed status of a tab.

    Returns updated row HTML for HTMX swap.
    """
    user_id = get_user_id()
    db = get_database()

    tab = await db.toggle_processed(user_id, tab_id)
    if not tab:
        raise HTTPException(404, "Tab not found")

    templates = request.app.state.templates
    return templates.TemplateResponse(
        "fragments/tab_row.html",
        {
            "request": request,
            "tab": tab,
        },
    )


@router.get("/tabs/{tab_id}", response_class=HTMLResponse)
async def get_tab_detail(
    request: Request,
    tab_id: int,
):
    """
    Get detailed view of a single tab.

    Returns HTML fragment with full tab details.
    """
    user_id = get_user_id()
    db = get_database()

    tab = await db.get_tab_by_id(user_id, tab_id)
    if not tab:
        raise HTTPException(404, "Tab not found")

    templates = request.app.state.templates
    return templates.TemplateResponse(
        "fragments/tab_detail.html",
        {
            "request": request,
            "tab": tab,
        },
    )


@router.get("/filters", response_class=HTMLResponse)
async def get_filter_options(request: Request):
    """
    Get available filter options.

    Returns JSON with filter option values.
    """
    user_id = get_user_id()
    db = get_database()

    options = await db.get_filter_options(user_id)
    return options
</file>

<file path="web_ui/templates/fragments/tab_detail.html">
<div class="tab-detail">
    <button class="modal-close" onclick="closeModal()">&times;</button>

    <header class="detail-header">
        <h2 class="detail-title">
            <a href="{{ tab.url }}" target="_blank" rel="noopener">
                {{ tab.display_title }}
            </a>
        </h2>
        <p class="detail-url">{{ tab.url }}</p>
    </header>

    <div class="detail-badges">
        {% if tab.content_type %}
        <span class="badge {{ tab.content_type_badge_class }}">{{ tab.content_type }}</span>
        {% endif %}
        <span class="badge {{ tab.status_badge_class }}">{{ tab.status }}</span>
        {% if tab.priority %}
        <span class="badge badge-priority-{{ tab.priority }}">{{ tab.priority }} priority</span>
        {% endif %}
        {% if tab.is_processed %}
        <span class="badge badge-processed">Processed</span>
        {% endif %}
    </div>

    {% if tab.summary %}
    <section class="detail-section">
        <h3>Summary</h3>
        <p class="detail-summary">{{ tab.summary }}</p>
    </section>
    {% endif %}

    <section class="detail-section">
        <h3>Details</h3>
        <dl class="detail-list">
            <dt>Site Type</dt>
            <dd>{{ tab.site_kind or '-' }}</dd>

            <dt>Window</dt>
            <dd>{{ tab.window_label or '-' }}</dd>

            <dt>Read Time</dt>
            <dd>{{ tab.read_time_display }}</dd>

            {% if tab.word_count %}
            <dt>Word Count</dt>
            <dd>{{ "{:,}".format(tab.word_count) }} words</dd>
            {% endif %}

            {% if tab.video_seconds %}
            <dt>Duration</dt>
            <dd>{{ (tab.video_seconds // 60) }}:{{ "{:02d}".format(tab.video_seconds % 60) }}</dd>
            {% endif %}

            <dt>Added</dt>
            <dd>{{ tab.created_at.strftime('%Y-%m-%d %H:%M') }}</dd>

            {% if tab.processed_at %}
            <dt>Processed</dt>
            <dd>{{ tab.processed_at.strftime('%Y-%m-%d %H:%M') }}</dd>
            {% endif %}
        </dl>
    </section>

    {% if tab.tags %}
    <section class="detail-section">
        <h3>Tags</h3>
        <div class="tags-list detail-tags">
            {% for tag in tab.tags %}
            <span class="tag">{{ tag }}</span>
            {% endfor %}
        </div>
    </section>
    {% endif %}

    {% if tab.projects %}
    <section class="detail-section">
        <h3>Projects</h3>
        <div class="projects-list">
            {% for project in tab.projects %}
            <span class="project-badge">{{ project }}</span>
            {% endfor %}
        </div>
    </section>
    {% endif %}

    <footer class="detail-footer">
        <button
            class="btn {% if tab.is_processed %}btn-secondary{% else %}btn-primary{% endif %}"
            hx-post="/tabs/{{ tab.id }}/toggle_processed"
            hx-target="#tab-row-{{ tab.id }}"
            hx-swap="outerHTML"
            onclick="closeModal()"
        >
            {% if tab.is_processed %}
            Mark as Unprocessed
            {% else %}
            Mark as Processed
            {% endif %}
        </button>
        <a href="{{ tab.url }}" target="_blank" rel="noopener" class="btn btn-secondary">
            Open Link
        </a>
    </footer>
</div>
</file>

<file path="web_ui/templates/fragments/tab_row.html">
<tr class="tab-row {% if tab.is_processed %}processed{% endif %}" id="tab-row-{{ tab.id }}">
    <!-- Checkbox -->
    <td class="col-checkbox">
        <input type="checkbox" class="tab-checkbox" value="{{ tab.id }}" onchange="updateSelectedCount()">
    </td>

    <!-- Title -->
    <td class="col-title">
        <div class="title-cell">
            <a href="{{ tab.url }}" target="_blank" rel="noopener" class="tab-link" title="{{ tab.url }}">
                {{ tab.display_title }}
            </a>
            {% if tab.window_label %}
            <span class="window-label">{{ tab.window_label }}</span>
            {% endif %}
            {% if tab.summary %}
            <p class="tab-summary">{{ tab.summary[:150] }}{% if tab.summary|length > 150 %}...{% endif %}</p>
            {% endif %}
        </div>
    </td>

    <!-- Content Type -->
    <td class="col-type">
        {% if tab.content_type %}
        <span class="badge {{ tab.content_type_badge_class }}">
            {{ tab.content_type }}
        </span>
        {% elif tab.site_kind %}
        <span class="badge badge-default">
            {{ tab.site_kind }}
        </span>
        {% else %}
        <span class="badge badge-default">-</span>
        {% endif %}
    </td>

    <!-- Status -->
    <td class="col-status">
        <span class="badge {{ tab.status_badge_class }}">
            {{ tab.status }}
        </span>
    </td>

    <!-- Read Time -->
    <td class="col-time">
        <span class="read-time">{{ tab.read_time_display }}</span>
    </td>

    <!-- Tags -->
    <td class="col-tags">
        <div class="tags-list">
            {% for tag in tab.tags[:3] %}
            <span class="tag">{{ tag }}</span>
            {% endfor %}
            {% if tab.tags|length > 3 %}
            <span class="tag tag-more">+{{ tab.tags|length - 3 }}</span>
            {% endif %}
        </div>
    </td>

    <!-- Actions -->
    <td class="col-actions">
        <div class="action-buttons">
            <button
                class="btn btn-icon {% if tab.is_processed %}btn-processed{% endif %}"
                hx-post="/tabs/{{ tab.id }}/toggle_processed"
                hx-target="#tab-row-{{ tab.id }}"
                hx-swap="outerHTML"
                title="{% if tab.is_processed %}Mark as unprocessed{% else %}Mark as processed{% endif %}"
            >
                {% if tab.is_processed %}
                <span class="icon">&#10003;</span>
                {% else %}
                <span class="icon">&#9675;</span>
                {% endif %}
            </button>
            <button
                class="btn btn-icon"
                onclick="showTabDetail({{ tab.id }})"
                title="View details"
            >
                <span class="icon">&#8943;</span>
            </button>
        </div>
    </td>
</tr>
</file>

<file path="web_ui/templates/fragments/tab_rows.html">
{% for tab in tabs %}
{% include "fragments/tab_row.html" %}
{% else %}
<tr>
    <td colspan="7" class="empty-cell">
        <div class="empty-state">
            <p>No tabs found</p>
            <p class="empty-hint">Try adjusting your filters or import some bookmarks</p>
        </div>
    </td>
</tr>
{% endfor %}

{% if tabs %}
<!-- Pagination info in a separate row -->
<tr class="pagination-row">
    <td colspan="7">
        <div class="pagination-controls">
            <span class="pagination-info">
                Showing {{ tabs|length }} of {{ total }} tabs (Page {{ page }} of {{ total_pages }})
            </span>

            <div class="pagination-buttons">
                {% if has_prev %}
                <button
                    class="btn btn-sm"
                    hx-get="/tabs?page={{ page - 1 }}{% if filters.status %}&status={{ filters.status }}{% endif %}{% if filters.content_type %}&content_type={{ filters.content_type }}{% endif %}{% if filters.search %}&search={{ filters.search }}{% endif %}{% if filters.is_processed is not none %}&is_processed={{ filters.is_processed }}{% endif %}"
                    hx-target="#tabs-body"
                    hx-swap="innerHTML"
                >
                    Previous
                </button>
                {% endif %}

                {% if has_next %}
                <button
                    class="btn btn-sm"
                    hx-get="/tabs?page={{ page + 1 }}{% if filters.status %}&status={{ filters.status }}{% endif %}{% if filters.content_type %}&content_type={{ filters.content_type }}{% endif %}{% if filters.search %}&search={{ filters.search }}{% endif %}{% if filters.is_processed is not none %}&is_processed={{ filters.is_processed }}{% endif %}"
                    hx-target="#tabs-body"
                    hx-swap="innerHTML"
                >
                    Next
                </button>
                {% endif %}
            </div>
        </div>
    </td>
</tr>
{% endif %}
</file>

<file path="web_ui/templates/base.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{% block title %}TabBacklog{% endblock %}</title>

    <!-- HTMX -->
    <script src="https://unpkg.com/htmx.org@1.9.10"></script>

    <!-- Styles -->
    <link rel="stylesheet" href="/static/css/style.css">

    {% block head %}{% endblock %}
</head>
<body>
    <header class="header">
        <div class="header-content">
            <h1 class="logo">TabBacklog</h1>
            <nav class="nav">
                <a href="/" class="nav-link">Tabs</a>
                <a href="/stats" class="nav-link">Stats</a>
            </nav>
        </div>
    </header>

    <main class="main-content">
        {% block content %}{% endblock %}
    </main>

    <footer class="footer">
        <p>TabBacklog v1.0 - Manage your browser tabs</p>
    </footer>

    <!-- Toast notifications -->
    <div id="toast-container"></div>

    <script>
        // HTMX event handlers
        document.body.addEventListener('htmx:afterSwap', function(event) {
            // Update selected count after table refresh
            updateSelectedCount();
        });

        document.body.addEventListener('htmx:responseError', function(event) {
            showToast('Error: ' + event.detail.xhr.statusText, 'error');
        });

        function showToast(message, type = 'info') {
            const container = document.getElementById('toast-container');
            const toast = document.createElement('div');
            toast.className = 'toast toast-' + type;
            toast.textContent = message;
            container.appendChild(toast);

            setTimeout(() => toast.remove(), 3000);
        }

        function updateSelectedCount() {
            const checkboxes = document.querySelectorAll('.tab-checkbox:checked');
            const countEl = document.getElementById('selected-count');
            if (countEl) {
                countEl.textContent = checkboxes.length;
            }

            // Enable/disable export buttons
            const exportBtns = document.querySelectorAll('.export-btn');
            exportBtns.forEach(btn => {
                btn.disabled = checkboxes.length === 0;
            });
        }
    </script>

    {% block scripts %}{% endblock %}
</body>
</html>
</file>

<file path="web_ui/templates/stats.html">
{% extends "base.html" %}

{% block title %}TabBacklog - Statistics{% endblock %}

{% block content %}
<div class="container">
    <h1 class="page-title">Statistics</h1>

    <!-- Summary Cards -->
    <section class="stats-cards">
        <div class="stat-card">
            <div class="stat-value">{{ filter_options.total }}</div>
            <div class="stat-label">Total Tabs</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">{{ filter_options.processed }}</div>
            <div class="stat-label">Processed</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">{{ filter_options.unprocessed }}</div>
            <div class="stat-label">Remaining</div>
        </div>
        <div class="stat-card">
            <div class="stat-value">{{ ((filter_options.processed / filter_options.total) * 100)|round|int if filter_options.total else 0 }}%</div>
            <div class="stat-label">Progress</div>
        </div>
    </section>

    <div class="stats-grid">
        <!-- Status Breakdown -->
        <section class="stats-section">
            <h2>By Status</h2>
            <div class="stats-list">
                {% for item in status_counts %}
                <div class="stats-row">
                    <span class="stats-label">
                        <span class="badge badge-{{ item.status.replace('_', '-') }}">{{ item.status }}</span>
                    </span>
                    <span class="stats-count">{{ item.count }}</span>
                    <div class="stats-bar">
                        <div class="stats-bar-fill" style="width: {{ (item.count / filter_options.total * 100)|round }}%"></div>
                    </div>
                </div>
                {% endfor %}
            </div>
        </section>

        <!-- Content Type Breakdown -->
        <section class="stats-section">
            <h2>By Content Type</h2>
            <div class="stats-list">
                {% for item in content_type_counts %}
                <div class="stats-row">
                    <span class="stats-label">
                        <span class="badge type-{{ item.content_type }}">{{ item.content_type }}</span>
                    </span>
                    <span class="stats-count">{{ item.count }}</span>
                    <div class="stats-bar">
                        <div class="stats-bar-fill" style="width: {{ (item.count / filter_options.total * 100)|round }}%"></div>
                    </div>
                </div>
                {% else %}
                <p class="empty-hint">No enriched tabs yet</p>
                {% endfor %}
            </div>
        </section>
    </div>

    <!-- Recent Activity -->
    <section class="stats-section">
        <h2>Recent Activity</h2>
        <table class="activity-table">
            <thead>
                <tr>
                    <th>Event</th>
                    <th>Entity</th>
                    <th>Time</th>
                </tr>
            </thead>
            <tbody>
                {% for event in recent_events %}
                <tr>
                    <td>{{ event.event_type }}</td>
                    <td>{{ event.entity_type or '-' }}</td>
                    <td>{{ event.created_at.strftime('%Y-%m-%d %H:%M') }}</td>
                </tr>
                {% else %}
                <tr>
                    <td colspan="3" class="empty-cell">No recent activity</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </section>
</div>

<style>
.page-title {
    margin-bottom: 1.5rem;
}

.stats-cards {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.stat-card {
    background: var(--color-surface);
    border-radius: var(--radius-md);
    padding: 1.5rem;
    text-align: center;
    box-shadow: var(--shadow-sm);
}

.stat-value {
    font-size: 2rem;
    font-weight: 700;
    color: var(--color-primary);
}

.stat-label {
    font-size: 0.875rem;
    color: var(--color-text-muted);
    margin-top: 0.25rem;
}

.stats-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 1.5rem;
    margin-bottom: 2rem;
}

.stats-section {
    background: var(--color-surface);
    border-radius: var(--radius-md);
    padding: 1.5rem;
    box-shadow: var(--shadow-sm);
}

.stats-section h2 {
    font-size: 1rem;
    margin: 0 0 1rem;
    color: var(--color-text-muted);
}

.stats-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.stats-row {
    display: grid;
    grid-template-columns: 120px 50px 1fr;
    align-items: center;
    gap: 0.75rem;
}

.stats-count {
    font-weight: 600;
    text-align: right;
}

.stats-bar {
    height: 8px;
    background: var(--color-bg);
    border-radius: 4px;
    overflow: hidden;
}

.stats-bar-fill {
    height: 100%;
    background: var(--color-primary);
    border-radius: 4px;
    transition: width 0.3s ease;
}

.activity-table {
    width: 100%;
    border-collapse: collapse;
}

.activity-table th,
.activity-table td {
    padding: 0.75rem;
    text-align: left;
    border-bottom: 1px solid var(--color-border);
}

.activity-table th {
    font-size: 0.75rem;
    text-transform: uppercase;
    color: var(--color-text-muted);
}
</style>
{% endblock %}
</file>

<file path="web_ui/__init__.py">
"""
TabBacklog v1 - Web UI

HTMX-powered web interface for managing tabs.
"""

__version__ = "1.0.0"
</file>

<file path="web_ui/Dockerfile">
FROM python:3.11-slim

WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy web UI code
COPY web_ui/ ./web_ui/

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Environment variables
ENV DATABASE_URL=""
ENV DEFAULT_USER_ID=""
ENV APP_SECRET_KEY=""

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import httpx; httpx.get('http://localhost:8000/health').raise_for_status()"

# Run the service
CMD ["uvicorn", "web_ui.main:app", "--host", "0.0.0.0", "--port", "8000"]
</file>

<file path="web_ui/models.py">
"""
TabBacklog v1 - Web UI Pydantic Models

Models for API requests/responses and data display.
"""

from datetime import datetime
from typing import Literal, Optional
from pydantic import BaseModel, Field


class TabFilters(BaseModel):
    """Query parameters for filtering tabs"""
    content_type: Optional[str] = Field(None, description="Filter by content type")
    project: Optional[str] = Field(None, description="Filter by project tag")
    status: Optional[str] = Field(None, description="Filter by pipeline status")
    is_processed: Optional[bool] = Field(None, description="Filter by processed flag")
    read_time_max: Optional[int] = Field(None, description="Max reading time in minutes")
    search: Optional[str] = Field(None, description="Search text (fuzzy)")
    page: int = Field(default=1, ge=1, description="Page number")
    per_page: int = Field(default=50, ge=1, le=200, description="Items per page")


class TabDisplay(BaseModel):
    """Tab data for display in the UI"""
    id: int
    url: str
    page_title: Optional[str]
    window_label: Optional[str]
    status: str
    is_processed: bool
    processed_at: Optional[datetime]
    created_at: datetime

    # Parsed data
    site_kind: Optional[str] = None
    word_count: Optional[int] = None
    video_seconds: Optional[int] = None

    # Enrichment data
    summary: Optional[str] = None
    content_type: Optional[str] = None
    est_read_min: Optional[int] = None
    priority: Optional[str] = None
    tags: list[str] = Field(default_factory=list)
    projects: list[str] = Field(default_factory=list)

    @property
    def display_title(self) -> str:
        """Get display title with fallback"""
        return self.page_title or self.url[:60] + "..." if len(self.url) > 60 else self.url

    @property
    def status_badge_class(self) -> str:
        """CSS class for status badge"""
        status_classes = {
            "new": "badge-new",
            "fetch_pending": "badge-pending",
            "parsed": "badge-parsed",
            "llm_pending": "badge-pending",
            "enriched": "badge-success",
            "fetch_error": "badge-error",
            "llm_error": "badge-error",
        }
        return status_classes.get(self.status, "badge-default")

    @property
    def content_type_badge_class(self) -> str:
        """CSS class for content type badge"""
        type_classes = {
            "article": "type-article",
            "video": "type-video",
            "paper": "type-paper",
            "code_repo": "type-code",
            "reference": "type-reference",
            "misc": "type-misc",
        }
        return type_classes.get(self.content_type or "", "type-default")

    @property
    def read_time_display(self) -> str:
        """Format read time for display"""
        if self.video_seconds:
            minutes = self.video_seconds // 60
            return f"{minutes}m video"
        if self.est_read_min:
            return f"{self.est_read_min}m read"
        if self.word_count:
            # Estimate ~200 words per minute
            est = max(1, self.word_count // 200)
            return f"~{est}m read"
        return "-"


class TabListResponse(BaseModel):
    """Response for tab list queries"""
    tabs: list[TabDisplay]
    total: int
    page: int
    per_page: int
    total_pages: int
    has_next: bool
    has_prev: bool


class ExportRequest(BaseModel):
    """Request for exporting tabs"""
    tab_ids: list[int] = Field(..., min_length=1, description="IDs of tabs to export")


class TabExport(BaseModel):
    """Tab data for export"""
    url: str
    title: Optional[str]
    summary: Optional[str]
    content_type: Optional[str]
    tags: list[str]
    projects: list[str]
    est_read_min: Optional[int]
    priority: Optional[str]
    window_label: Optional[str]
    created_at: str

    def to_markdown(self) -> str:
        """Convert to markdown format"""
        lines = []
        lines.append(f"## [{self.title or 'Untitled'}]({self.url})")
        lines.append("")

        if self.summary:
            lines.append(self.summary)
            lines.append("")

        metadata = []
        if self.content_type:
            metadata.append(f"**Type:** {self.content_type}")
        if self.est_read_min:
            metadata.append(f"**Read time:** {self.est_read_min} min")
        if self.priority:
            metadata.append(f"**Priority:** {self.priority}")
        if self.tags:
            metadata.append(f"**Tags:** {', '.join(self.tags)}")
        if self.projects:
            metadata.append(f"**Projects:** {', '.join(self.projects)}")

        if metadata:
            lines.append(" | ".join(metadata))
            lines.append("")

        lines.append("---")
        lines.append("")

        return "\n".join(lines)


class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    version: str
    database: str
</file>

<file path=".dockerignore">
# Git
.git
.gitignore

# Python
__pycache__
*.py[cod]
*$py.class
*.so
.Python
venv/
.venv/
env/
.env
*.egg-info/
.eggs/
dist/
build/

# IDE
.idea/
.vscode/
*.swp
*.swo

# Testing
.pytest_cache/
.coverage
htmlcov/
tests/

# Documentation
*.md
!README.md

# Local development
.env
.env.local
*.log

# Docker
Dockerfile*
docker-compose*.yml
.dockerignore

# Misc
.DS_Store
*.bak
</file>

<file path="CLAUDE_CODE_SPEC.md">
# TabBacklog v1 - Implementation Specification for Claude Code

## Project Summary

Build a Firefox tab management system that captures ~1000 browser tabs into a structured database, enriches them with LLM-generated summaries and metadata, and provides a searchable web interface for processing and exporting.

## Tech Stack

- **Database**: PostgreSQL (Supabase)
- **Backend Services**: Python 3.11+, FastAPI
- **LLM Framework**: DSPy with Llama 3.1 8B Instruct
- **Orchestration**: n8n workflows
- **Frontend**: HTMX + server-side rendering
- **Deployment**: Podman containers
- **Key Libraries**: httpx, BeautifulSoup4, yt-dlp, asyncpg/psycopg

## Architecture Overview

```
Firefox Bookmarks Export
    ↓
Ingest Script (CLI) → Supabase/Postgres
    ↓
n8n Orchestrator
    ├→ Fetch+Parse Microservice (plugins for YouTube, Twitter, generic HTML)
    ├→ LLM Enrichment Service (DSPy + local Llama)
    └→ Update Database (parsed content, enrichments, tags)
    ↓
Web UI (HTMX) ← User filters, searches, marks processed, exports
```

## Project Structure

```
tabbacklog/
├── README.md
├── docker-compose.yml (or podman-compose)
├── .env.example
│
├── database/
│   ├── schema/
│   │   ├── 01_core_tables.sql
│   │   ├── 02_indexes.sql
│   │   ├── 03_extensions.sql
│   │   └── 04_seed_data.sql
│   └── migrations/
│
├── ingest/
│   ├── __init__.py
│   ├── cli.py              # Main ingest script
│   ├── firefox_parser.py   # Parse Firefox bookmarks HTML
│   └── db.py               # Database operations
│
├── parser_service/
│   ├── __init__.py
│   ├── main.py            # FastAPI app
│   ├── parsers/
│   │   ├── __init__.py
│   │   ├── base.py        # BaseParser, ParsedPage
│   │   ├── registry.py    # Parser registration
│   │   ├── generic.py     # GenericHtmlParser
│   │   ├── youtube.py     # YouTubeParser
│   │   └── twitter.py     # TwitterParser
│   ├── models.py          # Pydantic models
│   └── Dockerfile
│
├── enrichment_service/
│   ├── __init__.py
│   ├── main.py            # FastAPI app with DSPy
│   ├── dspy_setup.py      # DSPy configuration
│   ├── models.py          # Pydantic enrichment model
│   └── Dockerfile
│
├── web_ui/
│   ├── __init__.py
│   ├── main.py            # FastAPI app
│   ├── routes/
│   │   ├── __init__.py
│   │   ├── tabs.py        # Tab listing/filtering
│   │   └── export.py      # Export endpoints
│   ├── templates/
│   │   ├── base.html
│   │   ├── index.html
│   │   └── fragments/
│   │       └── tab_rows.html
│   ├── static/
│   │   ├── css/
│   │   └── js/
│   └── Dockerfile
│
├── n8n/
│   └── workflows/
│       └── enrich_tabs.json
│
├── shared/
│   ├── __init__.py
│   ├── config.py         # Shared configuration
│   └── db.py             # Database connection utilities
│
└── tests/
    ├── test_parsers.py
    ├── test_enrichment.py
    └── test_ingest.py
```

## Implementation Phases

### PHASE 1: Database Setup

**Files to create:**
- `database/schema/01_core_tables.sql`
- `database/schema/02_indexes.sql`
- `database/schema/03_extensions.sql`
- `database/schema/04_seed_data.sql`

**Key tables:**
- `tab_item` - Main tab record with status tracking
- `tab_parsed` - Parsed content from fetch/parse
- `tab_enrichment` - LLM-generated metadata
- `tab_enrichment_history` - Enrichment run history
- `tag` - User-defined tags
- `tab_tag` - Many-to-many tag relationships
- `tab_embedding` - Vector embeddings for semantic search
- `event_log` - System events for debugging

**Requirements:**
- Enable `pg_trgm` extension for fuzzy search
- Enable `pgvector` extension for semantic search
- Create GIN indexes on text columns for trigram search
- Add unique constraint on `(user_id, url)` in `tab_item`
- All tables include: `user_id`, `created_at`, `updated_at`, `deleted_at`

### PHASE 2: Ingest Script

**Files to create:**
- `ingest/cli.py`
- `ingest/firefox_parser.py`
- `ingest/db.py`

**Functionality:**
- Parse Firefox bookmarks HTML export (BeautifulSoup)
- Look for "Session-" folders containing tab URLs
- Extract: URL, page title, window label
- Upsert into `tab_item` (dedupe on user_id + url)
- Log events to `event_log`
- CLI arguments: `--file`, `--user-id`

**Example usage:**
```bash
python -m ingest.cli --file ~/bookmarks.html --user-id UUID
```

### PHASE 3: Parser Service

**Files to create:**
- `parser_service/main.py`
- `parser_service/parsers/base.py`
- `parser_service/parsers/registry.py`
- `parser_service/parsers/generic.py`
- `parser_service/parsers/youtube.py`
- `parser_service/parsers/twitter.py`
- `parser_service/models.py`
- `parser_service/Dockerfile`

**Parser Plugin System:**

Create base class:
```python
@dataclass
class ParsedPage:
    site_kind: str
    title: str | None
    text_full: str | None
    word_count: int | None
    video_seconds: int | None
    metadata: dict
```

**GenericHtmlParser:**
- Extract `<title>` tag
- Extract all `<p>` text and article content
- Calculate word count
- site_kind = 'generic_html'

**YouTubeParser:**
- Match URLs: youtube.com/watch, youtu.be
- Use `yt-dlp -J URL` to fetch JSON metadata
- Extract: title, description, duration, uploader
- site_kind = 'youtube'

**TwitterParser:**
- Match URLs: twitter.com, x.com
- Parse meta tags for tweet text
- Extract author, timestamp
- site_kind = 'twitter'

**FastAPI Endpoints:**
```
POST /fetch_parse
{
  "url": "https://example.com"
}
→ Returns ParsedPage JSON
```

### PHASE 4: LLM Enrichment Service

**Files to create:**
- `enrichment_service/main.py`
- `enrichment_service/dspy_setup.py`
- `enrichment_service/models.py`
- `enrichment_service/Dockerfile`

**DSPy Configuration:**
- Initialize with JSONAdapter
- Connect to OpenAI-compatible API (LM Studio, Ollama, or custom)
- Use Llama 3.1 8B Instruct model

**Pydantic Model:**
```python
class Enrichment(BaseModel):
    summary: str
    content_type: Literal["article", "video", "paper", "code_repo", "reference", "misc"]
    tags: List[str]  # e.g., ["#video", "#longread"]
    projects: List[str]  # argumentation_on_the_web, democratic_economic_planning, other_research
    est_read_min: int | None
    priority: Literal["high", "medium", "low"] | None
```

**FastAPI Endpoints:**
```
POST /enrich_tab
{
  "url": "...",
  "title": "...",
  "site_kind": "...",
  "text": "..."
}
→ Returns Enrichment JSON
```

**Error Handling:**
- Retry on validation failure (max 3 attempts)
- Return 500 with `raw_output` if persistent failure

### PHASE 5: n8n Orchestration Workflow

**File to create:**
- `n8n/workflows/enrich_tabs.json`

**Workflow Steps:**

1. **Cron Trigger** (every 10 minutes)
2. **Postgres: Get New Tabs**
   - Query: `SELECT * FROM tab_item WHERE status = 'new' AND deleted_at IS NULL LIMIT 10`
3. **Split in Batches** (process 2 at a time)
4. **For each tab:**
   - **Update status to 'fetch_pending'**
   - **HTTP: POST to parser service** `/fetch_parse`
   - **On success:**
     - Insert/update `tab_parsed`
     - Update status to 'parsed'
     - Log event
   - **On error:**
     - Update status to 'fetch_error'
     - Log error
5. **For parsed tabs:**
   - **Update status to 'llm_pending'**
   - **HTTP: POST to enrichment service** `/enrich_tab`
   - **On success:**
     - Upsert `tab_enrichment`
     - Insert `tab_enrichment_history`
     - Upsert tags and `tab_tag` relationships
     - Update status to 'enriched'
     - Log event
   - **On error:**
     - Update status to 'llm_error'
     - Log error
6. **Aggregate Errors**
   - If any errors, send email summary

### PHASE 6: Web UI (HTMX)

**Files to create:**
- `web_ui/main.py`
- `web_ui/routes/tabs.py`
- `web_ui/routes/export.py`
- `web_ui/templates/base.html`
- `web_ui/templates/index.html`
- `web_ui/templates/fragments/tab_rows.html`
- `web_ui/static/css/style.css`
- `web_ui/Dockerfile`

**Routes:**

**GET /**
- Render main page with filters
- Auto-load tabs via HTMX

**GET /tabs** (HTMX fragment)
- Query parameters:
  - `content_type`: filter by type
  - `project`: filter by project tag
  - `read_time_max`: filter by estimated read time
  - `status`: filter by pipeline status
  - `is_processed`: filter by processed flag
  - `search`: fuzzy text search
  - `semantic_search`: semantic search mode
- Return `<tbody>` with tab rows
- Each row includes:
  - Checkbox for selection
  - Title (link to URL)
  - Content type badge
  - Tags
  - Estimated read time
  - Processed toggle button

**POST /tabs/{id}/toggle_processed**
- Toggle `is_processed` flag
- Set/clear `processed_at`
- Log event
- Return updated row HTML

**POST /export/json**
- Accept list of tab IDs
- Return JSON array

**POST /export/markdown**
- Accept list of tab IDs
- Generate Markdown for each tab
- Return as downloadable file

**UI Features:**
- Filter dropdowns for content_type, project, status
- Search box with debounce
- Select all/none checkboxes
- Bulk export buttons
- Responsive table
- HTMX for dynamic updates without page reload

### PHASE 7: Search Implementation

**Fuzzy Search (pg_trgm):**
- Already enabled in schema
- Use in `/tabs` query:
  ```sql
  WHERE (tab_item.page_title % :search 
      OR tab_enrichment.summary % :search)
  ```

**Semantic Search (optional):**
- Create embedding generation job
- Use same model for query and document embeddings
- Query with vector similarity:
  ```sql
  SELECT ... FROM tab_embedding
  ORDER BY embedding <-> :query_embedding
  LIMIT 50
  ```

### PHASE 8: Containerization

**Files to create:**
- `docker-compose.yml` (or `podman-compose.yml`)
- `.env.example`
- Individual Dockerfiles for each service

**Containers:**
- `tabbacklog-parser` - Parser microservice
- `tabbacklog-llm` - Enrichment service
- `tabbacklog-ui` - Web UI
- `tabbacklog-n8n` - n8n orchestrator

**Environment Variables:**
```bash
DATABASE_URL=postgresql://user:pass@host/db
SUPABASE_SERVICE_ROLE_KEY=...
LLM_API_BASE=http://localhost:1234/v1
LLM_API_KEY=...
LLM_MODEL_NAME=llama-3.1-8b-instruct
N8N_SMTP_HOST=...
N8N_SMTP_USER=...
N8N_SMTP_PASS=...
APP_SECRET_KEY=...
```

## Implementation Order

1. Set up database schema and extensions
2. Build and test ingest script
3. Build parser service with all plugins
4. Build enrichment service with DSPy
5. Create n8n workflow
6. Build web UI
7. Add search functionality
8. Containerize everything
9. Write documentation

## Testing Checklist

- [ ] Ingest script correctly parses Firefox bookmarks HTML
- [ ] Parser service handles generic HTML, YouTube, Twitter
- [ ] Enrichment service returns valid structured JSON
- [ ] n8n workflow processes tabs through full pipeline
- [ ] Web UI filters and search work correctly
- [ ] Export generates valid JSON and Markdown
- [ ] Containers start and communicate properly
- [ ] Error handling logs to event_log

## Key Design Principles

1. **Modularity**: Each service is independent and replaceable
2. **Idempotency**: Rerunning operations is safe
3. **Observability**: All operations logged to event_log
4. **Extensibility**: Easy to add new parsers or enrichment features
5. **Offline-first**: No real-time browser integration (batch processing)

## Non-Goals for v1

- Real-time tab syncing
- Multi-user authentication
- Firefox extension
- Advanced semantic features beyond basic scaffolding
- WebDriver/Selenium automation

## Success Criteria

- Successfully ingest 1000+ tabs from Firefox export
- Automatically enrich tabs with summaries and metadata
- Searchable, filterable web interface
- Export capabilities for Obsidian integration
- All services containerized and documented
</file>

<file path="config.py">
"""
TabBacklog v1 - Shared Configuration Module

This module provides centralized configuration management for all services.
It loads settings from environment variables and provides typed access.
"""

import os
from pathlib import Path
from typing import Optional
from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class DatabaseSettings(BaseSettings):
    """Database configuration"""
    url: str = Field(alias="DATABASE_URL")
    pool_size: int = Field(default=10, alias="DB_POOL_SIZE")
    max_overflow: int = Field(default=20, alias="DB_MAX_OVERFLOW")
    
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")


class LLMSettings(BaseSettings):
    """LLM service configuration"""
    api_base: str = Field(alias="LLM_API_BASE")
    api_key: str = Field(default="dummy_key", alias="LLM_API_KEY")
    model_name: str = Field(alias="LLM_MODEL_NAME")
    timeout: int = Field(default=60, alias="LLM_TIMEOUT")
    max_retries: int = Field(default=3, alias="MAX_RETRIES")
    
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")


class ServiceSettings(BaseSettings):
    """Service endpoint configuration"""
    parser_url: str = Field(alias="PARSER_SERVICE_URL")
    enrichment_url: str = Field(alias="ENRICHMENT_SERVICE_URL")
    web_ui_url: str = Field(alias="WEB_UI_URL")
    
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")


class ProcessingSettings(BaseSettings):
    """Processing and batch configuration"""
    batch_size: int = Field(default=10, alias="BATCH_SIZE")
    max_concurrent: int = Field(default=2, alias="MAX_CONCURRENT_REQUESTS")
    fetch_timeout: int = Field(default=30, alias="FETCH_TIMEOUT")
    retry_delay: int = Field(default=5, alias="RETRY_DELAY")
    
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")


class SearchSettings(BaseSettings):
    """Search configuration"""
    embedding_model: str = Field(
        default="sentence-transformers/all-MiniLM-L6-v2",
        alias="EMBEDDING_MODEL"
    )
    embedding_dimension: int = Field(default=384, alias="EMBEDDING_DIMENSION")
    similarity_threshold: float = Field(default=0.3, alias="SIMILARITY_THRESHOLD")
    
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")


class AppSettings(BaseSettings):
    """General application settings"""
    env: str = Field(default="development", alias="APP_ENV")
    secret_key: str = Field(alias="APP_SECRET_KEY")
    debug: bool = Field(default=False, alias="APP_DEBUG")
    default_user_id: str = Field(alias="DEFAULT_USER_ID")
    log_level: str = Field(default="INFO", alias="LOG_LEVEL")
    log_format: str = Field(default="text", alias="LOG_FORMAT")
    
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")


class Config:
    """Main configuration class that combines all settings"""
    
    def __init__(self):
        self.database = DatabaseSettings()
        self.llm = LLMSettings()
        self.services = ServiceSettings()
        self.processing = ProcessingSettings()
        self.search = SearchSettings()
        self.app = AppSettings()
    
    @property
    def is_development(self) -> bool:
        return self.app.env == "development"
    
    @property
    def is_production(self) -> bool:
        return self.app.env == "production"


# Global config instance
config = Config()


# Helper function to get config
def get_config() -> Config:
    """Get the global configuration instance"""
    return config


# Helper function to load environment from file
def load_env(env_file: str = ".env") -> None:
    """Load environment variables from file"""
    from dotenv import load_dotenv
    env_path = Path(env_file)
    if env_path.exists():
        load_dotenv(env_path)
    else:
        print(f"Warning: {env_file} not found")


# Example usage:
if __name__ == "__main__":
    load_env()
    cfg = get_config()
    
    print(f"Environment: {cfg.app.env}")
    print(f"Database URL: {cfg.database.url[:50]}...")
    print(f"LLM Model: {cfg.llm.model_name}")
    print(f"Parser Service: {cfg.services.parser_url}")
    print(f"Batch Size: {cfg.processing.batch_size}")
</file>

<file path="pytest.ini">
[pytest]
testpaths = tests
asyncio_mode = auto
markers =
    e2e: End-to-end browser tests (require running server)
    unit: Unit tests
    slow: Slow tests that may take a while
addopts = -v --tb=short
filterwarnings =
    ignore::DeprecationWarning
</file>

<file path="test_phoenix_setup.py">
#!/usr/bin/env python3
"""
Quick validation script to check Phoenix integration setup.
Run this to verify all dependencies and configuration are correct.
"""

import sys

def check_imports():
    """Check if all required Phoenix packages are importable."""
    print("Checking Phoenix dependencies...")
    
    required_packages = [
        ("openinference.instrumentation.dspy", "DSPy instrumentation"),
        ("opentelemetry.sdk.trace", "OpenTelemetry SDK"),
        ("opentelemetry.exporter.otlp.proto.grpc.trace_exporter", "OTLP exporter"),
    ]
    
    missing = []
    for package, description in required_packages:
        try:
            __import__(package)
            print(f"  ✓ {description}")
        except ImportError as e:
            print(f"  ✗ {description}: {e}")
            missing.append(package)
    
    return len(missing) == 0

def check_dspy():
    """Check if DSPy is properly installed."""
    print("\nChecking DSPy installation...")
    try:
        import dspy
        print(f"  ✓ DSPy version: {dspy.__version__ if hasattr(dspy, '__version__') else 'unknown'}")
        
        # Check for modern API
        if hasattr(dspy, 'Predict'):
            print("  ✓ Modern DSPy API (dspy.Predict) available")
        else:
            print("  ✗ Modern DSPy API not found")
            return False
            
        return True
    except ImportError as e:
        print(f"  ✗ DSPy not installed: {e}")
        return False

def check_env_vars():
    """Check if Phoenix environment variables are set."""
    print("\nChecking environment variables...")
    import os
    
    env_vars = [
        ("OTEL_EXPORTER_OTLP_ENDPOINT", "http://phoenix:4317"),
        ("PHOENIX_PROJECT_NAME", "tabbacklog"),
    ]
    
    all_set = True
    for var, default in env_vars:
        value = os.environ.get(var, default)
        if value and value != "disabled":
            print(f"  ✓ {var}={value}")
        else:
            print(f"  ℹ {var} not set (will use default: {default})")
    
    return all_set

def main():
    """Run all checks."""
    print("=" * 60)
    print("Phoenix Integration Setup Validation")
    print("=" * 60)
    
    checks = [
        ("Dependencies", check_imports),
        ("DSPy", check_dspy),
        ("Environment", check_env_vars),
    ]
    
    results = []
    for name, check_func in checks:
        try:
            result = check_func()
            results.append((name, result))
        except Exception as e:
            print(f"\n✗ Error checking {name}: {e}")
            results.append((name, False))
    
    print("\n" + "=" * 60)
    print("Summary:")
    print("=" * 60)
    
    all_passed = True
    for name, passed in results:
        status = "✓ PASS" if passed else "✗ FAIL"
        print(f"  {status}: {name}")
        if not passed:
            all_passed = False
    
    print("=" * 60)
    
    if all_passed:
        print("\n✓ All checks passed! Phoenix integration is ready.")
        print("\nNext steps:")
        print("  1. Start services: docker-compose up -d")
        print("  2. Access Phoenix: http://localhost:6006")
        print("  3. Trigger enrichment to see traces")
        return 0
    else:
        print("\n✗ Some checks failed. Please install missing dependencies:")
        print("  pip install -r requirements.txt")
        return 1

if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="enrichment_service/Dockerfile">
FROM python:3.11-slim

WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy enrichment service code
COPY enrichment_service/ ./enrichment_service/

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Environment variables (override in docker-compose or at runtime)
ENV LLM_API_BASE=http://localhost:1234/v1
ENV LLM_API_KEY=dummy_key
ENV LLM_MODEL_NAME=llama-3.1-8b-instruct
ENV LLM_TIMEOUT=60
ENV LLM_TEMPERATURE=0.7
ENV MAX_RETRIES=3

# Phoenix observability
ENV OTEL_EXPORTER_OTLP_ENDPOINT=http://phoenix:4317
ENV PHOENIX_PROJECT_NAME=tabbacklog

# Expose port
EXPOSE 8002

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD python -c "import httpx; httpx.get('http://localhost:8002/health').raise_for_status()"

# Run the service
CMD ["uvicorn", "enrichment_service.main:app", "--host", "0.0.0.0", "--port", "8002"]
</file>

<file path="enrichment_service/dspy_setup.py">
"""
TabBacklog v1 - DSPy Configuration and Enrichment Module

Sets up DSPy with OpenAI-compatible API and defines the enrichment signature.
"""

import logging
import os
from typing import Literal, Optional

import dspy
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


# Project categories that the LLM should classify into
PROJECT_CATEGORIES = [
    "argumentation_on_the_web",
    "democratic_economic_planning",
    "other_research",
    "personal",
    "work",
]

# Content type options
CONTENT_TYPES = ["article", "video", "paper", "code_repo", "reference", "misc"]


class EnrichmentOutput(BaseModel):
    """Structured output schema for the enrichment LLM call."""
    summary: str = Field(
        description="A brief 2-3 sentence summary of the content"
    )
    content_type: Literal["article", "video", "paper", "code_repo", "reference", "misc"] = Field(
        description="The type of content"
    )
    tags: list[str] = Field(
        description="3-5 relevant tags starting with # (e.g., #tutorial, #longread, #video)"
    )
    projects: list[str] = Field(
        description=f"Related project categories from: {', '.join(PROJECT_CATEGORIES)}"
    )
    est_read_min: Optional[int] = Field(
        description="Estimated reading/watching time in minutes"
    )
    priority: Optional[Literal["high", "medium", "low"]] = Field(
        description="Suggested priority based on content importance"
    )


class EnrichTabSignature(dspy.Signature):
    """
    Analyze web content and generate structured metadata for organization.

    Given information about a web page (URL, title, content), generate:
    - A concise summary
    - Content type classification
    - Relevant tags
    - Project categorization
    - Estimated reading time
    - Priority level
    """

    url: str = dspy.InputField(desc="The URL of the content")
    title: str = dspy.InputField(desc="The title of the content")
    site_kind: str = dspy.InputField(desc="Type of site (youtube, twitter, generic_html)")
    text: str = dspy.InputField(desc="The main text content (may be truncated)")
    word_count: int = dspy.InputField(desc="Word count of the full content")
    video_seconds: int = dspy.InputField(desc="Video duration in seconds (0 if not a video)")

    enrichment: EnrichmentOutput = dspy.OutputField(
        desc="Structured enrichment metadata"
    )


class TabEnricher(dspy.Module):
    """DSPy module for enriching tab content."""

    def __init__(self):
        super().__init__()
        self.enrich = dspy.Predict(EnrichTabSignature)

    def forward(
        self,
        url: str,
        title: str,
        site_kind: str,
        text: str,
        word_count: int = 0,
        video_seconds: int = 0,
    ) -> EnrichmentOutput:
        """
        Generate enrichment for a tab.

        Args:
            url: The URL of the content
            title: The title of the content
            site_kind: Type of site (youtube, twitter, generic_html)
            text: The main text content
            word_count: Word count of the content
            video_seconds: Video duration in seconds (0 if not video)

        Returns:
            EnrichmentOutput with structured metadata
        """
        # Truncate text if too long (keep under token limits)
        max_text_chars = 4000
        if len(text) > max_text_chars:
            text = text[:max_text_chars] + "... [truncated]"

        result = self.enrich(
            url=url,
            title=title or "Untitled",
            site_kind=site_kind,
            text=text or "No content available",
            word_count=word_count or 0,
            video_seconds=video_seconds or 0,
        )

        return result.enrichment


def configure_dspy(
    api_base: str,
    api_key: str,
    model_name: str,
    timeout: int = 60,
    temperature: float = 0.7,
) -> None:
    """
    Configure DSPy to use an OpenAI-compatible API.

    Args:
        api_base: Base URL for the API (e.g., http://localhost:1234/v1)
        api_key: API key (can be dummy for local models)
        model_name: Model name to use
        timeout: Request timeout in seconds
        temperature: Sampling temperature (0.0-1.0, higher = more creative)
    """
    logger.info(f"Configuring DSPy with model: {model_name} at {api_base}")

    lm = dspy.LM(
        model=f"openai/{model_name}",
        api_base=api_base,
        api_key=api_key,
        temperature=temperature,
        max_tokens=1024,
        timeout=timeout,
    )

    dspy.configure(lm=lm)
    logger.info("DSPy configured successfully")


def configure_dspy_from_env() -> str:
    """
    Configure DSPy from environment variables.

    Environment variables:
        LLM_API_BASE: Base URL for the API
        LLM_API_KEY: API key
        LLM_MODEL_NAME: Model name
        LLM_TIMEOUT: Request timeout in seconds
        LLM_TEMPERATURE: Sampling temperature (0.0-1.0)

    Returns:
        The model name that was configured
    """
    api_base = os.environ.get("LLM_API_BASE", "http://localhost:1234/v1")
    api_key = os.environ.get("LLM_API_KEY", "dummy_key")
    model_name = os.environ.get("LLM_MODEL_NAME", "llama-3.1-8b-instruct")
    timeout = int(os.environ.get("LLM_TIMEOUT", "60"))
    temperature = float(os.environ.get("LLM_TEMPERATURE", "0.7"))

    configure_dspy(api_base, api_key, model_name, timeout, temperature)
    return model_name


def get_enricher() -> TabEnricher:
    """Get a configured TabEnricher instance."""
    return TabEnricher()


async def test_llm_connection() -> bool:
    """
    Test if the LLM is reachable and responding.

    Performs a lightweight check by verifying DSPy configuration
    rather than making an actual LLM call.

    Returns:
        True if connection successful, False otherwise
    """
    try:
        # Check if DSPy is configured
        if dspy.settings.lm is None:
            logger.warning("DSPy LM not configured")
            return False
        
        # Verify we can create an enricher instance
        enricher = get_enricher()
        if enricher is None or enricher.enrich is None:
            logger.warning("Failed to create enricher instance")
            return False
        
        return True
    except Exception as e:
        logger.warning(f"LLM connection test failed: {e}")
        return False
</file>

<file path="enrichment_service/main.py">
"""
TabBacklog v1 - Enrichment Service

FastAPI service for LLM-based content enrichment using DSPy.
Generates summaries, classifications, and metadata for tabs.
"""

import logging
import os
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Phoenix observability imports
from openinference.instrumentation.dspy import DSPyInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry.sdk.resources import Resource

from . import __version__
from .models import (
    EnrichmentRequest,
    Enrichment,
    EnrichmentResponse,
    EnrichmentErrorResponse,
    HealthResponse,
)
from .dspy_setup import (
    configure_dspy_from_env,
    get_enricher,
    test_llm_connection,
    TabEnricher,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Global state
_model_name: str = ""
_enricher: Optional[TabEnricher] = None


def setup_phoenix_tracing():
    """
    Configure Phoenix/OpenTelemetry tracing for LLM observability.
    
    This sets up automatic instrumentation of DSPy calls to track:
    - LLM requests and responses
    - Token usage
    - Latency
    - Errors and retries
    """
    phoenix_endpoint = os.environ.get("OTEL_EXPORTER_OTLP_ENDPOINT", "http://phoenix:4317")
    project_name = os.environ.get("PHOENIX_PROJECT_NAME", "tabbacklog")
    
    # Only enable if Phoenix endpoint is configured
    if not phoenix_endpoint or phoenix_endpoint == "disabled":
        logger.info("Phoenix tracing disabled")
        return
    
    try:
        logger.info(f"Configuring Phoenix tracing to {phoenix_endpoint}")
        
        # Create resource with project name
        resource = Resource(attributes={
            "service.name": project_name,
            "service.version": __version__,
        })
        
        # Configure tracer provider
        tracer_provider = trace_sdk.TracerProvider(resource=resource)
        
        # Add OTLP exporter
        span_exporter = OTLPSpanExporter(endpoint=phoenix_endpoint, insecure=True)
        tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter))
        
        # Set as global tracer provider
        trace_api.set_tracer_provider(tracer_provider)
        
        # Instrument DSPy
        DSPyInstrumentor().instrument()
        
        logger.info("Phoenix tracing configured successfully")
    except Exception as e:
        logger.warning(f"Failed to configure Phoenix tracing: {e}")
        logger.warning("Continuing without observability")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    global _model_name, _enricher

    # Configure Phoenix tracing
    setup_phoenix_tracing()

    # Configure DSPy on startup
    logger.info("Enrichment service starting...")
    try:
        _model_name = configure_dspy_from_env()
        _enricher = get_enricher()
        logger.info(f"Enrichment service ready with model: {_model_name}")
    except Exception as e:
        logger.error(f"Failed to configure DSPy: {e}")
        raise

    yield

    logger.info("Enrichment service shutting down")


app = FastAPI(
    title="TabBacklog Enrichment Service",
    description="LLM-based content enrichment for tab metadata generation",
    version=__version__,
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def get_max_retries() -> int:
    """Get max retries from environment."""
    return int(os.environ.get("MAX_RETRIES", "3"))


class EnrichmentError(Exception):
    """Custom exception for enrichment failures."""
    def __init__(self, message: str, raw_output: Optional[str] = None):
        super().__init__(message)
        self.raw_output = raw_output


def enrich_with_retry(request: EnrichmentRequest) -> Enrichment:
    """
    Attempt enrichment with retries on validation failure.

    Args:
        request: The enrichment request

    Returns:
        Enrichment object on success

    Raises:
        EnrichmentError: If all retries fail
    """
    max_retries = get_max_retries()
    last_error = None
    raw_output = None

    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"Enrichment attempt {attempt}/{max_retries} for {request.url}")

            result = _enricher(
                url=request.url,
                title=request.title or "Untitled",
                site_kind=request.site_kind,
                text=request.text or "",
                word_count=request.word_count or 0,
                video_seconds=request.video_seconds or 0,
            )

            # Convert DSPy output to our Pydantic model
            enrichment = Enrichment(
                summary=result.summary,
                content_type=result.content_type,
                tags=result.tags[:10] if result.tags else [],  # Limit tags
                projects=result.projects[:5] if result.projects else [],  # Limit projects
                est_read_min=result.est_read_min,
                priority=result.priority,
            )

            logger.info(f"Successfully enriched {request.url} on attempt {attempt}")
            return enrichment

        except Exception as e:
            last_error = str(e)
            # Try to capture raw output if available
            if hasattr(e, 'raw_output'):
                raw_output = e.raw_output
            logger.warning(f"Enrichment attempt {attempt} failed: {e}")

    raise EnrichmentError(
        f"Enrichment failed after {max_retries} attempts: {last_error}",
        raw_output=raw_output,
    )


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """Check service health and LLM connection status."""
    llm_ok = await test_llm_connection()

    return HealthResponse(
        status="healthy" if llm_ok else "degraded",
        version=__version__,
        model_name=_model_name,
        llm_status="connected" if llm_ok else "disconnected",
    )


@app.post(
    "/enrich_tab",
    response_model=EnrichmentResponse,
    responses={
        400: {"model": EnrichmentErrorResponse, "description": "Invalid request"},
        500: {"model": EnrichmentErrorResponse, "description": "Enrichment failed"},
    },
    tags=["Enrichment"],
)
async def enrich_tab(request: EnrichmentRequest):
    """
    Generate LLM-based enrichment for a tab.

    Takes parsed content (URL, title, text) and generates:
    - Summary: Brief 2-3 sentence description
    - Content type: article, video, paper, code_repo, reference, or misc
    - Tags: Relevant hashtags for organization
    - Projects: Related project categories
    - Estimated read time: In minutes
    - Priority: high, medium, or low

    Automatically retries on validation failures (up to 3 attempts).
    """
    logger.info(f"Enrichment request for: {request.url}")

    if _enricher is None:
        raise HTTPException(
            status_code=500,
            detail=EnrichmentErrorResponse(
                error="Service not initialized",
                detail="DSPy enricher not configured",
                url=request.url,
            ).model_dump(),
        )

    try:
        enrichment = enrich_with_retry(request)

        return EnrichmentResponse(
            url=request.url,
            enrichment=enrichment,
            model_name=_model_name,
        )

    except EnrichmentError as e:
        logger.error(f"Enrichment failed for {request.url}: {e}")
        raise HTTPException(
            status_code=500,
            detail=EnrichmentErrorResponse(
                error="Enrichment failed",
                detail=str(e),
                url=request.url,
                raw_output=e.raw_output,
                attempts=get_max_retries(),
            ).model_dump(),
        )

    except Exception as e:
        logger.exception(f"Unexpected error enriching {request.url}")
        raise HTTPException(
            status_code=500,
            detail=EnrichmentErrorResponse(
                error="Internal error",
                detail=str(e),
                url=request.url,
            ).model_dump(),
        )


@app.post(
    "/enrich_batch",
    response_model=list[EnrichmentResponse],
    tags=["Enrichment"],
)
async def enrich_batch(requests: list[EnrichmentRequest]):
    """
    Enrich multiple tabs in a single request.

    Processes each tab sequentially. Failed enrichments are skipped
    and not included in the response (check response length vs input).
    """
    results = []

    for request in requests:
        try:
            enrichment = enrich_with_retry(request)
            results.append(EnrichmentResponse(
                url=request.url,
                enrichment=enrichment,
                model_name=_model_name,
            ))
        except Exception as e:
            logger.warning(f"Skipping failed enrichment for {request.url}: {e}")
            continue

    return results


@app.get("/model", tags=["Info"])
async def get_model_info():
    """Get information about the configured LLM model."""
    return {
        "model_name": _model_name,
        "max_retries": get_max_retries(),
    }


# Run with: uvicorn enrichment_service.main:app --host 0.0.0.0 --port 8002
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
</file>

<file path="n8n/README.md">
# TabBacklog n8n Workflows

This directory contains n8n workflow definitions for orchestrating the TabBacklog pipeline.

## Main Workflow: enrich_tabs.json

Processes new tabs through the fetch/parse and LLM enrichment pipeline.

### Workflow Steps

```
┌─────────────────┐
│  Cron Trigger   │  Every 10 minutes
│  (Schedule)     │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Get New Tabs   │  SELECT * FROM tab_item WHERE status = 'new' LIMIT 10
│  (Postgres)     │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Split In Batches│  Process 2 at a time
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Set Fetch       │  UPDATE status = 'fetch_pending'
│ Pending         │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Call Parser     │  POST /fetch_parse
│ Service         │
└────────┬────────┘
         │
    ┌────┴────┐
    │ Success?│
    └────┬────┘
    Yes  │  No
    ▼    │    ▼
┌────────┴──┐  ┌─────────────┐
│ Insert    │  │ Set Fetch   │
│ Parsed    │  │ Error       │
└────┬──────┘  └─────────────┘
     │
     ▼
┌─────────────────┐
│ Set LLM Pending │  UPDATE status = 'llm_pending'
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Call Enrichment │  POST /enrich_tab
│ Service         │
└────────┬────────┘
         │
    ┌────┴────┐
    │ Success?│
    └────┬────┘
    Yes  │  No
    ▼    │    ▼
┌────────┴───────┐  ┌─────────────┐
│ Upsert         │  │ Set LLM     │
│ Enrichment     │  │ Error       │
│ + History      │  └─────────────┘
│ + Tags         │
└────┬───────────┘
     │
     ▼
┌─────────────────┐
│ Set Enriched    │  UPDATE status = 'enriched'
└─────────────────┘
```

### Status Flow

```
new → fetch_pending → parsed → llm_pending → enriched
                  ↘            ↘
               fetch_error   llm_error
```

## Setup Instructions

### 1. Configure n8n Credentials

Create a PostgreSQL credential in n8n with name `TabBacklog Postgres`:
- Host: `postgres` (Docker service name) or your database host
- Database: `tabbacklog`
- User: Your database user (default: `postgres`)
- Password: Your database password (default: `postgres`)
- Port: `5432`

**Note:** When running in Docker, use the service name `postgres` as the host, not `localhost`.

### 2. Set Environment Variables

The workflow uses these environment variables (automatically set in docker-compose.yml):
- `PARSER_SERVICE_URL`: URL of the parser service (default: `http://parser:8001`)
- `ENRICHMENT_SERVICE_URL`: URL of the enrichment service (default: `http://enrichment:8002`)

These are configured in the `.env` file and passed to n8n via docker-compose.

### 3. Import the Workflow

1. Open n8n web interface
2. Go to Workflows → Import from File
3. Select `workflows/enrich_tabs.json`
4. Update the PostgreSQL credential reference
5. Activate the workflow

## Manual Trigger

To test the workflow manually:
1. Open the workflow in n8n
2. Click "Execute Workflow"
3. Check the execution log for results

## Customization

### Batch Size

Edit the "Set Config" node to change `batch_size` (default: 10)

### Schedule

Edit the "Every 10 Minutes" node to change the cron interval

### Concurrency

Edit the "Split In Batches" node to change `batchSize` (default: 2)

## Troubleshooting

### Workflow not triggering
- Check that the workflow is activated (toggle in top right)
- Verify n8n has correct timezone settings
- Check n8n logs: `docker compose logs n8n`

### Database connection errors
- Verify PostgreSQL credentials in n8n
- Ensure host is set to `postgres` (Docker service name), not `localhost`
- Check network connectivity: `docker exec tabbacklog-n8n ping postgres`
- Verify postgres is healthy: `docker compose ps postgres`

### Parser/Enrichment errors
- Check service health endpoints:
  ```bash
  curl http://localhost:8001/health  # parser
  curl http://localhost:8002/health  # enrichment
  ```
- Review service logs:
  ```bash
  docker compose logs parser
  docker compose logs enrichment
  ```
- Errors are logged to `event_log` table
- Verify services are healthy: `docker compose ps`

### Service URLs not resolving
- Ensure PARSER_SERVICE_URL and ENRICHMENT_SERVICE_URL use Docker service names
- Correct: `http://parser:8001` and `http://enrichment:8002`
- Wrong: `http://localhost:8001` (won't work inside Docker network)

## Monitoring

Query recent events:
```sql
SELECT * FROM event_log
WHERE event_type LIKE '%error%'
ORDER BY created_at DESC
LIMIT 20;
```

Check pipeline status:
```sql
SELECT status, COUNT(*)
FROM tab_item
WHERE deleted_at IS NULL
GROUP BY status;
```
</file>

<file path="web_ui/routes/__init__.py">
"""
TabBacklog v1 - Web UI Routes
"""

from .tabs import router as tabs_router
from .export import router as export_router
from .search import router as search_router

__all__ = ["tabs_router", "export_router", "search_router"]
</file>

<file path="web_ui/static/css/style.css">
/* TabBacklog v1 - Styles */

/* ========== Variables ========== */
:root {
    --color-bg: #f5f5f5;
    --color-bg-dark: #1a1a2e;
    --color-surface: #ffffff;
    --color-surface-dark: #16213e;
    --color-primary: #4361ee;
    --color-primary-hover: #3651d4;
    --color-secondary: #6c757d;
    --color-success: #28a745;
    --color-warning: #ffc107;
    --color-error: #dc3545;
    --color-text: #212529;
    --color-text-muted: #6c757d;
    --color-border: #dee2e6;

    --shadow-sm: 0 1px 2px rgba(0,0,0,0.05);
    --shadow-md: 0 4px 6px rgba(0,0,0,0.1);
    --shadow-lg: 0 10px 15px rgba(0,0,0,0.1);

    --radius-sm: 4px;
    --radius-md: 8px;
    --radius-lg: 12px;

    --font-sans: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
    --font-mono: 'SF Mono', 'Monaco', 'Inconsolata', monospace;
}

/* ========== Reset ========== */
*, *::before, *::after {
    box-sizing: border-box;
}

body {
    margin: 0;
    font-family: var(--font-sans);
    font-size: 14px;
    line-height: 1.5;
    color: var(--color-text);
    background: var(--color-bg);
}

/* ========== Layout ========== */
.header {
    background: var(--color-surface);
    border-bottom: 1px solid var(--color-border);
    padding: 0.75rem 1.5rem;
    position: sticky;
    top: 0;
    z-index: 100;
}

.header-content {
    max-width: 1400px;
    margin: 0 auto;
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.logo {
    font-size: 1.25rem;
    font-weight: 700;
    color: var(--color-primary);
    margin: 0;
}

.nav {
    display: flex;
    gap: 1rem;
}

.nav-link {
    color: var(--color-text-muted);
    text-decoration: none;
    padding: 0.5rem;
    border-radius: var(--radius-sm);
}

.nav-link:hover {
    color: var(--color-primary);
    background: rgba(67, 97, 238, 0.1);
}

.main-content {
    min-height: calc(100vh - 120px);
}

.container {
    max-width: 1400px;
    margin: 0 auto;
    padding: 1.5rem;
}

.footer {
    background: var(--color-surface);
    border-top: 1px solid var(--color-border);
    padding: 1rem;
    text-align: center;
    color: var(--color-text-muted);
    font-size: 0.875rem;
}

/* ========== Filters ========== */
.filters-section {
    background: var(--color-surface);
    border-radius: var(--radius-md);
    padding: 1rem;
    margin-bottom: 1rem;
    box-shadow: var(--shadow-sm);
}

.filters-row {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    align-items: flex-end;
}

.filter-group {
    display: flex;
    flex-direction: column;
    gap: 0.25rem;
}

.filter-group label {
    font-size: 0.75rem;
    font-weight: 500;
    color: var(--color-text-muted);
    text-transform: uppercase;
    letter-spacing: 0.05em;
}

.search-group {
    flex: 1;
    min-width: 200px;
}

.search-input,
.filter-select {
    padding: 0.5rem 0.75rem;
    border: 1px solid var(--color-border);
    border-radius: var(--radius-sm);
    font-size: 0.875rem;
    background: var(--color-surface);
    color: var(--color-text);
}

.search-input {
    width: 100%;
}

.filter-select {
    min-width: 120px;
}

.search-input:focus,
.filter-select:focus {
    outline: none;
    border-color: var(--color-primary);
    box-shadow: 0 0 0 2px rgba(67, 97, 238, 0.2);
}

/* ========== Actions Bar ========== */
.actions-bar {
    display: flex;
    flex-wrap: wrap;
    align-items: center;
    justify-content: space-between;
    gap: 1rem;
    padding: 0.75rem 1rem;
    background: var(--color-surface);
    border-radius: var(--radius-md);
    margin-bottom: 1rem;
    box-shadow: var(--shadow-sm);
}

.selection-info {
    display: flex;
    align-items: center;
    gap: 1rem;
}

.checkbox-label {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    cursor: pointer;
}

.selected-count {
    color: var(--color-text-muted);
    font-size: 0.875rem;
}

.export-buttons {
    display: flex;
    gap: 0.5rem;
}

.stats-summary {
    display: flex;
    gap: 1rem;
    font-size: 0.875rem;
    color: var(--color-text-muted);
}

.stat {
    padding: 0.25rem 0.5rem;
    background: var(--color-bg);
    border-radius: var(--radius-sm);
}

/* ========== Buttons ========== */
.btn {
    display: inline-flex;
    align-items: center;
    justify-content: center;
    gap: 0.5rem;
    padding: 0.5rem 1rem;
    font-size: 0.875rem;
    font-weight: 500;
    border: none;
    border-radius: var(--radius-sm);
    cursor: pointer;
    transition: all 0.15s ease;
    text-decoration: none;
}

.btn:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

.btn-primary {
    background: var(--color-primary);
    color: white;
}

.btn-primary:hover:not(:disabled) {
    background: var(--color-primary-hover);
}

.btn-secondary {
    background: var(--color-bg);
    color: var(--color-text);
    border: 1px solid var(--color-border);
}

.btn-secondary:hover:not(:disabled) {
    background: var(--color-border);
}

.btn-sm {
    padding: 0.25rem 0.5rem;
    font-size: 0.75rem;
}

.btn-icon {
    width: 32px;
    height: 32px;
    padding: 0;
    background: transparent;
    border: 1px solid var(--color-border);
    border-radius: var(--radius-sm);
}

.btn-icon:hover {
    background: var(--color-bg);
}

.btn-icon .icon {
    font-size: 1rem;
}

.btn-processed {
    background: var(--color-success);
    color: white;
    border-color: var(--color-success);
}

/* ========== Table ========== */
.tabs-section {
    background: var(--color-surface);
    border-radius: var(--radius-md);
    box-shadow: var(--shadow-sm);
    overflow: hidden;
}

.tabs-table {
    width: 100%;
    border-collapse: collapse;
}

.tabs-table th {
    padding: 0.75rem 1rem;
    text-align: left;
    font-weight: 600;
    font-size: 0.75rem;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    color: var(--color-text-muted);
    background: var(--color-bg);
    border-bottom: 1px solid var(--color-border);
}

.tabs-table td {
    padding: 0.75rem 1rem;
    border-bottom: 1px solid var(--color-border);
    vertical-align: top;
}

.tabs-table tr:last-child td {
    border-bottom: none;
}

.tab-row:hover {
    background: rgba(67, 97, 238, 0.02);
}

.tab-row.processed {
    opacity: 0.7;
    background: rgba(40, 167, 69, 0.05);
}

.col-checkbox {
    width: 40px;
}

.col-title {
    min-width: 300px;
}

.col-type,
.col-status,
.col-time {
    width: 100px;
}

.col-tags {
    width: 150px;
}

.col-actions {
    width: 80px;
}

/* ========== Title Cell ========== */
.title-cell {
    display: flex;
    flex-direction: column;
    gap: 0.25rem;
}

.tab-link {
    color: var(--color-text);
    text-decoration: none;
    font-weight: 500;
    word-break: break-word;
}

.tab-link:hover {
    color: var(--color-primary);
}

.window-label {
    font-size: 0.75rem;
    color: var(--color-text-muted);
    background: var(--color-bg);
    padding: 0.125rem 0.375rem;
    border-radius: var(--radius-sm);
    display: inline-block;
}

.tab-summary {
    margin: 0.25rem 0 0;
    font-size: 0.8125rem;
    color: var(--color-text-muted);
    line-height: 1.4;
}

/* ========== Badges ========== */
.badge {
    display: inline-block;
    padding: 0.25rem 0.5rem;
    font-size: 0.75rem;
    font-weight: 500;
    border-radius: var(--radius-sm);
    text-transform: capitalize;
}

.badge-default {
    background: var(--color-bg);
    color: var(--color-text-muted);
}

.badge-new {
    background: #e3f2fd;
    color: #1565c0;
}

.badge-pending {
    background: #fff3e0;
    color: #ef6c00;
}

.badge-parsed {
    background: #e8f5e9;
    color: #2e7d32;
}

.badge-success {
    background: #e8f5e9;
    color: #2e7d32;
}

.badge-error {
    background: #ffebee;
    color: #c62828;
}

.badge-processed {
    background: var(--color-success);
    color: white;
}

/* Content type badges */
.type-article { background: #e3f2fd; color: #1565c0; }
.type-video { background: #fce4ec; color: #c2185b; }
.type-paper { background: #f3e5f5; color: #7b1fa2; }
.type-code { background: #e8eaf6; color: #303f9f; }
.type-reference { background: #fff8e1; color: #ff8f00; }
.type-misc { background: var(--color-bg); color: var(--color-text-muted); }

/* Priority badges */
.badge-priority-high { background: #ffebee; color: #c62828; }
.badge-priority-medium { background: #fff3e0; color: #ef6c00; }
.badge-priority-low { background: #e8f5e9; color: #2e7d32; }

/* ========== Tags ========== */
.tags-list {
    display: flex;
    flex-wrap: wrap;
    gap: 0.25rem;
}

.tag {
    display: inline-block;
    padding: 0.125rem 0.375rem;
    font-size: 0.6875rem;
    background: var(--color-bg);
    color: var(--color-text-muted);
    border-radius: var(--radius-sm);
}

.tag-more {
    background: var(--color-primary);
    color: white;
}

/* ========== Actions ========== */
.action-buttons {
    display: flex;
    gap: 0.25rem;
}

/* ========== Pagination ========== */
.pagination-row td {
    background: var(--color-bg);
}

.pagination-controls {
    display: flex;
    align-items: center;
    justify-content: space-between;
}

.pagination-info {
    font-size: 0.875rem;
    color: var(--color-text-muted);
}

.pagination-buttons {
    display: flex;
    gap: 0.5rem;
}

/* ========== Empty & Loading States ========== */
.empty-cell,
.loading-cell {
    text-align: center;
    padding: 3rem 1rem !important;
}

.empty-state {
    color: var(--color-text-muted);
}

.empty-hint {
    font-size: 0.875rem;
    margin-top: 0.5rem;
}

.loading-spinner {
    width: 24px;
    height: 24px;
    border: 2px solid var(--color-border);
    border-top-color: var(--color-primary);
    border-radius: 50%;
    animation: spin 1s linear infinite;
    margin: 0 auto 0.5rem;
}

@keyframes spin {
    to { transform: rotate(360deg); }
}

/* ========== Modal ========== */
.modal {
    position: fixed;
    inset: 0;
    display: flex;
    align-items: center;
    justify-content: center;
    z-index: 1000;
}

.modal-backdrop {
    position: absolute;
    inset: 0;
    background: rgba(0, 0, 0, 0.5);
}

.modal-content {
    position: relative;
    background: var(--color-surface);
    border-radius: var(--radius-lg);
    box-shadow: var(--shadow-lg);
    max-width: 600px;
    max-height: 80vh;
    overflow-y: auto;
    width: 90%;
}

.modal-close {
    position: absolute;
    top: 1rem;
    right: 1rem;
    width: 32px;
    height: 32px;
    border: none;
    background: var(--color-bg);
    border-radius: 50%;
    font-size: 1.25rem;
    cursor: pointer;
    color: var(--color-text-muted);
}

.modal-close:hover {
    background: var(--color-border);
}

/* ========== Tab Detail ========== */
.tab-detail {
    padding: 1.5rem;
}

.detail-header {
    margin-bottom: 1rem;
    padding-right: 2rem;
}

.detail-title {
    font-size: 1.25rem;
    margin: 0 0 0.5rem;
}

.detail-title a {
    color: var(--color-text);
    text-decoration: none;
}

.detail-title a:hover {
    color: var(--color-primary);
}

.detail-url {
    font-size: 0.75rem;
    color: var(--color-text-muted);
    word-break: break-all;
    margin: 0;
}

.detail-badges {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
    margin-bottom: 1.5rem;
}

.detail-section {
    margin-bottom: 1.5rem;
}

.detail-section h3 {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    color: var(--color-text-muted);
    margin: 0 0 0.5rem;
}

.detail-summary {
    margin: 0;
    line-height: 1.6;
}

.detail-list {
    display: grid;
    grid-template-columns: auto 1fr;
    gap: 0.5rem 1rem;
    margin: 0;
}

.detail-list dt {
    font-weight: 500;
    color: var(--color-text-muted);
}

.detail-list dd {
    margin: 0;
}

.detail-tags {
    flex-wrap: wrap;
}

.project-badge {
    display: inline-block;
    padding: 0.25rem 0.5rem;
    font-size: 0.75rem;
    background: var(--color-primary);
    color: white;
    border-radius: var(--radius-sm);
}

.detail-footer {
    display: flex;
    gap: 0.5rem;
    padding-top: 1rem;
    border-top: 1px solid var(--color-border);
}

/* ========== Toast Notifications ========== */
#toast-container {
    position: fixed;
    bottom: 1rem;
    right: 1rem;
    z-index: 2000;
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

.toast {
    padding: 0.75rem 1rem;
    border-radius: var(--radius-sm);
    background: var(--color-surface);
    box-shadow: var(--shadow-lg);
    animation: slideIn 0.3s ease;
}

.toast-success { border-left: 3px solid var(--color-success); }
.toast-error { border-left: 3px solid var(--color-error); }
.toast-warning { border-left: 3px solid var(--color-warning); }
.toast-info { border-left: 3px solid var(--color-primary); }

@keyframes slideIn {
    from {
        transform: translateX(100%);
        opacity: 0;
    }
    to {
        transform: translateX(0);
        opacity: 1;
    }
}

/* ========== Search Mode Toggle ========== */
.search-wrapper {
    display: flex;
    align-items: center;
    gap: 0.5rem;
}

.search-wrapper .search-input {
    flex: 1;
}

.search-mode-toggle {
    display: flex;
    align-items: center;
    gap: 0.25rem;
    padding: 0.375rem 0.5rem;
    background: var(--color-bg);
    border: 1px solid var(--color-border);
    border-radius: var(--radius-sm);
    cursor: pointer;
    font-size: 0.75rem;
    font-weight: 600;
    color: var(--color-text-muted);
    transition: all 0.15s ease;
}

.search-mode-toggle:hover {
    border-color: var(--color-primary);
}

.search-mode-toggle input {
    display: none;
}

.search-mode-toggle input:checked + .toggle-label {
    color: var(--color-primary);
}

.search-mode-toggle:has(input:checked) {
    background: rgba(67, 97, 238, 0.1);
    border-color: var(--color-primary);
    color: var(--color-primary);
}

.toggle-label {
    user-select: none;
}

/* ========== Responsive ========== */
@media (max-width: 768px) {
    .filters-row {
        flex-direction: column;
    }

    .filter-group {
        width: 100%;
    }

    .actions-bar {
        flex-direction: column;
        align-items: stretch;
    }

    .export-buttons {
        flex-wrap: wrap;
    }

    .stats-summary {
        justify-content: center;
    }

    .col-tags {
        display: none;
    }

    .tab-summary {
        display: none;
    }
}
</file>

<file path="web_ui/templates/index.html">
{% extends "base.html" %}

{% block title %}TabBacklog - Your Tabs{% endblock %}

{% block content %}
<div class="container">
    <!-- Filters Section -->
    <section class="filters-section">
        <form id="filter-form" hx-get="/tabs" hx-target="#tabs-body" hx-trigger="change, keyup delay:300ms from:#search-input">
            <div class="filters-row">
                <!-- Search -->
                <div class="filter-group search-group">
                    <label for="search-input">Search</label>
                    <div class="search-wrapper">
                        <input
                            type="text"
                            id="search-input"
                            name="search"
                            placeholder="Search tabs..."
                            class="search-input"
                        >
                        <label class="search-mode-toggle" title="Use semantic search (AI-powered)">
                            <input type="checkbox" id="semantic-search-toggle" onchange="toggleSearchMode()">
                            <span class="toggle-label">AI</span>
                        </label>
                    </div>
                </div>

                <!-- Status Filter -->
                <div class="filter-group">
                    <label for="status-filter">Status</label>
                    <select id="status-filter" name="status" class="filter-select">
                        <option value="">All Status</option>
                        {% for status in filter_options.statuses %}
                        <option value="{{ status }}">{{ status }}</option>
                        {% endfor %}
                    </select>
                </div>

                <!-- Content Type Filter -->
                <div class="filter-group">
                    <label for="content-type-filter">Type</label>
                    <select id="content-type-filter" name="content_type" class="filter-select">
                        <option value="">All Types</option>
                        {% for ctype in filter_options.content_types %}
                        <option value="{{ ctype }}">{{ ctype }}</option>
                        {% endfor %}
                    </select>
                </div>

                <!-- Processed Filter -->
                <div class="filter-group">
                    <label for="processed-filter">Processed</label>
                    <select id="processed-filter" name="is_processed" class="filter-select">
                        <option value="">All</option>
                        <option value="false">Unprocessed</option>
                        <option value="true">Processed</option>
                    </select>
                </div>

                <!-- Read Time Filter -->
                <div class="filter-group">
                    <label for="read-time-filter">Max Read Time</label>
                    <select id="read-time-filter" name="read_time_max" class="filter-select">
                        <option value="">Any</option>
                        <option value="5">5 min</option>
                        <option value="10">10 min</option>
                        <option value="15">15 min</option>
                        <option value="30">30 min</option>
                        <option value="60">1 hour</option>
                    </select>
                </div>
            </div>
        </form>
    </section>

    <!-- Actions Bar -->
    <section class="actions-bar">
        <div class="selection-info">
            <label class="checkbox-label">
                <input type="checkbox" id="select-all" onchange="toggleSelectAll(this)">
                Select All
            </label>
            <span class="selected-count">
                <span id="selected-count">0</span> selected
            </span>
        </div>

        <div class="export-buttons">
            <button
                class="btn btn-secondary export-btn"
                onclick="exportSelected('json')"
                disabled
            >
                Export JSON
            </button>
            <button
                class="btn btn-secondary export-btn"
                onclick="exportSelected('markdown')"
                disabled
            >
                Export Markdown
            </button>
            <button
                class="btn btn-primary export-btn"
                onclick="exportSelected('obsidian')"
                disabled
            >
                Export to Obsidian
            </button>
        </div>

        <div class="stats-summary">
            <span class="stat">Total: {{ filter_options.total }}</span>
            <span class="stat">Processed: {{ filter_options.processed }}</span>
            <span class="stat">Remaining: {{ filter_options.unprocessed }}</span>
        </div>
    </section>

    <!-- Tabs Table -->
    <section class="tabs-section">
        <table class="tabs-table">
            <thead>
                <tr>
                    <th class="col-checkbox"></th>
                    <th class="col-title">Title</th>
                    <th class="col-type">Type</th>
                    <th class="col-status">Status</th>
                    <th class="col-time">Time</th>
                    <th class="col-tags">Tags</th>
                    <th class="col-actions">Actions</th>
                </tr>
            </thead>
            <tbody id="tabs-body" hx-get="/tabs" hx-trigger="load" hx-swap="innerHTML">
                <tr>
                    <td colspan="7" class="loading-cell">
                        <div class="loading-spinner"></div>
                        Loading tabs...
                    </td>
                </tr>
            </tbody>
        </table>
    </section>

    <!-- Pagination -->
    <section id="pagination-section" class="pagination-section">
        <!-- Populated by HTMX -->
    </section>
</div>

<!-- Tab Detail Modal -->
<div id="tab-modal" class="modal" style="display: none;">
    <div class="modal-backdrop" onclick="closeModal()"></div>
    <div class="modal-content" id="modal-content">
        <!-- Content loaded via HTMX -->
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
    function toggleSelectAll(checkbox) {
        const checkboxes = document.querySelectorAll('.tab-checkbox');
        checkboxes.forEach(cb => cb.checked = checkbox.checked);
        updateSelectedCount();
    }

    function getSelectedIds() {
        const checkboxes = document.querySelectorAll('.tab-checkbox:checked');
        return Array.from(checkboxes).map(cb => parseInt(cb.value));
    }

    async function exportSelected(format) {
        const ids = getSelectedIds();
        if (ids.length === 0) {
            showToast('Please select tabs to export', 'warning');
            return;
        }

        try {
            const response = await fetch(`/export/${format}`, {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({tab_ids: ids})
            });

            if (!response.ok) {
                throw new Error('Export failed');
            }

            // Get filename from Content-Disposition header
            const disposition = response.headers.get('Content-Disposition');
            const filename = disposition
                ? disposition.split('filename=')[1].replace(/"/g, '')
                : `export.${format === 'json' ? 'json' : 'md'}`;

            // Download the file
            const blob = await response.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = filename;
            document.body.appendChild(a);
            a.click();
            a.remove();
            window.URL.revokeObjectURL(url);

            showToast(`Exported ${ids.length} tabs`, 'success');
        } catch (error) {
            showToast('Export failed: ' + error.message, 'error');
        }
    }

    function showTabDetail(tabId) {
        const modal = document.getElementById('tab-modal');
        const content = document.getElementById('modal-content');

        // Load content via HTMX
        htmx.ajax('GET', `/tabs/${tabId}`, {target: '#modal-content', swap: 'innerHTML'});
        modal.style.display = 'flex';
    }

    function closeModal() {
        document.getElementById('tab-modal').style.display = 'none';
    }

    // Close modal on escape
    document.addEventListener('keydown', function(e) {
        if (e.key === 'Escape') closeModal();
    });

    // Update count on checkbox change
    document.addEventListener('change', function(e) {
        if (e.target.classList.contains('tab-checkbox')) {
            updateSelectedCount();
        }
    });

    // Search mode toggle
    let semanticSearchEnabled = false;

    function toggleSearchMode() {
        semanticSearchEnabled = document.getElementById('semantic-search-toggle').checked;
        const form = document.getElementById('filter-form');

        if (semanticSearchEnabled) {
            // Update form to use semantic search endpoint
            form.setAttribute('hx-get', '/search/semantic');
            form.setAttribute('data-search-mode', 'semantic');
            showToast('Semantic search enabled (AI-powered)', 'info');
        } else {
            // Reset to regular search
            form.setAttribute('hx-get', '/tabs');
            form.setAttribute('data-search-mode', 'fuzzy');
        }

        // Re-process HTMX attributes
        htmx.process(form);
    }

    // Semantic search requires a minimum query length
    document.getElementById('search-input').addEventListener('input', function(e) {
        if (semanticSearchEnabled && e.target.value.length > 0 && e.target.value.length < 3) {
            // Don't trigger semantic search for very short queries
            e.stopPropagation();
        }
    });

    // Generate embeddings button handler
    async function generateEmbeddings() {
        try {
            const response = await fetch('/search/generate-embeddings', {method: 'POST'});
            const data = await response.json();
            showToast(data.message, 'success');
        } catch (error) {
            showToast('Failed to start embedding generation', 'error');
        }
    }
</script>
{% endblock %}
</file>

<file path="web_ui/db.py">
"""
TabBacklog v1 - Web UI Database Operations

Database queries for the web UI.
"""

import json
import logging
import os
from contextlib import asynccontextmanager
from datetime import datetime
from typing import AsyncIterator, Optional

import asyncpg

from .models import TabDisplay, TabFilters, TabListResponse, TabExport

logger = logging.getLogger(__name__)


class Database:
    """Async database connection pool manager"""

    def __init__(self, database_url: str):
        self.database_url = database_url
        self._pool: Optional[asyncpg.Pool] = None

    async def connect(self):
        """Create connection pool"""
        self._pool = await asyncpg.create_pool(
            self.database_url,
            min_size=2,
            max_size=10,
        )
        logger.info("Database pool created")

    async def disconnect(self):
        """Close connection pool"""
        if self._pool:
            await self._pool.close()
            logger.info("Database pool closed")

    @asynccontextmanager
    async def connection(self) -> AsyncIterator[asyncpg.Connection]:
        """Get a connection from the pool"""
        async with self._pool.acquire() as conn:
            yield conn

    async def get_tabs(
        self,
        user_id: str,
        filters: TabFilters,
    ) -> TabListResponse:
        """
        Get filtered list of tabs with pagination.
        """
        # Build WHERE clauses
        conditions = ["t.user_id = $1", "t.deleted_at IS NULL"]
        params = [user_id]
        param_idx = 2

        if filters.status:
            conditions.append(f"t.status = ${param_idx}")
            params.append(filters.status)
            param_idx += 1

        if filters.is_processed is not None:
            conditions.append(f"t.is_processed = ${param_idx}")
            params.append(filters.is_processed)
            param_idx += 1

        if filters.content_type:
            conditions.append(f"e.content_type = ${param_idx}")
            params.append(filters.content_type)
            param_idx += 1

        if filters.read_time_max:
            conditions.append(f"(e.est_read_min IS NULL OR e.est_read_min <= ${param_idx})")
            params.append(filters.read_time_max)
            param_idx += 1

        if filters.search:
            # Use pg_trgm similarity search for fuzzy matching
            # Falls back to ILIKE if similarity threshold not met
            conditions.append(f"""
                (t.page_title % ${param_idx}
                 OR e.summary % ${param_idx}
                 OR t.page_title ILIKE ${param_idx + 1}
                 OR t.url ILIKE ${param_idx + 1}
                 OR e.summary ILIKE ${param_idx + 1})
            """)
            params.append(filters.search)  # For trigram similarity
            params.append(f"%{filters.search}%")  # For ILIKE fallback
            param_idx += 2

        if filters.project:
            conditions.append(f"e.raw_meta->>'projects' ILIKE ${param_idx}")
            params.append(f"%{filters.project}%")
            param_idx += 1

        where_clause = " AND ".join(conditions)

        # Count query
        count_query = f"""
            SELECT COUNT(*) as total
            FROM tab_item t
            LEFT JOIN tab_enrichment e ON t.id = e.tab_id
            WHERE {where_clause}
        """

        # Main query with pagination
        offset = (filters.page - 1) * filters.per_page
        query = f"""
            SELECT
                t.id,
                t.url,
                t.page_title,
                t.window_label,
                t.status,
                t.is_processed,
                t.processed_at,
                t.created_at,
                p.site_kind,
                p.word_count,
                p.video_seconds,
                e.summary,
                e.content_type,
                e.est_read_min,
                e.priority,
                e.raw_meta
            FROM tab_item t
            LEFT JOIN tab_parsed p ON t.id = p.tab_id
            LEFT JOIN tab_enrichment e ON t.id = e.tab_id
            WHERE {where_clause}
            ORDER BY t.created_at DESC
            LIMIT ${param_idx} OFFSET ${param_idx + 1}
        """
        params.extend([filters.per_page, offset])

        async with self.connection() as conn:
            # Get total count
            total = await conn.fetchval(count_query, *params[:-2])

            # Get tabs
            rows = await conn.fetch(query, *params)

        tabs = []
        for row in rows:
            raw_meta = row["raw_meta"] or {}
            if isinstance(raw_meta, str):
                raw_meta = json.loads(raw_meta)

            tabs.append(TabDisplay(
                id=row["id"],
                url=row["url"],
                page_title=row["page_title"],
                window_label=row["window_label"],
                status=row["status"],
                is_processed=row["is_processed"],
                processed_at=row["processed_at"],
                created_at=row["created_at"],
                site_kind=row["site_kind"],
                word_count=row["word_count"],
                video_seconds=row["video_seconds"],
                summary=row["summary"],
                content_type=row["content_type"],
                est_read_min=row["est_read_min"],
                priority=row["priority"],
                tags=raw_meta.get("tags", []),
                projects=raw_meta.get("projects", []),
            ))

        total_pages = (total + filters.per_page - 1) // filters.per_page

        return TabListResponse(
            tabs=tabs,
            total=total,
            page=filters.page,
            per_page=filters.per_page,
            total_pages=total_pages,
            has_next=filters.page < total_pages,
            has_prev=filters.page > 1,
        )

    async def toggle_processed(self, user_id: str, tab_id: int) -> Optional[TabDisplay]:
        """Toggle the is_processed flag for a tab"""
        query = """
            UPDATE tab_item
            SET
                is_processed = NOT is_processed,
                processed_at = CASE WHEN is_processed THEN NULL ELSE now() END,
                updated_at = now()
            WHERE id = $1 AND user_id = $2 AND deleted_at IS NULL
            RETURNING id, is_processed, processed_at
        """

        log_query = """
            INSERT INTO event_log (user_id, event_type, entity_type, entity_id, details)
            VALUES ($1, $2, 'tab_item', $3, '{"source": "web_ui"}'::jsonb)
        """

        async with self.connection() as conn:
            row = await conn.fetchrow(query, tab_id, user_id)
            if row:
                event_type = "tab_processed" if row["is_processed"] else "tab_unprocessed"
                await conn.execute(log_query, user_id, event_type, tab_id)

                # Fetch full tab data
                return await self.get_tab_by_id(user_id, tab_id)

        return None

    async def get_tab_by_id(self, user_id: str, tab_id: int) -> Optional[TabDisplay]:
        """Get a single tab by ID"""
        query = """
            SELECT
                t.id,
                t.url,
                t.page_title,
                t.window_label,
                t.status,
                t.is_processed,
                t.processed_at,
                t.created_at,
                p.site_kind,
                p.word_count,
                p.video_seconds,
                e.summary,
                e.content_type,
                e.est_read_min,
                e.priority,
                e.raw_meta
            FROM tab_item t
            LEFT JOIN tab_parsed p ON t.id = p.tab_id
            LEFT JOIN tab_enrichment e ON t.id = e.tab_id
            WHERE t.id = $1 AND t.user_id = $2 AND t.deleted_at IS NULL
        """

        async with self.connection() as conn:
            row = await conn.fetchrow(query, tab_id, user_id)

        if not row:
            return None

        raw_meta = row["raw_meta"] or {}
        if isinstance(raw_meta, str):
            raw_meta = json.loads(raw_meta)

        return TabDisplay(
            id=row["id"],
            url=row["url"],
            page_title=row["page_title"],
            window_label=row["window_label"],
            status=row["status"],
            is_processed=row["is_processed"],
            processed_at=row["processed_at"],
            created_at=row["created_at"],
            site_kind=row["site_kind"],
            word_count=row["word_count"],
            video_seconds=row["video_seconds"],
            summary=row["summary"],
            content_type=row["content_type"],
            est_read_min=row["est_read_min"],
            priority=row["priority"],
            tags=raw_meta.get("tags", []),
            projects=raw_meta.get("projects", []),
        )

    async def get_tabs_for_export(self, user_id: str, tab_ids: list[int]) -> list[TabExport]:
        """Get tabs for export"""
        query = """
            SELECT
                t.url,
                t.page_title,
                t.window_label,
                t.created_at,
                e.summary,
                e.content_type,
                e.est_read_min,
                e.priority,
                e.raw_meta
            FROM tab_item t
            LEFT JOIN tab_enrichment e ON t.id = e.tab_id
            WHERE t.id = ANY($1) AND t.user_id = $2 AND t.deleted_at IS NULL
            ORDER BY t.created_at DESC
        """

        async with self.connection() as conn:
            rows = await conn.fetch(query, tab_ids, user_id)

        exports = []
        for row in rows:
            raw_meta = row["raw_meta"] or {}
            if isinstance(raw_meta, str):
                raw_meta = json.loads(raw_meta)

            exports.append(TabExport(
                url=row["url"],
                title=row["page_title"],
                summary=row["summary"],
                content_type=row["content_type"],
                tags=raw_meta.get("tags", []),
                projects=raw_meta.get("projects", []),
                est_read_min=row["est_read_min"],
                priority=row["priority"],
                window_label=row["window_label"],
                created_at=row["created_at"].isoformat() if row["created_at"] else "",
            ))

        return exports

    async def semantic_search(
        self,
        user_id: str,
        query_embedding: list[float],
        limit: int = 50,
    ) -> list[TabDisplay]:
        """
        Search tabs using vector similarity (semantic search).

        Args:
            user_id: User ID to filter by
            query_embedding: Query embedding vector
            limit: Maximum results to return

        Returns:
            List of TabDisplay sorted by similarity
        """
        query = """
            SELECT
                t.id,
                t.url,
                t.page_title,
                t.window_label,
                t.status,
                t.is_processed,
                t.processed_at,
                t.created_at,
                p.site_kind,
                p.word_count,
                p.video_seconds,
                e.summary,
                e.content_type,
                e.est_read_min,
                e.priority,
                e.raw_meta,
                1 - (emb.embedding <=> $2::vector) as similarity
            FROM tab_item t
            LEFT JOIN tab_parsed p ON t.id = p.tab_id
            LEFT JOIN tab_enrichment e ON t.id = e.tab_id
            JOIN tab_embedding emb ON t.id = emb.tab_id
            WHERE t.user_id = $1 AND t.deleted_at IS NULL
            ORDER BY emb.embedding <=> $2::vector
            LIMIT $3
        """

        async with self.connection() as conn:
            # Convert embedding list to string format for pgvector
            embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"
            rows = await conn.fetch(query, user_id, embedding_str, limit)

        tabs = []
        for row in rows:
            raw_meta = row["raw_meta"] or {}
            if isinstance(raw_meta, str):
                raw_meta = json.loads(raw_meta)

            tabs.append(TabDisplay(
                id=row["id"],
                url=row["url"],
                page_title=row["page_title"],
                window_label=row["window_label"],
                status=row["status"],
                is_processed=row["is_processed"],
                processed_at=row["processed_at"],
                created_at=row["created_at"],
                site_kind=row["site_kind"],
                word_count=row["word_count"],
                video_seconds=row["video_seconds"],
                summary=row["summary"],
                content_type=row["content_type"],
                est_read_min=row["est_read_min"],
                priority=row["priority"],
                tags=raw_meta.get("tags", []),
                projects=raw_meta.get("projects", []),
            ))

        return tabs

    async def get_tabs_without_embeddings(self, user_id: str, limit: int = 100) -> list[dict]:
        """Get tabs that don't have embeddings yet"""
        query = """
            SELECT t.id, t.url, t.page_title, e.summary, p.text_full
            FROM tab_item t
            LEFT JOIN tab_enrichment e ON t.id = e.tab_id
            LEFT JOIN tab_parsed p ON t.id = p.tab_id
            LEFT JOIN tab_embedding emb ON t.id = emb.tab_id
            WHERE t.user_id = $1
              AND t.deleted_at IS NULL
              AND emb.tab_id IS NULL
              AND t.status = 'enriched'
            ORDER BY t.created_at DESC
            LIMIT $2
        """

        async with self.connection() as conn:
            rows = await conn.fetch(query, user_id, limit)

        return [dict(row) for row in rows]

    async def save_embedding(
        self,
        tab_id: int,
        embedding: list[float],
        model_name: str,
    ) -> None:
        """Save an embedding for a tab"""
        query = """
            INSERT INTO tab_embedding (tab_id, embedding, model_name)
            VALUES ($1, $2::vector, $3)
            ON CONFLICT (tab_id) DO UPDATE SET
                embedding = EXCLUDED.embedding,
                model_name = EXCLUDED.model_name,
                updated_at = now()
        """

        async with self.connection() as conn:
            embedding_str = "[" + ",".join(map(str, embedding)) + "]"
            await conn.execute(query, tab_id, embedding_str, model_name)

    async def get_filter_options(self, user_id: str) -> dict:
        """Get available filter options"""
        async with self.connection() as conn:
            # Get distinct statuses
            statuses = await conn.fetch("""
                SELECT DISTINCT status FROM tab_item
                WHERE user_id = $1 AND deleted_at IS NULL
                ORDER BY status
            """, user_id)

            # Get distinct content types
            content_types = await conn.fetch("""
                SELECT DISTINCT e.content_type
                FROM tab_item t
                JOIN tab_enrichment e ON t.id = e.tab_id
                WHERE t.user_id = $1 AND t.deleted_at IS NULL AND e.content_type IS NOT NULL
                ORDER BY e.content_type
            """, user_id)

            # Get counts
            counts = await conn.fetchrow("""
                SELECT
                    COUNT(*) as total,
                    COUNT(*) FILTER (WHERE is_processed) as processed,
                    COUNT(*) FILTER (WHERE NOT is_processed) as unprocessed
                FROM tab_item
                WHERE user_id = $1 AND deleted_at IS NULL
            """, user_id)

        return {
            "statuses": [r["status"] for r in statuses],
            "content_types": [r["content_type"] for r in content_types],
            "total": counts["total"],
            "processed": counts["processed"],
            "unprocessed": counts["unprocessed"],
        }


# Global database instance
_db: Optional[Database] = None


def get_database() -> Database:
    """Get the global database instance"""
    global _db
    if _db is None:
        database_url = os.environ.get("DATABASE_URL")
        if not database_url:
            raise RuntimeError("DATABASE_URL environment variable not set")
        _db = Database(database_url)
    return _db


async def init_database():
    """Initialize the database connection"""
    db = get_database()
    await db.connect()


async def close_database():
    """Close the database connection"""
    global _db
    if _db:
        await _db.disconnect()
        _db = None
</file>

<file path="web_ui/main.py">
"""
TabBacklog v1 - Web UI Main Application

FastAPI application with HTMX-powered interface.
"""

import logging
import os
from contextlib import asynccontextmanager
from pathlib import Path

from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

from . import __version__
from .db import init_database, close_database, get_database
from .models import HealthResponse
from .routes import tabs_router, export_router, search_router

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Paths
BASE_DIR = Path(__file__).parent
TEMPLATES_DIR = BASE_DIR / "templates"
STATIC_DIR = BASE_DIR / "static"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler."""
    logger.info("Web UI starting...")

    # Initialize database
    await init_database()
    logger.info("Database connected")

    yield

    # Cleanup
    await close_database()
    logger.info("Web UI shutting down")


app = FastAPI(
    title="TabBacklog Web UI",
    description="Web interface for managing browser tabs",
    version=__version__,
    lifespan=lifespan,
)

# Mount static files
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# Setup templates
templates = Jinja2Templates(directory=TEMPLATES_DIR)
app.state.templates = templates

# Include routers
app.include_router(tabs_router)
app.include_router(export_router)
app.include_router(search_router)


def get_user_id() -> str:
    """Get the default user ID from environment"""
    return os.environ.get("DEFAULT_USER_ID", "")


@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    """Render the main page."""
    user_id = get_user_id()
    db = get_database()

    # Get filter options
    filter_options = await db.get_filter_options(user_id)

    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "filter_options": filter_options,
        },
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    db = get_database()

    try:
        async with db.connection() as conn:
            await conn.fetchval("SELECT 1")
        db_status = "connected"
    except Exception:
        db_status = "disconnected"

    return HealthResponse(
        status="healthy" if db_status == "connected" else "degraded",
        version=__version__,
        database=db_status,
    )


@app.get("/stats", response_class=HTMLResponse)
async def stats_page(request: Request):
    """Render the stats page."""
    user_id = get_user_id()
    db = get_database()

    # Get stats
    filter_options = await db.get_filter_options(user_id)

    # Get status breakdown
    async with db.connection() as conn:
        status_counts = await conn.fetch("""
            SELECT status, COUNT(*) as count
            FROM tab_item
            WHERE user_id = $1 AND deleted_at IS NULL
            GROUP BY status
            ORDER BY count DESC
        """, user_id)

        content_type_counts = await conn.fetch("""
            SELECT e.content_type, COUNT(*) as count
            FROM tab_item t
            JOIN tab_enrichment e ON t.id = e.tab_id
            WHERE t.user_id = $1 AND t.deleted_at IS NULL AND e.content_type IS NOT NULL
            GROUP BY e.content_type
            ORDER BY count DESC
        """, user_id)

        recent_events = await conn.fetch("""
            SELECT event_type, entity_type, created_at
            FROM event_log
            WHERE user_id = $1
            ORDER BY created_at DESC
            LIMIT 10
        """, user_id)

    return templates.TemplateResponse(
        "stats.html",
        {
            "request": request,
            "filter_options": filter_options,
            "status_counts": [dict(r) for r in status_counts],
            "content_type_counts": [dict(r) for r in content_type_counts],
            "recent_events": [dict(r) for r in recent_events],
        },
    )


# Run with: uvicorn web_ui.main:app --host 0.0.0.0 --port 8000
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="docker-compose.yml">
# TabBacklog v1 - Docker Compose Configuration
# This orchestrates all services for the tab management system

services:
  # ============================================================================
  # PostgreSQL Database (optional - use if not using Supabase)
  # ============================================================================
  postgres:
    image: pgvector/pgvector:pg15
    container_name: tabbacklog-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-tabbacklog}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/schema:/docker-entrypoint-initdb.d:ro
    ports:
      - "5432:5432"
    networks:
      - tabbacklog-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # Parser Microservice
  # ============================================================================
  parser-service:
    build:
      context: .
      dockerfile: parser_service/Dockerfile
    container_name: tabbacklog-parser
    environment:
      - APP_ENV=${APP_ENV:-development}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - FETCH_TIMEOUT=${FETCH_TIMEOUT:-30}
      - MAX_RETRIES=${MAX_RETRIES:-3}
    ports:
      - "8001:8001"
    networks:
      - tabbacklog-network
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8001/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # ============================================================================
  # LLM Enrichment Service
  # ============================================================================
  enrichment-service:
    build:
      context: .
      dockerfile: enrichment_service/Dockerfile
    container_name: tabbacklog-enrichment
    environment:
      - APP_ENV=${APP_ENV:-development}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LLM_API_BASE=${LLM_API_BASE}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL_NAME=${LLM_MODEL_NAME}
      - LLM_TIMEOUT=${LLM_TIMEOUT:-60}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      # Phoenix observability
      - PHOENIX_COLLECTOR_ENDPOINT=${PHOENIX_COLLECTOR_ENDPOINT:-http://phoenix:6006}
      - PHOENIX_PROJECT_NAME=${PHOENIX_PROJECT_NAME:-tabbacklog}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://phoenix:4317
    ports:
      - "8002:8002"
    networks:
      - tabbacklog-network
    depends_on:
      phoenix:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8002/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    # If running LLM locally via LM Studio/Ollama, use host network
    # network_mode: host

  # ============================================================================
  # Web UI
  # ============================================================================
  web-ui:
    build:
      context: .
      dockerfile: web_ui/Dockerfile
    container_name: tabbacklog-ui
    environment:
      - APP_ENV=${APP_ENV:-development}
      - APP_SECRET_KEY=${APP_SECRET_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - DEFAULT_USER_ID=${DEFAULT_USER_ID}
      - PARSER_SERVICE_URL=http://parser-service:8001
      - ENRICHMENT_SERVICE_URL=http://enrichment-service:8002
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    ports:
      - "8000:8000"
    networks:
      - tabbacklog-network
    depends_on:
      postgres:
        condition: service_healthy
      parser-service:
        condition: service_healthy
      enrichment-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # ============================================================================
  # n8n Workflow Orchestrator
  # ============================================================================
  n8n:
    image: n8nio/n8n:latest
    container_name: tabbacklog-n8n
    environment:
      - N8N_HOST=${N8N_HOST:-0.0.0.0}
      - N8N_PORT=${N8N_PORT:-5678}
      - N8N_PROTOCOL=${N8N_PROTOCOL:-http}
      - WEBHOOK_URL=http://localhost:5678
      - GENERIC_TIMEZONE=America/New_York
      
      # Email configuration for error notifications
      - N8N_SMTP_HOST=${N8N_SMTP_HOST}
      - N8N_SMTP_PORT=${N8N_SMTP_PORT:-587}
      - N8N_SMTP_USER=${N8N_SMTP_USER}
      - N8N_SMTP_PASS=${N8N_SMTP_PASS}
      - N8N_SMTP_SENDER=${N8N_SMTP_FROM}
      
      # Database connection for n8n workflows
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-tabbacklog}

      # Service URLs for workflows
      - PARSER_SERVICE_URL=http://parser-service:8001
      - ENRICHMENT_SERVICE_URL=http://enrichment-service:8002
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n/workflows:/home/node/.n8n/workflows:ro
    ports:
      - "5678:5678"
    networks:
      - tabbacklog-network
    depends_on:
      postgres:
        condition: service_healthy
      parser-service:
        condition: service_healthy
      enrichment-service:
        condition: service_healthy
    restart: unless-stopped

  # ============================================================================
  # Phoenix (Arize) - LLM Observability
  # ============================================================================
  phoenix:
    image: arizephoenix/phoenix:latest
    container_name: tabbacklog-phoenix
    ports:
      - "6006:6006"
      - "4317:4317"  # OTLP gRPC endpoint
      - "4318:4318"  # OTLP HTTP endpoint
    networks:
      - tabbacklog-network
    environment:
      - PHOENIX_PORT=6006
      - PHOENIX_HOST=0.0.0.0
    volumes:
      - phoenix_data:/phoenix
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:6006/healthz').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # ============================================================================
  # Optional: PgAdmin for database management
  # ============================================================================
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: tabbacklog-pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@tabbacklog.local
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    networks:
      - tabbacklog-network
    profiles:
      - dev  # Only start with: docker-compose --profile dev up
    restart: unless-stopped

# ============================================================================
# Networks
# ============================================================================
networks:
  tabbacklog-network:
    driver: bridge
    name: ${NETWORK_NAME:-tabbacklog-network}

# ============================================================================
# Volumes
# ============================================================================
volumes:
  postgres_data:
    name: tabbacklog-postgres-data
  n8n_data:
    name: tabbacklog-n8n-data
  phoenix_data:
    name: tabbacklog-phoenix-data

# ============================================================================
# Usage Instructions
# ============================================================================
#
# Development setup:
#   1. Copy .env.example to .env and configure
#   2. docker-compose up -d postgres
#   3. docker-compose up -d parser-service enrichment-service
#   4. docker-compose up -d web-ui
#   5. docker-compose up -d n8n
#
# Full stack:
#   docker-compose up -d
#
# With PgAdmin:
#   docker-compose --profile dev up -d
#
# View logs:
#   docker-compose logs -f [service-name]
#
# Stop all:
#   docker-compose down
#
# Stop and remove volumes:
#   docker-compose down -v
#
# Rebuild after code changes:
#   docker-compose up -d --build [service-name]
#
# Access services:
#   - Web UI: http://localhost:8000
#   - Parser API: http://localhost:8001
#   - Enrichment API: http://localhost:8002
#   - n8n: http://localhost:5678
#   - Phoenix (LLM Observability): http://localhost:6006
#   - PgAdmin: http://localhost:5050 (dev profile only)
#   - PostgreSQL: localhost:5432
#
</file>

<file path="requirements.txt">
# TabBacklog v1 - Python Dependencies

# Web Frameworks
fastapi>=0.115.0
uvicorn[standard]>=0.32.0
jinja2>=3.1.4
python-multipart>=0.0.9

# Database
asyncpg>=0.30.0
psycopg[binary]>=3.2.0
sqlalchemy>=2.0.35

# HTTP Client
httpx>=0.27.0
requests>=2.32.0

# HTML/XML Parsing
beautifulsoup4>=4.12.3
lxml>=5.3.0
html5lib>=1.1

# Video/Content Extraction
yt-dlp>=2025.1.26

# LLM & DSPy
dspy>=2.6.0
openai>=1.50.0
pydantic>=2.9.0
pydantic-settings>=2.5.0

# Observability
arize-phoenix>=4.0.0
openinference-instrumentation-dspy>=0.1.0
opentelemetry-sdk>=1.20.0
opentelemetry-exporter-otlp>=1.20.0

# Vector Search
pgvector>=0.3.0

# CLI
click>=8.1.7
rich>=13.9.0
python-dotenv>=1.0.1

# Data Processing
pandas>=2.2.3
numpy>=2.1.0

# Utilities
tenacity>=9.0.0
python-dateutil>=2.9.0

# Testing
pytest>=8.3.0
pytest-asyncio>=0.24.0
pytest-cov>=5.0.0
pytest-playwright>=0.5.0
playwright>=1.48.0

# Development
black>=24.10.0
ruff>=0.7.0
mypy>=1.13.0

# Markdown Export
markdown>=3.7
</file>

</files>
